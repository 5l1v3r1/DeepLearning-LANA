{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GeracaoAutomaticaDeTexto-English.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladimiralencar/DeepLearning-LANA/blob/master/LSTM/GeracaoAutomaticaDeTexto_English.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hBuNL6TjCt1Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Geração Automática de Texto com LSTMs"
      ]
    },
    {
      "metadata": {
        "id": "e373_beDCt1d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As Redes Neurais Recorrentes também podem ser usadas como modelos generativos. Isso significa que além de serem usadas como modelos preditivos (fazendo previsões), elas podem aprender as sequências de um problema e, em seguida, gerar sequências plausíveis inteiramente novas para o domínio do problema. Modelos Generativos como este são úteis não apenas para estudar o quão bem um modelo aprendeu um problema, mas para saber mais sobre o próprio domínio do problema. "
      ]
    },
    {
      "metadata": {
        "id": "RULNmw5QCt1f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Uma boa forma de praticar a criação de texto, é usando livros clássicos, os quais já temos uma boa ideia sobre a história e que não estejamos violando nenhum direito de copyright. Muitos livros clássicos já não possuem mais restrição de uso e podem ser usados gratuitamente. Um bom lugar para encontrar esses livros é no site do Projeto Gutenberg. É de lá que usaremos o livro para o qual criaremos um modelo generativo: Alice no País das Maravilhas ou o nome em inglês Alice's Adventures in Wonderland. O arquivo txt do livro pode ser baixado aqui: https://www.gutenberg.org/ebooks/11. \n",
        "Este livro tem cerca de 3.300 linhas de texto. O cabeçalho e a marca de final de arquivo foram removidos, já que não são necessários para o que vamos fazer."
      ]
    },
    {
      "metadata": {
        "id": "DXEWf-mrCt1g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Vamos aprender as dependências entre os caracteres e as probabilidades condicionais de caracteres em sequências para que possamos gerar sequências totalmente novas e originais de caracteres. Esta é uma tarefa divertida e recomendo repetir essas experiências com outros livros do Projeto Gutenberg. Essas experiências não se limitam ao texto, você também pode experimentar com outros dados ASCII, como código fonte de linguagens de programação, documentos marcados em LaTeX, HTML ou Markdown e muito mais. \n",
        "\n",
        "Faremos aqui algo muito similar ao que foi feito pelo programador, que escreveu um novo livro de Game ofthrones: http://www.businessinsider.com/ai-just-wrote-the-next-book-of-game-of-thrones-for-us-2017-8"
      ]
    },
    {
      "metadata": {
        "id": "vDkYKGW0Ct1g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b56b0a8-f064-4f49-f0ce-73a687709df0"
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy\n",
        "import sys\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "roE9o-YBCt1o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "3369a2b0-f169-49b4-818b-b08a3a90668f"
      },
      "cell_type": "code",
      "source": [
        "# Carregamos os dados e convertemos para lowercase \n",
        "# Estamos usando aqui arquivo texto no formato ASCII\n",
        "from urllib.request import urlopen\n",
        "import codecs\n",
        "\n",
        "!wget 'https://raw.githubusercontent.com/vladimiralencar/DeepLearning-LANA/master/LSTM/wonderland.txt'\n",
        "\n",
        "#raw_text = urlopen(filename).read()\n",
        "#raw_text = \"\".join( chr(x) for x in bytearray(raw_text) ) # converte de byte para char\n",
        "\n",
        "filename = \"wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()\n",
        "print(raw_text[:400])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-23 01:26:09--  https://raw.githubusercontent.com/vladimiralencar/DeepLearning-LANA/master/LSTM/wonderland.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 147674 (144K) [text/plain]\n",
            "Saving to: ‘wonderland.txt’\n",
            "\n",
            "\rwonderland.txt        0%[                    ]       0  --.-KB/s               \rwonderland.txt      100%[===================>] 144.21K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-01-23 01:26:09 (8.94 MB/s) - ‘wonderland.txt’ saved [147674/147674]\n",
            "\n",
            "﻿chapter i. down the rabbit-hole\n",
            "\n",
            "alice was beginning to get very tired of sitting by her sister on the\n",
            "bank, and of having nothing to do: once or twice she had peeped into the\n",
            "book her sister was reading, but it had no pictures or conversations in\n",
            "it, 'and what is the use of a book,' thought alice 'without pictures or\n",
            "conversations?'\n",
            "\n",
            "so she was considering in her own mind (as well as she could, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bqeKhn3NCt1s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora que o livro está carregado, devemos preparar os dados para modelagem. Não podemos modelar os caracteres diretamente, em vez disso, devemos converter os caracteres em números inteiros. Podemos fazer isso facilmente, criando um conjunto de todos os caracteres distintos do livro, então criando um mapa de cada caractere para um único inteiro."
      ]
    },
    {
      "metadata": {
        "id": "zTrARETMCt1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Criando o mapeamento caracter/inteiro\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vYpjKkGUCt1w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "5d195241-fe01-4dba-971a-d3e7f794f204"
      },
      "cell_type": "code",
      "source": [
        "chars"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " '[',\n",
              " ']',\n",
              " '_',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '\\ufeff']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "grm3TaVUCt11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "aa2fce4f-5a90-42d3-b6d7-af7d796154c2"
      },
      "cell_type": "code",
      "source": [
        "char_to_int"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " \"'\": 4,\n",
              " '(': 5,\n",
              " ')': 6,\n",
              " '*': 7,\n",
              " ',': 8,\n",
              " '-': 9,\n",
              " '.': 10,\n",
              " ':': 11,\n",
              " ';': 12,\n",
              " '?': 13,\n",
              " '[': 14,\n",
              " ']': 15,\n",
              " '_': 16,\n",
              " 'a': 17,\n",
              " 'b': 18,\n",
              " 'c': 19,\n",
              " 'd': 20,\n",
              " 'e': 21,\n",
              " 'f': 22,\n",
              " 'g': 23,\n",
              " 'h': 24,\n",
              " 'i': 25,\n",
              " 'j': 26,\n",
              " 'k': 27,\n",
              " 'l': 28,\n",
              " 'm': 29,\n",
              " 'n': 30,\n",
              " 'o': 31,\n",
              " 'p': 32,\n",
              " 'q': 33,\n",
              " 'r': 34,\n",
              " 's': 35,\n",
              " 't': 36,\n",
              " 'u': 37,\n",
              " 'v': 38,\n",
              " 'w': 39,\n",
              " 'x': 40,\n",
              " 'y': 41,\n",
              " 'z': 42,\n",
              " '\\ufeff': 43}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "J-_8KJfSCt15",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pode haver alguns caracteres que podemos remover para limpar mais o conjunto de dados que reduzirá o vocabulário e poderá melhorar o processo de modelagem. "
      ]
    },
    {
      "metadata": {
        "id": "V1_7yCG4Ct16",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c453c4ae-addb-4a4a-f67a-778acf53f613"
      },
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: {:,d}\".format(n_chars))  \n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters: 144,343\n",
            "Total Vocab:  44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R_omy5DaCt1-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Podemos ver que o livro tem pouco menos de 150.000 caracteres e que quando convertidos para minúsculas, existem apenas 44 caracteres distintos no vocabulário para a rede aprender, muito mais do que os 26 no alfabeto. Agora, precisamos definir os dados de treinamento para a rede. Existe muita flexibilidade em como você escolhe dividir o texto e expô-lo a rede durante o treino. Aqui dividiremos o texto do livro em subsequências com um comprimento de 100 caracteres, um comprimento arbitrário. Poderíamos facilmente dividir os dados por sentenças e ajustar as sequências mais curtas e truncar as mais longas. Cada padrão de treinamento da rede é composto de 100 passos de tempo (time steps) de um caractere (X) seguido por um caracter de saída (y). Ao criar essas sequências, deslizamos esta janela ao longo de todo o livro um caracter de cada vez, permitindo que cada caracter tenha a chance de ser aprendido a partir dos 100 caracteres que o precederam (exceto os primeiros 100 caracteres, é claro). Por exemplo, se o comprimento da sequência é 5 (para simplificar), os dois primeiros padrões de treinamento seriam os seguintes:\n",
        "\n",
        "* Palavra: CHAPTER\n",
        "* CHAPT -> E\n",
        "* HAPTE -> R"
      ]
    },
    {
      "metadata": {
        "id": "CtbhASwACt1_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0a77444-8e46-4f31-9a34-31297049a8cb"
      },
      "cell_type": "code",
      "source": [
        "# À medida que dividimos o livro em sequências, convertemos os caracteres em números inteiros usando nossa\n",
        "# tabela de pesquisa que preparamos anteriormente.\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = raw_text[i:i + seq_length]\n",
        "    seq_out = raw_text[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total de Padrões: \", n_patterns)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de Padrões:  144243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2Scw5bBqCt2C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora que preparamos nossos dados de treinamento, precisamos transformá-lo para que possamos usá-lo com o Keras. Primeiro, devemos transformar a lista de sequências de entrada na forma [amostras, passos de tempo, recursos] esperados por uma rede LSTM. Em seguida, precisamos redimensionar os números inteiros para o intervalo de 0 a 1 para tornar os padrões mais fáceis de aprender pela rede LSTM que usa a função de ativação sigmoide por padrão."
      ]
    },
    {
      "metadata": {
        "id": "GgVhEEfgCt2D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reshape de X para [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# Normalização\n",
        "X = X / float(n_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8xCkKUteCt2I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finalmente, precisamos converter os padrões de saída (caracteres únicos convertidos em números inteiros) usando Hot-Encoding. Isto é para que possamos configurar a rede para prever a probabilidade de cada um dos 44 caracteres diferentes no vocabulário (uma representação mais fácil) em vez de tentar forçá-lo a prever com precisão o próximo caracter. Cada valor de y é convertido em um vetor com um comprimento 44, cheio de zeros, exceto com um 1 na coluna para a letra (inteiro) que o padrão representa. Por exemplo, quando a letra n (valor inteiro 30) tiver sido transformada usando One-Hot Encoding, vai se parecer com isso:\n",
        "\n",
        "[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
      ]
    },
    {
      "metadata": {
        "id": "RGuvCQUmCt2J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# One-Hot Encoding da variável de saída\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CzfUz-v7Ct2N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Modelo LSTM com duas camadas de Dropout com 20%\n",
        "# O tempo de treinamento é bem longo\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mXW7yYmgCt2Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Não há conjunto de dados de teste. Estamos modelando todo o conjunto de dados de treinamento para aprender a probabilidade de cada caracter em uma sequência. Não estamos interessados nos mais preciso modelo do conjunto de dados de treinamento (Acurácia de Classificação). Este seria um modelo que prevê cada caracter no conjunto de dados de treinamento perfeitamente. Em vez disso, estamos interessados em uma generalização do conjunto de dados que minimiza a função de perda escolhida. Estamos buscando um equilíbrio entre generalização e\n",
        "overfitting."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6d8XmE-8C5L-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define o checkpoint\n",
        "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5-English\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3tvYcjDKCt2V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fit do modelo"
      ]
    },
    {
      "metadata": {
        "id": "mRaxPa0HCt2X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model.fit(X, y, epochs = 20, batch_size = 128, callbacks = callbacks_list)\n",
        "model.fit(X, y, epochs = 50, batch_size = 64, callbacks = callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7eGF3aA8Ct2a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Depois de executar o fit, você deve ter uma série de arquivos de checkpoint no mesmo diretório onde está este Jupyter Notebook. Você pode excluí-los todos exceto aquele com o menor valor de perda. Por exemplo, neste caso, o arquivo weights-improvement-19-1.9119.hdf5 será usado. Ele contém os melhores valores de peso."
      ]
    },
    {
      "metadata": {
        "id": "Aq7MCXmZPeoZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "ea19a1ad-b8fb-4e86-94c7-57f200ae8cfa"
      },
      "cell_type": "code",
      "source": [
        "# Carrega os melhores pesos da rede e compila o modelo\n",
        "!wget 'https://raw.githubusercontent.com/vladimiralencar/DeepLearning-LANA/master/LSTM/weights-improvement-49-1.3344.hdf5'\n",
        "file = \"weights-improvement-49-1.3344.hdf5\"\n",
        "\n",
        "model.load_weights(file)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-23 01:26:56--  https://raw.githubusercontent.com/vladimiralencar/DeepLearning-LANA/master/LSTM/weights-improvement-49-1.3344.hdf5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9634184 (9.2M) [application/octet-stream]\n",
            "Saving to: ‘weights-improvement-49-1.3344.hdf5.1’\n",
            "\n",
            "\r          weights-i   0%[                    ]       0  --.-KB/s               \rweights-improvement 100%[===================>]   9.19M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2019-01-23 01:26:56 (144 MB/s) - ‘weights-improvement-49-1.3344.hdf5.1’ saved [9634184/9634184]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D4ap6YwKCt2k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_IdCzcuSCt2s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text():\n",
        "    # Obtém um random seed\n",
        "    start = numpy.random.randint(0, len(dataX)-1)\n",
        "\n",
        "    # Inicia a geração de texto de um ponto qualquer, definido pelo random seed \"start\"\n",
        "    pattern = dataX[start]\n",
        "    print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "\n",
        "    # Gerando caracteres\n",
        "    for i in range(1000):\n",
        "        x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "        x = x / float(n_vocab)\n",
        "        prediction = model.predict(x, verbose=0)\n",
        "        index = numpy.argmax(prediction)\n",
        "        result = int_to_char[index]\n",
        "        seq_in = [int_to_char[value] for value in pattern]\n",
        "        sys.stdout.write(result)\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "    print('\\n=================================================================================' +\n",
        "          '================================\\n')     \n",
        "    #print (\"\\nConcluído.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-9bE5MkbCt2v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "f47f4871-6cc4-4815-edf2-23c8766523d3"
      },
      "cell_type": "code",
      "source": [
        "for i in range(4):\n",
        "    generate_text()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"  which is which?' she said to herself, and nibbled a little of\n",
            "the right-hand bit to try the effect: \"\n",
            " the dould heard off the caby was the pueen said to her head to have to be an ond of the coorersations and teemed to be a lettor of tee it was a great duactly was a great durrals the cane with the soom of the door, and the soom as she was soon a little shale on the soom of the door and the too of the sooes, and she was not and sooetheng at the cook had to be tee with the moment to have to some of the door with the soof of the door and then sueden the cook, and the momk with the same way of exerything the same with the soom of the door, and the soom aadk again, and she was a very curious senarking to herself 'it was a parhe mittle birds with the same with what it was a large canlessations and the soofs, and the soom as she was a very curious semark, and the doom had to say the caby with the soom of the door and the tooes, and she was not and sooetheng about the caby was the pueen said to her eyes, and the soom aadk again, and she was a very curious cropiens and the caby was snon and see\n",
            "=================================================================================================================\n",
            "\n",
            "\" d!'\n",
            "\n",
            "'what for?' said the one who had spoken first.\n",
            "\n",
            "'that's none of your business, two!' said seven \"\n",
            " in a low, wor of ger voice. 'i don't know what i mnow what i mnow what i mnow what i mnow what i was so be a beal to come on the same as you lnow what it was anl that made of mine a book with the soof of the door, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parh\n",
            "=================================================================================================================\n",
            "\n",
            "\"  shan't go, at any rate,' said alice: 'besides, that's not a\n",
            "regular rule: you invented it just now. \"\n",
            "'\n",
            "\n",
            "'i don't know what i mnow what i mnow what i mnow what i mnow what i mnow what i mnow what i mnow it was an anll, and the mext witness!'\n",
            "\n",
            "'wou dan't tee the dance? '                                                                                                   \n",
            "  \n",
            "  \n",
            "\n",
            " come, iy dear and mone and she was the datelpilg 'i ael nu dear the moment all the cistance in the same with mike a battraliny with the same with mittle bett to see it was a great duactle the caby woice, and the soom of the door and the thing was a little sable to the danee of cxtrersations in the soof of the door, and the soom aadk again, and she was a very curious senarking to herself 'it was a parhe mittle birds with the same with what it was a large canlessations and the soofs, and the soom as she was a very curious semark, and the doom had to say the caby with the soom of the door and the tooes, and she was not and sooetheng about the caby was the pueen said to her eyes, and the soom aadk again, and she was a \n",
            "=================================================================================================================\n",
            "\n",
            "\" d to the porpoise, \"keep back, please: we\n",
            "don't want you with us!\"'\n",
            "\n",
            "'they were obliged to have him  \"\n",
            "in the babk,' she said to herself, 'and the soo of a wery little biildren the cirds with the same with a brtter, and the soom of the door and the thing was a little sable to the dance of cenightenl the dirtance the court, and was going to say anything the soof of the soo of the sooes, and she was not and sooe of the toods of her hands, and she was not and some minutes the was a gis siate wooder the cook had to be tee would the way to be tee was to all the way the way the cook had to be tee whth the same with with the same as the cook, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with wi\n",
            "=================================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NcOip50UCt20",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for i in range(5):\n",
        "#    generate_text()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5sK7ezLoCt23",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}