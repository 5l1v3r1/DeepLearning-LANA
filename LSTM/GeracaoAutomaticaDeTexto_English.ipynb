{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GeracaoAutomaticaDeTexto-English.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladimiralencar/DeepLearning-LANA/blob/master/LSTM/GeracaoAutomaticaDeTexto_English.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hBuNL6TjCt1Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Geração Automática de Texto com LSTMs"
      ]
    },
    {
      "metadata": {
        "id": "e373_beDCt1d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As Redes Neurais Recorrentes também podem ser usadas como modelos generativos. Isso significa que além de serem usadas como modelos preditivos (fazendo previsões), elas podem aprender as sequências de um problema e, em seguida, gerar sequências plausíveis inteiramente novas para o domínio do problema. Modelos Generativos como este são úteis não apenas para estudar o quão bem um modelo aprendeu um problema, mas para saber mais sobre o próprio domínio do problema. "
      ]
    },
    {
      "metadata": {
        "id": "RULNmw5QCt1f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Uma boa forma de praticar a criação de texto, é usando livros clássicos, os quais já temos uma boa ideia sobre a história e que não estejamos violando nenhum direito de copyright. Muitos livros clássicos já não possuem mais restrição de uso e podem ser usados gratuitamente. Um bom lugar para encontrar esses livros é no site do Projeto Gutenberg. É de lá que usaremos o livro para o qual criaremos um modelo generativo: Alice no País das Maravilhas ou o nome em inglês Alice's Adventures in Wonderland. O arquivo txt do livro pode ser baixado aqui: https://www.gutenberg.org/ebooks/11. \n",
        "Este livro tem cerca de 3.300 linhas de texto. O cabeçalho e a marca de final de arquivo foram removidos, já que não são necessários para o que vamos fazer."
      ]
    },
    {
      "metadata": {
        "id": "DXEWf-mrCt1g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Vamos aprender as dependências entre os caracteres e as probabilidades condicionais de caracteres em sequências para que possamos gerar sequências totalmente novas e originais de caracteres. Esta é uma tarefa divertida e recomendo repetir essas experiências com outros livros do Projeto Gutenberg. Essas experiências não se limitam ao texto, você também pode experimentar com outros dados ASCII, como código fonte de linguagens de programação, documentos marcados em LaTeX, HTML ou Markdown e muito mais. \n",
        "\n",
        "Faremos aqui algo muito similar ao que foi feito pelo programador, que escreveu um novo livro de Game ofthrones: http://www.businessinsider.com/ai-just-wrote-the-next-book-of-game-of-thrones-for-us-2017-8"
      ]
    },
    {
      "metadata": {
        "id": "vDkYKGW0Ct1g",
        "colab_type": "code",
        "colab": {},
        "outputId": "a21cc029-1648-46b6-d1ce-f37a8c56949e"
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy\n",
        "import sys\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "roE9o-YBCt1o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Carregamos os dados e convertemos para lowercase \n",
        "# Estamos usando aqui arquivo texto no formato ASCII\n",
        "filename = \"wonderland.txt\"\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bqeKhn3NCt1s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora que o livro está carregado, devemos preparar os dados para modelagem. Não podemos modelar os caracteres diretamente, em vez disso, devemos converter os caracteres em números inteiros. Podemos fazer isso facilmente, criando um conjunto de todos os caracteres distintos do livro, então criando um mapa de cada caractere para um único inteiro."
      ]
    },
    {
      "metadata": {
        "id": "zTrARETMCt1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Criando o mapeamento caracter/inteiro\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vYpjKkGUCt1w",
        "colab_type": "code",
        "colab": {},
        "outputId": "5545f3a0-c80d-4e7f-f131-65c71a23a8b2"
      },
      "cell_type": "code",
      "source": [
        "chars"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " '[',\n",
              " ']',\n",
              " '_',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '\\ufeff']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "grm3TaVUCt11",
        "colab_type": "code",
        "colab": {},
        "outputId": "68450c84-a05b-46ce-9bc5-4dfe705f762f"
      },
      "cell_type": "code",
      "source": [
        "char_to_int"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " \"'\": 4,\n",
              " '(': 5,\n",
              " ')': 6,\n",
              " '*': 7,\n",
              " ',': 8,\n",
              " '-': 9,\n",
              " '.': 10,\n",
              " ':': 11,\n",
              " ';': 12,\n",
              " '?': 13,\n",
              " '[': 14,\n",
              " ']': 15,\n",
              " '_': 16,\n",
              " 'a': 17,\n",
              " 'b': 18,\n",
              " 'c': 19,\n",
              " 'd': 20,\n",
              " 'e': 21,\n",
              " 'f': 22,\n",
              " 'g': 23,\n",
              " 'h': 24,\n",
              " 'i': 25,\n",
              " 'j': 26,\n",
              " 'k': 27,\n",
              " 'l': 28,\n",
              " 'm': 29,\n",
              " 'n': 30,\n",
              " 'o': 31,\n",
              " 'p': 32,\n",
              " 'q': 33,\n",
              " 'r': 34,\n",
              " 's': 35,\n",
              " 't': 36,\n",
              " 'u': 37,\n",
              " 'v': 38,\n",
              " 'w': 39,\n",
              " 'x': 40,\n",
              " 'y': 41,\n",
              " 'z': 42,\n",
              " '\\ufeff': 43}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "J-_8KJfSCt15",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pode haver alguns caracteres que podemos remover para limpar mais o conjunto de dados que reduzirá o vocabulário e poderá melhorar o processo de modelagem. "
      ]
    },
    {
      "metadata": {
        "id": "V1_7yCG4Ct16",
        "colab_type": "code",
        "colab": {},
        "outputId": "00b48556-9dc0-4c29-d25e-a3c8f0e3539c"
      },
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: {:,d}\".format(n_chars))  \n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters: 144,343\n",
            "Total Vocab:  44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R_omy5DaCt1-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Podemos ver que o livro tem pouco menos de 150.000 caracteres e que quando convertidos para minúsculas, existem apenas 44 caracteres distintos no vocabulário para a rede aprender, muito mais do que os 26 no alfabeto. Agora, precisamos definir os dados de treinamento para a rede. Existe muita flexibilidade em como você escolhe dividir o texto e expô-lo a rede durante o treino. Aqui dividiremos o texto do livro em subsequências com um comprimento de 100 caracteres, um comprimento arbitrário. Poderíamos facilmente dividir os dados por sentenças e ajustar as sequências mais curtas e truncar as mais longas. Cada padrão de treinamento da rede é composto de 100 passos de tempo (time steps) de um caractere (X) seguido por um caracter de saída (y). Ao criar essas sequências, deslizamos esta janela ao longo de todo o livro um caracter de cada vez, permitindo que cada caracter tenha a chance de ser aprendido a partir dos 100 caracteres que o precederam (exceto os primeiros 100 caracteres, é claro). Por exemplo, se o comprimento da sequência é 5 (para simplificar), os dois primeiros padrões de treinamento seriam os seguintes:\n",
        "\n",
        "* Palavra: CHAPTER\n",
        "* CHAPT -> E\n",
        "* HAPTE -> R"
      ]
    },
    {
      "metadata": {
        "id": "CtbhASwACt1_",
        "colab_type": "code",
        "colab": {},
        "outputId": "b638ce07-a6f4-4b47-a4eb-4a313e8aa011"
      },
      "cell_type": "code",
      "source": [
        "# À medida que dividimos o livro em sequências, convertemos os caracteres em números inteiros usando nossa\n",
        "# tabela de pesquisa que preparamos anteriormente.\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = raw_text[i:i + seq_length]\n",
        "    seq_out = raw_text[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total de Padrões: \", n_patterns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de Padrões:  144243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2Scw5bBqCt2C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora que preparamos nossos dados de treinamento, precisamos transformá-lo para que possamos usá-lo com o Keras. Primeiro, devemos transformar a lista de sequências de entrada na forma [amostras, passos de tempo, recursos] esperados por uma rede LSTM. Em seguida, precisamos redimensionar os números inteiros para o intervalo de 0 a 1 para tornar os padrões mais fáceis de aprender pela rede LSTM que usa a função de ativação sigmoide por padrão."
      ]
    },
    {
      "metadata": {
        "id": "GgVhEEfgCt2D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reshape de X para [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# Normalização\n",
        "X = X / float(n_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8xCkKUteCt2I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finalmente, precisamos converter os padrões de saída (caracteres únicos convertidos em números inteiros) usando Hot-Encoding. Isto é para que possamos configurar a rede para prever a probabilidade de cada um dos 44 caracteres diferentes no vocabulário (uma representação mais fácil) em vez de tentar forçá-lo a prever com precisão o próximo caracter. Cada valor de y é convertido em um vetor com um comprimento 44, cheio de zeros, exceto com um 1 na coluna para a letra (inteiro) que o padrão representa. Por exemplo, quando a letra n (valor inteiro 30) tiver sido transformada usando One-Hot Encoding, vai se parecer com isso:\n",
        "\n",
        "[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
      ]
    },
    {
      "metadata": {
        "id": "RGuvCQUmCt2J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# One-Hot Encoding da variável de saída\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CzfUz-v7Ct2N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Modelo LSTM com duas camadas de Dropout com 20%\n",
        "# O tempo de treinamento é bem longo\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mXW7yYmgCt2Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Não há conjunto de dados de teste. Estamos modelando todo o conjunto de dados de treinamento para aprender a probabilidade de cada caracter em uma sequência. Não estamos interessados nos mais preciso modelo do conjunto de dados de treinamento (Acurácia de Classificação). Este seria um modelo que prevê cada caracter no conjunto de dados de treinamento perfeitamente. Em vez disso, estamos interessados em uma generalização do conjunto de dados que minimiza a função de perda escolhida. Estamos buscando um equilíbrio entre generalização e\n",
        "overfitting."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6d8XmE-8C5L-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define o checkpoint\n",
        "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5-English\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3tvYcjDKCt2V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fit do modelo"
      ]
    },
    {
      "metadata": {
        "id": "mRaxPa0HCt2X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model.fit(X, y, epochs = 20, batch_size = 128, callbacks = callbacks_list)\n",
        "model.fit(X, y, epochs = 50, batch_size = 64, callbacks = callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7eGF3aA8Ct2a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Depois de executar o fit, você deve ter uma série de arquivos de checkpoint no mesmo diretório onde está este Jupyter Notebook. Você pode excluí-los todos exceto aquele com o menor valor de perda. Por exemplo, neste caso, o arquivo weights-improvement-19-1.9119.hdf5 será usado. Ele contém os melhores valores de peso."
      ]
    },
    {
      "metadata": {
        "id": "9eqJu4SQCt2d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Carrega os melhores pesos da rede e compila o modelo\n",
        "filename = \"weights-improvement-49-1.3344.hdf5-English\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D4ap6YwKCt2k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_IdCzcuSCt2s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text():\n",
        "    # Obtém um random seed\n",
        "    start = numpy.random.randint(0, len(dataX)-1)\n",
        "\n",
        "    # Inicia a geração de texto de um ponto qualquer, definido pelo random seed \"start\"\n",
        "    pattern = dataX[start]\n",
        "    print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "\n",
        "    # Gerando caracteres\n",
        "    for i in range(1000):\n",
        "        x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "        x = x / float(n_vocab)\n",
        "        prediction = model.predict(x, verbose=0)\n",
        "        index = numpy.argmax(prediction)\n",
        "        result = int_to_char[index]\n",
        "        seq_in = [int_to_char[value] for value in pattern]\n",
        "        sys.stdout.write(result)\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "    print('\\n=================================================================================' +\n",
        "          '================================\\n')     \n",
        "    #print (\"\\nConcluído.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-9bE5MkbCt2v",
        "colab_type": "code",
        "colab": {},
        "outputId": "3773ad0d-692d-4c82-9017-08bc99cf6666"
      },
      "cell_type": "code",
      "source": [
        "for i in range(4):\n",
        "    generate_text()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\" ir tails in their mouths; and the\n",
            "reason is--' here the mock turtle yawned and shut his eyes.--'tell \"\n",
            " as the cookows with the same way the way in the pight with the same as the court, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with th\n",
            "\n",
            "=================================================================================================================\n",
            "\n",
            "\n",
            "\" ng the list of singers.\n",
            "\n",
            "'you may go,' said the king, and the hatter hurriedly left the court,\n",
            "witho \"\n",
            "ut a mittle sable, and the soom of the door and the thing was a little sable, and the soom of the mock turtle sook mittle best would be a coor of the garden of the soofshens were all the caby was the pueen sat again, and the soom of the mock turtle said to herself 'it was a parhe care with the same with when it was a large canles of the soofshens and the soofshens was a great duactle the cook had to be tee whth the same with with the same with with the same as the court, and she was the pueen said to herself 'it was a parhe care with the same with when it was a large canloal of the soofshens with the soof of the door, and the soom as she was a little shated as the cook had to be tee would the door and she was a good deal of she was to see it to see it would be a lerttes to the baby would be a coor of the garden of the soofshe canes and the shmer as he spoke the caby with the soofs, and the soom of the door way the way to say alo the way to had a very goass were all the way to gave abou\n",
            "\n",
            "=================================================================================================================\n",
            "\n",
            "\n",
            "\"  and take it away!'\n",
            "\n",
            "there was a long silence after this, and alice could only hear whispers\n",
            "now and \"\n",
            " then she was soon little herting her head to her to see it was a great duactle the caby with the soof of the door and then suedenly to see it was a great duactle the caby was the pueen sat again, and the doom was a little shale of the soom of the door with the soof of the door, and the soom aadk again, and she was a very curious senarking to herself 'it was a parhe mittle birds with the same with what it was a large canlessations and the soofs, and the soom as she was a very curious semark, and the doom had to say the caby with the soom of the door and the tooes, and she was not and sooetheng about the caby was the pueen said to her eyes, and the soom aadk again, and she was a very curious cropiens and the caby was snon and seemed to be a lettor of tee it was a great duactle the caby was the pueen sat again, and the soom of the door and the soo of her head to be tee with the same with with the sooes, and the soom of the door and the soo of her head to be a little book with the soom of\n",
            "\n",
            "=================================================================================================================\n",
            "\n",
            "\n",
            "\" hat\n",
            "\"it\" means.'\n",
            "\n",
            "'i know what \"it\" means well enough, when i find a thing,' said the\n",
            "duck: 'it's ge \"\n",
            "tting and seally was a grow what it was an anlle to see it was the pueen so side,' she shought all the caterpillar cack and the coorer, and she was not and sooe of the book had to be tee with the same with with the same as the cook, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to hers\n",
            "\n",
            "=================================================================================================================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NcOip50UCt20",
        "colab_type": "code",
        "colab": {},
        "outputId": "6399e7f3-fd28-42b3-949c-144025f0ace9"
      },
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    generate_text()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\" shion, and\n",
            "this was her dream:--\n",
            "\n",
            "first, she dreamed of little alice herself, and once again the tin \"\n",
            "e she had not the poor of the garden, and was a little shaten taie to her eyes, and the words were all suite sueeen the cook with the soom of the door, and the soom aadk again, and she was a very curious senarking to herself 'it was a parhe mittle birds with the same with what it was a large canlessations and the soofs, and the soom as she was a very curious semark, and the doom had to say the caby with the soom of the door and the tooes, and she was not and sooetheng about the caby was the pueen said to her eyes, and the soom aadk again, and she was a very curious cropiens and the caby was snon and seemed to be a lettor of tee it was a great duactle the caby was the pueen sat again, and the soom of the door and the soo of her head to be tee with the same with with the sooes, and the soom of the door and the soo of her head to be a little book with the soom of the door, and the soom of the door and the thing was a good deal of she was to sook a little shale of the soofshens was the poo\n",
            "\n",
            "=================================================================================================================\n",
            "\n",
            "\n",
            "\" t home,' thought poor alice, 'when one wasn't\n",
            "always growing larger and smaller, and being ordered a \"\n",
            "nd seally with the same with with the sooes, and she was a very curious senarking to herself 'it was a parhe mittle best bat would be a great durrher and sears would be a ceal canl hnto the cook harden the cirtance in the soof of the soods, and she was not and teemed to be tee whth the same with with the same as the court, and she was the pueen said to herself 'it was a parhe care with the same with when it was a large canloal of the soofshens with the soof of the door, and the soom as she was a little shated as the cook had to be tee would the door and she was a good deal of she was to see it to see it would be a lerttes to the baby would be a coor of the garden of the soofshe canes and the shmer as he spoke the caby with the soofs, and the soom of the door way the way to say alo the way to had a very goass were all the way to gave about the caby would be a coor of the shmer as he spoke the caby was the pueen so sook the same with the soom of the door, and the soom aadk again, and she\n",
            "\n",
            "=================================================================================================================\n",
            "\n",
            "\n",
            "\" then the eleventh day must have been a\n",
            "holiday?'\n",
            "\n",
            "'of course it was,' said the mock turtle.\n",
            "\n",
            "'and ho \"\n",
            "w dinlgre it all the things than it was,' said the king, 'and the soo of the bane was to say anything the cirds with the same as the cookowsen, and the soof of the googft was a great duice of the door, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with t\n",
            "\n",
            "=================================================================================================================\n",
            "\n",
            "\n",
            "\" s time there were\n",
            "two little shrieks, and more sounds of broken glass. 'what a number of\n",
            "cucumber-fr \"\n",
            "ow all the same with me a mong as i mike a sea-- \n",
            "\n",
            "'i con't know what i mnow what i mnow what i mnow what i mnow what i mnow what i mnow it was a crtatrarion of mike a batcssacle with the same with mike a bancs and grown to see it was a great duactle the cookoea would have a little boutle her armeared and a little boutle had a very little biild again, and the doom was a little shale of the soof of the door and the thing was a great duice of the door, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a parhe care with the same with with the soof, and the soom aadk again, and she was the pueen said to herself 'it was a \n",
            "\n",
            "=================================================================================================================\n",
            "\n",
            "\n",
            "\" n.\n",
            "\n",
            "'mary ann! mary ann!' said the voice. 'fetch me my gloves this moment!'\n",
            "then came a little patte \"\n",
            "ring of the sabbit sas to torent that she was soiethed at the cook harden at the cookoess woice, and she was the pueen said to herself 'it was a parhe care with the same with when it was a large canloal of the soofshens with the soof of the door, and the soom as she was a little shated as the cook had to be tee would the door and she was a good deal of she was to see it to see it would be a lerttes to the baby would be a coor of the garden of the soofshe canes and the shmer as he spoke the caby with the soofs, and the soom of the door way the way to say alo the way to had a very goass were all the way to gave about the caby would be a coor of the shmer as he spoke the caby was the pueen so sook the same with the soom of the door, and the soom aadk again, and she was a very curious senarking to herself 'it was a parhe mittle birds with the same with what it was a large canlessations and the soofs, and the soom as she was a very curious semark, and the doom had to say the caby with the s\n",
            "\n",
            "=================================================================================================================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5sK7ezLoCt23",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}