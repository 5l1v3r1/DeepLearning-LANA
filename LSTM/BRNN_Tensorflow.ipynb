{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BRNN-Tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladimiralencar/DeepLearning-LANA/blob/master/LSTM/BRNN_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "QPfWIXgDdfEW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## RNNs\n",
        "\n",
        "A ideia básica de RNNs é fazer uso de informações de tipo sequencial na entrada. Essas redes são recorrentes porque executam os mesmos cálculos para todos os elementos de uma sequência de entradas e a saída de cada elemento depende, além da entrada atual, de todos os cálculos anteriores. As RNNs provaram ter um excelente desempenho em problemas como a previsão do próximo caracter em um texto ou, da mesma forma, a predição da próxima sequência de palavras em uma frase.\n",
        "\n",
        "De fato, as RNNs executam a mesma computação em cada instância, em diferentes entradas da mesma sequência. Compartilhando os mesmos parâmetros, também, uma RNN reduz fortemente o número de parâmetros que a rede deve aprender durante a fase de treinamento, melhorando assim os tempos de treinamento.\n",
        "\n",
        "Por exemplo, para calcular o gradiente (no tempo t = 4), é necessário propagar o gradiente calculado por três instantes do tempo anterior e depois somar os gradientes assim obtidos. De fato, uma sequência de entrada inteira normalmente é considerada um único elemento do conjunto de treinamento. Portanto, se o erro total é simplesmente a soma dos erros em cada instante de tempo (para cada elemento da sequência de entrada) como resultado, o gradiente de erro resulta ser a soma dos gradientes de erro em cada instante de tempo. Este procedimento é chamado de Backpropagation Through Time (BPTT).\n",
        "\n",
        "No algoritmo de backpropagation, os pesos são ajustados em proporção ao erro de gradiente e para a forma como os gradientes são computados. Se os pesos forem pequenos, isso pode levar a uma situação chamada Vanishing Gradient, onde o sinal de gradiente é tão pequeno que o aprendizado torna-se muito lento ou deixa de funcionar completamente. Se os pesos nesta matriz forem grandes, isso pode levar a uma situação em que o sinal de gradiente é tão grande que pode levar a aprendizagem a divergir. Isso geralmente é referido como Exploding Gradient."
      ]
    },
    {
      "metadata": {
        "id": "bjmmbKfYdfEb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classificador de Imagens com LSTMs Bidirecionais"
      ]
    },
    {
      "metadata": {
        "id": "eTrNlje6dfEd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Embora as RNNs sejam amplamente utilizadas em tarefas de processamento de linguagem, elas podem ser usadas para resolver outros tipos de problemas, como por exemplo classificação de imagens. Neste exercício usaremos LSTMs Bidirecionais no TensorFlow. As RNNs bidirecionais são baseadas na ideia de que a saída no tempo t pode depender de elementos anteriores e futuros na sequência. Para atingir isso, a saída de dois modelos RNNs deve ser misturada - um executa o processo em uma direção e o segundo executa o processo na direção oposta. A rede divide os neurônios de uma RNN regular em duas direções, uma para direção de tempo positivo (forward state) e outra para direção de tempo negativo (backward state). Por esta estrutura, a camada de saída pode obter informações de estados passados e futuros.\n",
        "\n",
        "Usaremos o dataset MNIST e para classificar as imagens usando uma Rede Neural Recorrente, devemos considerar cada linha de imagem como uma sequência de pixels, pois a forma da imagem MNIST é de 28 × 28 pixels, então vamos processar 28 sequências de 28 timesteps para cada amostra."
      ]
    },
    {
      "metadata": {
        "id": "n16sfLILdfEf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7McKv6r6dfEl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "df52e63a-367f-4620-afb0-c7c56a722c45"
      },
      "cell_type": "code",
      "source": [
        "# Importando e carregando o dataset MNIST\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"data/\", one_hot = True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-3ff8b16c8389>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sCL8970xdfE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyperparâmetros de treinamento\n",
        "learning_rate = 0.001\n",
        "training_iters = 100000\n",
        "batch_size = 128\n",
        "display_step = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F9naIAiqdfE8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Parâmetros da rede\n",
        "n_input = 28 \n",
        "n_steps = 28 \n",
        "n_hidden = 128 \n",
        "n_classes = 10 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KCrzne5bdfE_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Definindo x e y como Placeholders\n",
        "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
        "y = tf.placeholder(\"float\", [None, n_classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q8eNh7dbdfFJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Inicializando pesos e bias\n",
        "weights = {'out': tf.Variable(tf.random_normal([2*n_hidden, n_classes]))}\n",
        "biases = {'out': tf.Variable(tf.random_normal([n_classes]))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YX5DlCIedfFQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Classe para o modelo\n",
        "def BiRNN(x, weights, biases):\n",
        "    x = tf.transpose(x, [1, 0, 2])\n",
        "    x = tf.reshape(x, [-1, n_input])\n",
        "    x = tf.split(axis=0, num_or_size_splits=n_steps, value=x)\n",
        "    \n",
        "    # Estamos implementando 2 modelos LSTM, um para forward e outro para backward\n",
        "    lstm_fw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
        "    lstm_bw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
        "    \n",
        "    # Bloco para capturar exceções\n",
        "    try:\n",
        "        # Executa a rede\n",
        "        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x, dtype=tf.float32)\n",
        "    except Exception: \n",
        "        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x, dtype=tf.float32)\n",
        "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2BrAaZhGdfFU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e6eb42ca-e8ba-4e3e-fa68-b184d03187b9"
      },
      "cell_type": "code",
      "source": [
        "# Criando o modelo\n",
        "modelo = BiRNN(x, weights, biases)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-7-4add08107ecf>:7: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qsHHeuiCdfFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "76adf013-5d64-459b-d2d2-19201ef469fd"
      },
      "cell_type": "code",
      "source": [
        "# Função de custo\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = modelo, labels = y))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-670d11ccf81f>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5OJ4E4ErdfFf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Otimizador\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hGz6ExZVdfFi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Avaliação\n",
        "correct_pred = tf.equal(tf.argmax(modelo,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cXUSWtw8dfFq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Incialização das variáveis\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h8ZZVplndfFu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1377
        },
        "outputId": "b069e82e-f683-496e-8102-09c4bc463d14"
      },
      "cell_type": "code",
      "source": [
        "# Sessão TensorFlow\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    step = 1\n",
        "    while step * batch_size < training_iters:\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
        "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
        "        if step % display_step == 0:\n",
        "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
        "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
        "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.6f}\".format(loss) + \", Acurácia em Treino = \" + \\\n",
        "                  \"{:.5f}\".format(acc))\n",
        "        step += 1\n",
        "    print(\"Otimização Finalizada!\")\n",
        "\n",
        "    test_len = 128\n",
        "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
        "    test_label = mnist.test.labels[:test_len]\n",
        "    print(\"Acurácia em Teste:\", \\\n",
        "sess.run(accuracy, feed_dict={x: test_data, y: test_label}))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter 1280, Minibatch Loss= 1.880477, Acurácia em Treino = 0.38281\n",
            "Iter 2560, Minibatch Loss= 1.616861, Acurácia em Treino = 0.44531\n",
            "Iter 3840, Minibatch Loss= 1.343122, Acurácia em Treino = 0.60156\n",
            "Iter 5120, Minibatch Loss= 1.125509, Acurácia em Treino = 0.64062\n",
            "Iter 6400, Minibatch Loss= 0.926978, Acurácia em Treino = 0.67188\n",
            "Iter 7680, Minibatch Loss= 0.770490, Acurácia em Treino = 0.73438\n",
            "Iter 8960, Minibatch Loss= 0.662792, Acurácia em Treino = 0.78906\n",
            "Iter 10240, Minibatch Loss= 0.562921, Acurácia em Treino = 0.82812\n",
            "Iter 11520, Minibatch Loss= 0.562661, Acurácia em Treino = 0.82031\n",
            "Iter 12800, Minibatch Loss= 0.484952, Acurácia em Treino = 0.87500\n",
            "Iter 14080, Minibatch Loss= 0.541616, Acurácia em Treino = 0.82812\n",
            "Iter 15360, Minibatch Loss= 0.575102, Acurácia em Treino = 0.80469\n",
            "Iter 16640, Minibatch Loss= 0.372537, Acurácia em Treino = 0.90625\n",
            "Iter 17920, Minibatch Loss= 0.428721, Acurácia em Treino = 0.82812\n",
            "Iter 19200, Minibatch Loss= 0.402138, Acurácia em Treino = 0.88281\n",
            "Iter 20480, Minibatch Loss= 0.293077, Acurácia em Treino = 0.90625\n",
            "Iter 21760, Minibatch Loss= 0.396002, Acurácia em Treino = 0.87500\n",
            "Iter 23040, Minibatch Loss= 0.347948, Acurácia em Treino = 0.90625\n",
            "Iter 24320, Minibatch Loss= 0.350818, Acurácia em Treino = 0.86719\n",
            "Iter 25600, Minibatch Loss= 0.181742, Acurácia em Treino = 0.95312\n",
            "Iter 26880, Minibatch Loss= 0.263839, Acurácia em Treino = 0.92969\n",
            "Iter 28160, Minibatch Loss= 0.129871, Acurácia em Treino = 0.96094\n",
            "Iter 29440, Minibatch Loss= 0.257625, Acurácia em Treino = 0.89062\n",
            "Iter 30720, Minibatch Loss= 0.242419, Acurácia em Treino = 0.91406\n",
            "Iter 32000, Minibatch Loss= 0.154870, Acurácia em Treino = 0.93750\n",
            "Iter 33280, Minibatch Loss= 0.210211, Acurácia em Treino = 0.93750\n",
            "Iter 34560, Minibatch Loss= 0.169890, Acurácia em Treino = 0.93750\n",
            "Iter 35840, Minibatch Loss= 0.242075, Acurácia em Treino = 0.89844\n",
            "Iter 37120, Minibatch Loss= 0.133909, Acurácia em Treino = 0.94531\n",
            "Iter 38400, Minibatch Loss= 0.212179, Acurácia em Treino = 0.92188\n",
            "Iter 39680, Minibatch Loss= 0.098135, Acurácia em Treino = 0.96875\n",
            "Iter 40960, Minibatch Loss= 0.188111, Acurácia em Treino = 0.95312\n",
            "Iter 42240, Minibatch Loss= 0.281790, Acurácia em Treino = 0.91406\n",
            "Iter 43520, Minibatch Loss= 0.220800, Acurácia em Treino = 0.91406\n",
            "Iter 44800, Minibatch Loss= 0.151360, Acurácia em Treino = 0.96875\n",
            "Iter 46080, Minibatch Loss= 0.110007, Acurácia em Treino = 0.96875\n",
            "Iter 47360, Minibatch Loss= 0.082903, Acurácia em Treino = 0.96875\n",
            "Iter 48640, Minibatch Loss= 0.250729, Acurácia em Treino = 0.92969\n",
            "Iter 49920, Minibatch Loss= 0.110211, Acurácia em Treino = 0.95312\n",
            "Iter 51200, Minibatch Loss= 0.181986, Acurácia em Treino = 0.94531\n",
            "Iter 52480, Minibatch Loss= 0.163767, Acurácia em Treino = 0.96094\n",
            "Iter 53760, Minibatch Loss= 0.137641, Acurácia em Treino = 0.94531\n",
            "Iter 55040, Minibatch Loss= 0.117761, Acurácia em Treino = 0.95312\n",
            "Iter 56320, Minibatch Loss= 0.069129, Acurácia em Treino = 0.96875\n",
            "Iter 57600, Minibatch Loss= 0.135231, Acurácia em Treino = 0.95312\n",
            "Iter 58880, Minibatch Loss= 0.118564, Acurácia em Treino = 0.96094\n",
            "Iter 60160, Minibatch Loss= 0.094008, Acurácia em Treino = 0.95312\n",
            "Iter 61440, Minibatch Loss= 0.134065, Acurácia em Treino = 0.96094\n",
            "Iter 62720, Minibatch Loss= 0.076635, Acurácia em Treino = 0.96094\n",
            "Iter 64000, Minibatch Loss= 0.077454, Acurácia em Treino = 0.96094\n",
            "Iter 65280, Minibatch Loss= 0.110234, Acurácia em Treino = 0.96875\n",
            "Iter 66560, Minibatch Loss= 0.117277, Acurácia em Treino = 0.96094\n",
            "Iter 67840, Minibatch Loss= 0.129652, Acurácia em Treino = 0.96094\n",
            "Iter 69120, Minibatch Loss= 0.095654, Acurácia em Treino = 0.96875\n",
            "Iter 70400, Minibatch Loss= 0.175803, Acurácia em Treino = 0.96094\n",
            "Iter 71680, Minibatch Loss= 0.115240, Acurácia em Treino = 0.95312\n",
            "Iter 72960, Minibatch Loss= 0.171308, Acurácia em Treino = 0.94531\n",
            "Iter 74240, Minibatch Loss= 0.164284, Acurácia em Treino = 0.94531\n",
            "Iter 75520, Minibatch Loss= 0.338355, Acurácia em Treino = 0.90625\n",
            "Iter 76800, Minibatch Loss= 0.050902, Acurácia em Treino = 0.98438\n",
            "Iter 78080, Minibatch Loss= 0.144169, Acurácia em Treino = 0.95312\n",
            "Iter 79360, Minibatch Loss= 0.201189, Acurácia em Treino = 0.92969\n",
            "Iter 80640, Minibatch Loss= 0.087262, Acurácia em Treino = 0.97656\n",
            "Iter 81920, Minibatch Loss= 0.071651, Acurácia em Treino = 0.96875\n",
            "Iter 83200, Minibatch Loss= 0.114223, Acurácia em Treino = 0.95312\n",
            "Iter 84480, Minibatch Loss= 0.124056, Acurácia em Treino = 0.96875\n",
            "Iter 85760, Minibatch Loss= 0.223096, Acurácia em Treino = 0.94531\n",
            "Iter 87040, Minibatch Loss= 0.083248, Acurácia em Treino = 0.97656\n",
            "Iter 88320, Minibatch Loss= 0.033266, Acurácia em Treino = 0.98438\n",
            "Iter 89600, Minibatch Loss= 0.156292, Acurácia em Treino = 0.96875\n",
            "Iter 90880, Minibatch Loss= 0.146878, Acurácia em Treino = 0.94531\n",
            "Iter 92160, Minibatch Loss= 0.176970, Acurácia em Treino = 0.96094\n",
            "Iter 93440, Minibatch Loss= 0.038903, Acurácia em Treino = 0.99219\n",
            "Iter 94720, Minibatch Loss= 0.036564, Acurácia em Treino = 0.99219\n",
            "Iter 96000, Minibatch Loss= 0.045978, Acurácia em Treino = 0.98438\n",
            "Iter 97280, Minibatch Loss= 0.056843, Acurácia em Treino = 0.98438\n",
            "Iter 98560, Minibatch Loss= 0.132609, Acurácia em Treino = 0.96875\n",
            "Iter 99840, Minibatch Loss= 0.061629, Acurácia em Treino = 0.98438\n",
            "Otimização Finalizada!\n",
            "Acurácia em Teste: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iNnhlGcRdfF0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}