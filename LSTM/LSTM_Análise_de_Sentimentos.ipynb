{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM-Análise-de-Sentimentos.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladimiralencar/DeepLearning-LANA/blob/master/LSTM/LSTM_Ana%CC%81lise_de_Sentimentos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "BVXahgKHnERc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inteligência Artificial Para Análise de Sentimentos com LSTMs"
      ]
    },
    {
      "metadata": {
        "id": "ApuBGx8WnERf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Neste Mini-Projeto examinaremos como aplicar técnicas de Deep Learning à tarefa de análise de sentimento. A análise de sentimento pode ser pensada como o exercício de receber uma frase, parágrafo, documento ou qualquer linguagem natural e determinar se o tom emocional desse texto é positivo, negativo ou neutro.\n",
        "\n",
        "Antes de entrar nos detalhes, vamos discutir as razões pelas quais o aprendizado profundo se encaixa nas tarefas de Processamento de Linguagem Natural."
      ]
    },
    {
      "metadata": {
        "id": "72YsHA46nERg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Deep Learning e NLP "
      ]
    },
    {
      "metadata": {
        "id": "jpH1NdITnERh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "O Processamento de Linguagem Natural é a criação de sistemas que processam ou \"entendem\" o idioma para executar determinadas tarefas. Essas tarefas podem incluir:\n",
        "\n",
        "* Responder Perguntas - O principal trabalho de tecnologias como Siri, Alexa e Cortana\n",
        "* Análise de Sentimento - Determinar o tom emocional por trás de um texto\n",
        "* Mapeamento de Texto a Partir de Imagens - Gerar uma legenda para uma imagem \n",
        "* Tradução Automática - Traduzir um parágrafo de texto para outro idioma\n",
        "* Reconhecimento de Voz - Reconhecimento de palavras faladas\n",
        "\n",
        "Antes mesmo da Era do Deep Learning, o PLN já era um campo próspero que via muitos avanços diferentes. No entanto, em todos os êxitos nas tarefas acima mencionadas, era necessário fazer muita engenharia de recursos e, portanto, ter muito conhecimento em linguística e os praticantes precisavam se sentir confortáveis com termos como fonemas e morfemas. Nos últimos anos, o aprendizado profundo teve um progresso incrível e eliminou amplamente o requisito de um forte conhecimento de domínio. Como resultado da menor barreira à entrada, as aplicações para tarefas de PLN se tornaram uma das maiores áreas de pesquisa com Deep Learning."
      ]
    },
    {
      "metadata": {
        "id": "x9INCymjnERi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word Vectors"
      ]
    },
    {
      "metadata": {
        "id": "w4PEkaiXnERj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Para entender como a aprendizagem profunda pode ser aplicada, pense em todas as diferentes formas de dados que são usadas como insumos em modelos de aprendizagem de máquina ou aprendizagem profunda. As redes neurais convolucionais usam matrizes de valores de pixels, a regressão logística usa recursos quantificáveis e os modelos de aprendizado por reforço usam sinais de recompensa. O tema comum é que as insumos precisam ser valores escalares, ou matrizes de valores escalares. Quando você pensa em tarefas de PLN, no entanto, um pipeline de dados como este pode vir à mente.\n",
        " \n",
        "![caption](https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/images/SentimentAnalysis.png)\n",
        "\n",
        "Este tipo de pipeline é problemático. Não há como fazer operações comuns como dot product ou backpropagation em uma única string. Em vez de ter uma entrada de string, precisamos converter cada palavra na frase em um vetor.\n",
        "\n",
        "![caption](https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/images/SentimentAnalysis2.png)\n",
        "\n",
        "Você pode pensar na entrada do módulo de análise de sentimento como sendo uma matriz dimensional de 16 x D."
      ]
    },
    {
      "metadata": {
        "id": "y7DXWIWinERk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Queremos que esses vetores sejam criados de tal forma que representem a palavra e seu contexto, significado e semântica. Por exemplo, gostaríamos que os vetores para as palavras \"amor\" e \"adorar\" residissem em relativamente a mesma área no espaço vetorial, uma vez que ambos possuem definições semelhantes e ambos são usados em contextos semelhantes. A representação vetorial de uma palavra também é conhecida como word embedding.\n",
        "\n",
        "![caption](https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/images/SentimentAnalysis8.png)"
      ]
    },
    {
      "metadata": {
        "id": "yuOZjIl9nERk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word2Vec"
      ]
    },
    {
      "metadata": {
        "id": "gR2WW7kdnERm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Para criar estes word embeddings, usaremos um modelo chamado \"Word2Vec\". Esse modelo cria vetores de palavras, observando o contexto com o qual as palavras aparecem nas frases. As palavras com contextos semelhantes serão colocadas próximas no espaço vetorial. Em linguagem natural, o contexto das palavras pode ser muito importante ao tentar determinar seus significados. Tomando nosso exemplo anterior das palavras \"adorar\" e \"amar\", considere os tipos de frases em que encontraríamos essas palavras.\n",
        "\n",
        "![caption](https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/images/SentimentAnalysis9.png)\n",
        "\n",
        "Do contexto das frases, podemos ver que ambas as palavras são geralmente usadas em frases com conotações positivas e geralmente precedem nomes ou frases nominais. Esta é uma indicação de que ambas as palavras têm algo em comum e possivelmente podem ser sinônimos. O contexto também é muito importante quando se considera a estrutura gramatical em frases. A maioria das frases seguirá os paradigmas tradicionais de ter verbos seguindo os substantivos, os adjetivos precedem os substantivos, e assim por diante. Por esse motivo, o modelo é mais propenso a posicionar substantivos na mesma área geral que outros substantivos. O modelo possui um grande conjunto de dados de frases (Wikipédia em inglês, por exemplo) e produz vetores para cada palavra única no corpus. A saída de um modelo Word2Vec é chamada de embedding matrix.\n",
        "\n",
        "![caption](https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/images/SentimentAnalysis3.png)\n",
        "\n",
        "Esta embedding matrix conterá vetores para cada palavra distinta no corpus de treinamento. Tradicionalmente, as matrizes embedding podem conter mais de 3 milhões de vetores de palavras.\n",
        "\n",
        "O modelo Word2Vec é treinado com cada frase no conjunto de dados, deslizando uma janela de tamanho fixo sobre ela e tentando prever a palavra central da janela, com as outras palavras. Usando uma função de perda e procedimento de otimização, o modelo gera vetores para cada palavra única. Os detalhes desse procedimento de treinamento podem ser um pouco complicados, então vamos ignorar os detalhes por enquanto, mas o principal ponto aqui é que as entradas em qualquer abordagem de Aprendizagem Profunda para uma tarefa de PLN provavelmente terá vetores de palavras como entrada.\n",
        "\n",
        "Para obter mais informações sobre a teoria por trás do Word2Vec e como você cria seus próprios embeddings, confira este [tutorial](https://www.tensorflow.org/tutorials/word2vec) do Tensorflow. Veremos isso em mais detalhes no curso de Processamento de Linguagem Natural."
      ]
    },
    {
      "metadata": {
        "id": "IPGtwjYfnERn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Networks (RNNs)"
      ]
    },
    {
      "metadata": {
        "id": "1Wa2SyOSnERo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora que temos nossos vetores de palavras como entrada, vejamos a arquitetura de rede que vamos construir. O aspecto único dos dados de PLN é que existe um aspecto temporal. Cada palavra em uma frase depende muito do que veio antes e vem depois. Para atender a essa dependência, usamos uma rede neural recorrente.\n",
        "\n",
        "A estrutura de rede neural recorrente é um pouco diferente de uma NN tradicional de feedforward que você já está acostumado a ver. A rede feedforward consiste em nós de entrada, unidades ocultas e nós de saída.\n",
        "\n",
        "A principal diferença entre as redes neurais feedforward e as recorrentes é o aspecto temporal. Em RNNs, cada palavra em uma sequência de entrada será associada a um passo de tempo específico. O número de etapas de tempo será igual ao comprimento máximo da sequência.\n",
        "\n",
        "![caption](https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/images/SentimentAnalysis18.png)\n",
        "\n",
        "Associado a cada passo de tempo há também um novo componente chamado vetor de estado oculto ht. A partir de um nível elevado, este vetor procura encapsular e resumir toda a informação que foi vista nas etapas do tempo anterior. Assim como xt é um vetor que encapsula todas as informações de uma palavra específica, ht é um vetor que resume informações de etapas de tempo anteriores.\n",
        "\n",
        "O estado oculto é uma função do vetor de palavras atual e do vetor de estado oculto no passo de tempo anterior. O sigma indica que a soma dos dois termos será colocada através de uma função de ativação.\n",
        "\n",
        "![caption](https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/images/SentimentAnalysis15.png)\n",
        "\n",
        "Os 2 \"W\" na fórmula acima representam matrizes de peso. Se você olhar atentamente, verá que há uma matriz de peso WX que vamos multiplicar com nossa entrada, e há uma matriz de peso recorrente WH que é multiplicado com o vetor de estado oculto no passo de tempo anterior. WH é uma matriz que permanece igual em todos os passos de tempo, e a matriz de peso WX é diferente para cada entrada.\n",
        "\n",
        "A magnitude dessas matrizes de peso afeta a quantidade que o vetor de estado oculto é afetado pelo vetor atual ou o estado oculto anterior. Como um exercício, dê uma olhada na fórmula acima e considere como ht mudaria se WX ou WH tivessem grandes ou pequenos valores.\n",
        "\n",
        "Vejamos um exemplo rápido. Quando a magnitude de WH é grande e a magnitude de WX é pequena, sabemos que ht é amplamente afetado por ht-1 e não é afetado por xt. Em outras palavras, o atual vetor de estado oculto vê que a palavra atual é em grande parte inconsequente para o resumo geral da frase, e, portanto, assumirá principalmente o mesmo valor que o vetor na etapa anterior.\n",
        "\n",
        "As matrizes de peso são atualizadas através de um processo de otimização chamado Backpropagation Through Time (BPTT).\n",
        "\n",
        "O vetor de estado oculto no passo de tempo final é alimentado em um classificador softmax binário onde é multiplicado por outra matriz de peso e colocada através de uma função softmax que produz valores entre 0 e 1, efetivamente nos dando as probabilidades de sentimentos positivos e negativos.\n",
        "\n",
        "![caption](https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/images/SentimentAnalysis16.png)"
      ]
    },
    {
      "metadata": {
        "id": "YzNgzNn3nERp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Long Short Term Memory Units (LSTMs) "
      ]
    },
    {
      "metadata": {
        "id": "ILssA1fCnERr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As LSTMs são módulos que adicionamos às RNNs. Esses módulos garantem que o vetor de estado oculto h seja capaz de encapsular informações sobre dependências de longo prazo no texto. Como vimos na seção anterior, a formulação para h em RNNs tradicionais é relativamente simples. Essa abordagem não conseguirá efetivamente conectar informações que são separadas por mais de alguns passos de tempo. Podemos ilustrar essa ideia de lidar com dependências de longo prazo através de um exemplo pergunta/resposta. Vejamos o exemplo a seguir.\n",
        "\n",
        "![caption](https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/images/SentimentAnalysis4.png)\n",
        "\n",
        "Aqui, vemos que a frase do meio (The dog ran in the backyard) não teve impacto na pergunta que foi feita. No entanto, existe uma forte conexão entre a primeira e a terceira frases. Com um RNN clássico, o vetor de estado oculto no final da rede pode ter armazenado mais informações sobre a frase do cão do que sobre a primeira frase sobre o número. Basicamente, a adição de unidades LSTM permite determinar a informação correta e útil que precisa ser armazenada no vetor de estado oculto.\n",
        "\n",
        "Olhando para as unidades LSTM de um ponto de vista mais técnico, as unidades recebem o vetor de palavras atual xt e exibem o vetor de estado oculto ht. Nessas unidades, a formulação para ht será um pouco mais complexa do que a de uma RNN típica. O cálculo é dividido em 4 componentes, um portão de entrada, um portão de esquecimento, um portão de saída e um novo recipiente de memória.\n",
        "\n",
        "![caption](https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/images/SentimentAnalysis10.png)\n",
        "\n",
        "Cada portão receberá xt e ht-1 (não mostrado na imagem) como entradas e executará algumas computações sobre elas para obter estados intermediários. Cada estado intermediário é alimentado em diferentes tubulações e, eventualmente, a informação é agregada para formar ht. Vale ressaltar que cada um desses portões pode ser considerado como diferentes módulos dentro do LSTM e cada um tem funções diferentes. O portão de entrada determina a quantidade de ênfase a colocar em cada uma das entradas, o portão de esquecimento determina a informação que descartaremos, e o portão de saída determinará o ht baseado nos estados intermediários. \n",
        "\n",
        "Olhando para o primeiro exemplo com a pergunta \"Qual é a soma dos dois números?\", O modelo teria que ser treinado em tipos e perguntas semelhantes. As unidades LSTM poderiam então perceber que qualquer frase sem números provavelmente não teria impacto na resposta à pergunta e, portanto, a unidade poderá utilizar seu portão de esquecimento para descartar as informações desnecessárias sobre o cão e, ao contrário, manter as informações relativas aos números."
      ]
    },
    {
      "metadata": {
        "id": "Offgjz10nERt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Análise de Sentimento como um Problema de Deep Learning"
      ]
    },
    {
      "metadata": {
        "id": "OUnA3gnMnERu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A análise de sentimento envolve a entrada de uma sequência de palavras e a determinação de que o sentimento é positivo, negativo ou neutro. Podemos separar esta tarefa específica (e a maioria das outras tarefas de PLN) em 5 componentes diferentes.\n",
        "\n",
        "  * 1) Treinar um modelo de geração de vetor de palavras (como Word2Vec) ou carregar vetores de palavras pré-treinados\n",
        "  * 2) Criar uma matriz de ID para o nosso conjunto de treinamento \n",
        "  * 3) Criar o grafo RNN (com unidades LSTM)\n",
        "  * 4) Treinamento\n",
        "  * 5) Teste"
      ]
    },
    {
      "metadata": {
        "id": "bO8-bEzGnERv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Carregando os Dados"
      ]
    },
    {
      "metadata": {
        "id": "jQ6hc5DnnERw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Primeiro, queremos criar nossos vetores de palavras. Por simplicidade, vamos usar um modelo pré-treinado.\n",
        "\n",
        "O Google conseguiu treinar um modelo Word2Vec https://code.google.com/archive/p/word2vec/#Pre-trained_word_and_phrase_vectors em um conjunto de dados maciço do Google News que continha mais de 100 bilhões de palavras diferentes! A partir desse modelo, o Google foi capaz de criar 3 milhões de vetores de palavras, cada um com uma dimensionalidade de 300.\n",
        "\n",
        "Em um cenário ideal, usaríamos esses vetores mas, como a matriz de vetores de palavras é bastante grande (3.6 GB), usaremos uma matriz muito mais gerenciável usando [GloVe] http://nlp.stanford.edu/projects/glove, um modelo de geração de vetor de palavras semelhantes. A matriz conterá 400.000 vetores de palavras, cada um com uma dimensionalidade de 50.\n",
        "\n",
        "Vamos importar duas estruturas de dados diferentes, uma será uma lista Python com as 400.000 palavras, e uma será uma matriz embedding com dimensão de 400.000 x 50 que contenha todos os valores de vetor de palavras."
      ]
    },
    {
      "metadata": {
        "id": "ZFMvXuiB9NKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1139
        },
        "outputId": "81689ae3-e252-40f5-b0df-86ac1f057a05"
      },
      "cell_type": "code",
      "source": [
        "# carrega os arquivos do servidor, depois junta os pedaços e descompacta\n",
        "!rm -Rf training_data\n",
        "!rm -Rf negativeReviews\n",
        "!rm -Rf positiveReviews\n",
        "!rm sample_data\n",
        "!rm *\n",
        "\n",
        "!wget https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/data/s-training_dataaa\n",
        "!wget https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/data/s-training_dataab\n",
        "!wget https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/data/s-training_dataac\n",
        "!wget https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/data/s-training_dataad\n",
        "!cat s-tr* > training_data.tar.gz\n",
        "!tar xzf training_data.tar.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'sample_data': Is a directory\n",
            "rm: cannot remove 'sample_data': Is a directory\n",
            "--2019-01-24 13:26:16--  https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/data/s-training_dataaa\n",
            "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/vladimiralencar/DeepLearning-LANA/master/LSTM/data/s-training_dataaa [following]\n",
            "--2019-01-24 13:26:16--  https://raw.githubusercontent.com/vladimiralencar/DeepLearning-LANA/master/LSTM/data/s-training_dataaa\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25000000 (24M) [application/octet-stream]\n",
            "Saving to: ‘s-training_dataaa’\n",
            "\n",
            "s-training_dataaa   100%[===================>]  23.84M   111MB/s    in 0.2s    \n",
            "\n",
            "2019-01-24 13:26:18 (111 MB/s) - ‘s-training_dataaa’ saved [25000000/25000000]\n",
            "\n",
            "--2019-01-24 13:26:19--  https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/data/s-training_dataab\n",
            "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/vladimiralencar/DeepLearning-LANA/master/LSTM/data/s-training_dataab [following]\n",
            "--2019-01-24 13:26:20--  https://raw.githubusercontent.com/vladimiralencar/DeepLearning-LANA/master/LSTM/data/s-training_dataab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25000000 (24M) [application/octet-stream]\n",
            "Saving to: ‘s-training_dataab’\n",
            "\n",
            "s-training_dataab   100%[===================>]  23.84M   112MB/s    in 0.2s    \n",
            "\n",
            "2019-01-24 13:26:21 (112 MB/s) - ‘s-training_dataab’ saved [25000000/25000000]\n",
            "\n",
            "--2019-01-24 13:26:23--  https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/data/s-training_dataac\n",
            "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/vladimiralencar/DeepLearning-LANA/master/LSTM/data/s-training_dataac [following]\n",
            "--2019-01-24 13:26:24--  https://raw.githubusercontent.com/vladimiralencar/DeepLearning-LANA/master/LSTM/data/s-training_dataac\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25000000 (24M) [application/octet-stream]\n",
            "Saving to: ‘s-training_dataac’\n",
            "\n",
            "s-training_dataac   100%[===================>]  23.84M   113MB/s    in 0.2s    \n",
            "\n",
            "2019-01-24 13:26:25 (113 MB/s) - ‘s-training_dataac’ saved [25000000/25000000]\n",
            "\n",
            "--2019-01-24 13:26:27--  https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/data/s-training_dataad\n",
            "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/vladimiralencar/DeepLearning-LANA/master/LSTM/data/s-training_dataad [following]\n",
            "--2019-01-24 13:26:28--  https://raw.githubusercontent.com/vladimiralencar/DeepLearning-LANA/master/LSTM/data/s-training_dataad\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23090940 (22M) [application/octet-stream]\n",
            "Saving to: ‘s-training_dataad’\n",
            "\n",
            "s-training_dataad   100%[===================>]  22.02M   112MB/s    in 0.2s    \n",
            "\n",
            "2019-01-24 13:26:29 (112 MB/s) - ‘s-training_dataad’ saved [23090940/23090940]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hljAwZXE_7G9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "9e977e0f-575d-4086-8df5-d194f68fcff1"
      },
      "cell_type": "code",
      "source": [
        "!ls -ilah "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 314M\n",
            " 1048583 drwxr-xr-x 1 root root 4.0K Jan 24 13:26 .\n",
            "16908290 drwxr-xr-x 1 root root 4.0K Jan 24 13:02 ..\n",
            " 1048584 drwxr-xr-x 1 root root 4.0K Jan  8 17:14 .config\n",
            " 7627939 -rw-r--r-- 1 1000 1000  24M Mar 13  2017 idsMatrix.npy\n",
            " 7615438 drwxr-xr-x 2 1000 1000 344K Apr 12  2011 negativeReviews\n",
            " 7602937 drwxr-xr-x 2 1000 1000 368K Apr 12  2011 positiveReviews\n",
            "17956872 drwxr-xr-x 1 root root 4.0K Jan  8 17:15 sample_data\n",
            " 7602931 -rw-r--r-- 1 root root  24M Jan 24 13:26 s-training_dataaa\n",
            " 7602933 -rw-r--r-- 1 root root  24M Jan 24 13:26 s-training_dataab\n",
            " 7602934 -rw-r--r-- 1 root root  24M Jan 24 13:26 s-training_dataac\n",
            " 7602935 -rw-r--r-- 1 root root  23M Jan 24 13:26 s-training_dataad\n",
            " 7602936 -rw-r--r-- 1 root root  94M Jan 24 13:26 training_data.tar.gz\n",
            " 7627940 -rw-r--r-- 1 1000 1000  26M Mar 13  2017 wordsList.npy\n",
            " 7627941 -rw-r--r-- 1 1000 1000  77M Mar 23  2017 wordVectors.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lj1-qZ2tnERy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "98371023-4614-46ac-ac72-6b8b82e3ea5d"
      },
      "cell_type": "code",
      "source": [
        "# Carrega a lista de palavras\n",
        "import numpy as np\n",
        "wordsList = np.load('wordsList.npy')\n",
        "print('Lista de Palavras Carregada!')\n",
        "wordsList = wordsList.tolist() \n",
        "wordsList = [word.decode('UTF-8') for word in wordsList] \n",
        "wordsList[:10]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lista de Palavras Carregada!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "F2T62DH1nER4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "c31617ec-6452-4941-f64c-8299b3918855"
      },
      "cell_type": "code",
      "source": [
        "# Carrega os vetores\n",
        "wordVectors = np.load('wordVectors.npy')\n",
        "print ('Vetores de Palavras Carregados!')\n",
        "wordVectors[:3]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vetores de Palavras Carregados!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "       [ 1.3441e-02,  2.3682e-01, -1.6899e-01,  4.0951e-01,  6.3812e-01,\n",
              "         4.7709e-01, -4.2852e-01, -5.5641e-01, -3.6400e-01, -2.3938e-01,\n",
              "         1.3001e-01, -6.3734e-02, -3.9575e-01, -4.8162e-01,  2.3291e-01,\n",
              "         9.0201e-02, -1.3324e-01,  7.8639e-02, -4.1634e-01, -1.5428e-01,\n",
              "         1.0068e-01,  4.8891e-01,  3.1226e-01, -1.2520e-01, -3.7512e-02,\n",
              "        -1.5179e+00,  1.2612e-01, -2.4420e-02, -4.2961e-02, -2.8351e-01,\n",
              "         3.5416e+00, -1.1956e-01, -1.4533e-02, -1.4990e-01,  2.1864e-01,\n",
              "        -3.3412e-01, -1.3872e-01,  3.1806e-01,  7.0358e-01,  4.4858e-01,\n",
              "        -8.0262e-02,  6.3003e-01,  3.2111e-01, -4.6765e-01,  2.2786e-01,\n",
              "         3.6034e-01, -3.7818e-01, -5.6657e-01,  4.4691e-02,  3.0392e-01],\n",
              "       [ 1.5164e-01,  3.0177e-01, -1.6763e-01,  1.7684e-01,  3.1719e-01,\n",
              "         3.3973e-01, -4.3478e-01, -3.1086e-01, -4.4999e-01, -2.9486e-01,\n",
              "         1.6608e-01,  1.1963e-01, -4.1328e-01, -4.2353e-01,  5.9868e-01,\n",
              "         2.8825e-01, -1.1547e-01, -4.1848e-02, -6.7989e-01, -2.5063e-01,\n",
              "         1.8472e-01,  8.6876e-02,  4.6582e-01,  1.5035e-02,  4.3474e-02,\n",
              "        -1.4671e+00, -3.0384e-01, -2.3441e-02,  3.0589e-01, -2.1785e-01,\n",
              "         3.7460e+00,  4.2284e-03, -1.8436e-01, -4.6209e-01,  9.8329e-02,\n",
              "        -1.1907e-01,  2.3919e-01,  1.1610e-01,  4.1705e-01,  5.6763e-02,\n",
              "        -6.3681e-05,  6.8987e-02,  8.7939e-02, -1.0285e-01, -1.3931e-01,\n",
              "         2.2314e-01, -8.0803e-02, -3.5652e-01,  1.6413e-02,  1.0216e-01]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "A087L996nER9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Apenas para ter certeza de que tudo foi carregado corretamente, podemos observar as dimensões da lista de palavras e a matriz de embeddings."
      ]
    },
    {
      "metadata": {
        "id": "PTBQIhuRnER_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c8f7dcbd-7591-4772-f92c-bec112a7458e"
      },
      "cell_type": "code",
      "source": [
        "print(len(wordsList))\n",
        "print(wordVectors.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000\n",
            "(400000, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M0aZ-ql-nESD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Também podemos pesquisar nossa lista de palavras para uma palavra como \"baseball\" e, em seguida, acessar seu vetor correspondente através da matriz de embeddings."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "61bfES6QnESE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "85535f58-e2e5-404a-bd65-256564d9b406"
      },
      "cell_type": "code",
      "source": [
        "baseballIndex = wordsList.index('baseball')\n",
        "wordVectors[baseballIndex]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.9327  ,  1.0421  , -0.78515 ,  0.91033 ,  0.22711 , -0.62158 ,\n",
              "       -1.6493  ,  0.07686 , -0.5868  ,  0.058831,  0.35628 ,  0.68916 ,\n",
              "       -0.50598 ,  0.70473 ,  1.2664  , -0.40031 , -0.020687,  0.80863 ,\n",
              "       -0.90566 , -0.074054, -0.87675 , -0.6291  , -0.12685 ,  0.11524 ,\n",
              "       -0.55685 , -1.6826  , -0.26291 ,  0.22632 ,  0.713   , -1.0828  ,\n",
              "        2.1231  ,  0.49869 ,  0.066711, -0.48226 , -0.17897 ,  0.47699 ,\n",
              "        0.16384 ,  0.16537 , -0.11506 , -0.15962 , -0.94926 , -0.42833 ,\n",
              "       -0.59457 ,  1.3566  , -0.27506 ,  0.19918 , -0.36008 ,  0.55667 ,\n",
              "       -0.70315 ,  0.17157 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "cS6W8zbGnESJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora que temos nossos vetores, nosso primeiro passo é pegar uma sentença de entrada e depois construir a sua representação vetorial. Digamos que temos a frase de entrada \"I thought the movie was incredible and inspiring\". Para obter o vetor das palavras, podemos usar a função de pesquisa de embedding do Tensorflow. Esta função assume dois argumentos, um para a matriz de embeddings (a matriz de vetores de palavras no nosso caso) e um para os ids de cada uma das palavras. O vetor de ids pode ser pensado como a representação integrada do conjunto de treinamento. Este é basicamente apenas o índice de linhas de cada uma das palavras. Vejamos um exemplo rápido para tornar isso concreto."
      ]
    },
    {
      "metadata": {
        "id": "GibHWvBPnESL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cffd2e07-7cf6-47d3-94d2-9c8e779ac065"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Comprimento máximo da sentença\n",
        "maxSeqLength = 10 \n",
        "\n",
        "# Dimensão para cada vetor de palavras\n",
        "numDimensions = 300 \n",
        "\n",
        "firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
        "firstSentence[0] = wordsList.index(\"i\")\n",
        "firstSentence[1] = wordsList.index(\"thought\")\n",
        "firstSentence[2] = wordsList.index(\"the\")\n",
        "firstSentence[3] = wordsList.index(\"movie\")\n",
        "firstSentence[4] = wordsList.index(\"was\")\n",
        "firstSentence[5] = wordsList.index(\"incredible\")\n",
        "firstSentence[6] = wordsList.index(\"and\")\n",
        "firstSentence[7] = wordsList.index(\"inspiring\")\n",
        "\n",
        "# firstSentence[8] e firstSentence[9] serão 0\n",
        "print(firstSentence.shape)\n",
        "\n",
        "# Mostra o índice de linha para cada palavra\n",
        "print(firstSentence) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10,)\n",
            "[    41    804 201534   1005     15   7446      5  13767      0      0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ngll_vw4nESP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "O pipeline de dados pode ser ilustrado abaixo."
      ]
    },
    {
      "metadata": {
        "id": "tA_ZOXaZnESQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**bold text**![caption](https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/images/SentimentAnalysis5.png)"
      ]
    },
    {
      "metadata": {
        "id": "iDPumqqwnESR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A saída de 10 x 50 deve conter os vetores de palavras de 50 dimensões para cada uma das 10 palavras da sequência."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "cWe6e0zunESS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1461ea7-f7c8-47cf-ce9e-b83a7597a303"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zue1JyHLnESW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Antes de criar a matriz de ids para todo o conjunto de treinamento, vamos primeiro visualizar o tipo de dados que temos. Isso nos ajudará a determinar o melhor valor para definir nosso comprimento máximo de sequência. No exemplo anterior, usamos um comprimento máximo de 10, mas esse valor depende em grande parte das entradas que você possui."
      ]
    },
    {
      "metadata": {
        "id": "UtXKXPODnESZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "O conjunto de treinamento que vamos usar é o conjunto de dados da análise de filmes do Imdb. Este conjunto tem 25.000 avaliações de filmes, com 12.500 avaliações positivas e 12.500 críticas negativas. Cada uma das avaliações é armazenada em um arquivo txt que precisamos analisar. Os comentários positivos são armazenados em um diretório e as avaliações negativas são armazenadas em outro. O código a seguir determinará o número total e médio de palavras em cada avaliação."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "gj9rNHimnESa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d3218258-6665-484c-c19e-348b83bf87f2"
      },
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') \n",
        "                 if isfile(join('positiveReviews/', f))]\n",
        "negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') \n",
        "                 if isfile(join('negativeReviews/', f))]\n",
        "numWords = []\n",
        "\n",
        "for pf in positiveFiles:\n",
        "    with open(pf, \"r\", encoding='utf-8') as f:\n",
        "        line=f.readline()\n",
        "        counter = len(line.split())\n",
        "        numWords.append(counter)       \n",
        "print('Arquivos positivos concluídos')\n",
        "\n",
        "for nf in negativeFiles:\n",
        "    with open(nf, \"r\", encoding='utf-8') as f:\n",
        "        line=f.readline()\n",
        "        counter = len(line.split())\n",
        "        numWords.append(counter)  \n",
        "print('Arquivos negativos concluídos')\n",
        "\n",
        "numFiles = len(numWords)\n",
        "print('O número total de arquivos é ', numFiles)\n",
        "print('O número total de palavras nos arquivos é ', sum(numWords))\n",
        "print('O número médio de palavras nos arquivos é ', sum(numWords)/len(numWords))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Arquivos positivos concluídos\n",
            "Arquivos negativos concluídos\n",
            "O número total de arquivos é  25000\n",
            "O número total de palavras nos arquivos é  5844680\n",
            "O número médio de palavras nos arquivos é  233.7872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tADwT3glnESi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Podemos usar a biblioteca Matplotlib para visualizar esses dados em um formato de histograma."
      ]
    },
    {
      "metadata": {
        "id": "zkyeec8onESj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "2a2c627b-f3d0-4fe1-c7ee-c3eb50020ecd"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.hist(numWords, 50)\n",
        "plt.xlabel('Comprimento da Sequência')\n",
        "plt.ylabel('Frequência')\n",
        "plt.axis([0, 1200, 0, 8000])\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEKCAYAAADXdbjqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "wX3UkQxonESq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A partir do histograma, bem como o número médio de palavras por arquivo, podemos dizer com segurança que a maioria das avaliações será inferior a 250 palavras, que é o valor máximo do comprimento da sequência que vamos definir."
      ]
    },
    {
      "metadata": {
        "id": "RFjxbqbpnESs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "maxSeqLength = 250"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sn_-H9MNnESv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Vamos ver como podemos obter um único arquivo e transformá-lo em nossa matriz de ids. Isto é o que parece um dos comentários no formato de arquivo de texto:"
      ]
    },
    {
      "metadata": {
        "id": "WLJO5zr8nESw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0ebf54fd-8738-4806-ea87-32d9fba07911"
      },
      "cell_type": "code",
      "source": [
        "# Pode usar qualquer índice válido(não apenas 3)\n",
        "fname = positiveFiles[3] \n",
        "with open(fname) as f:\n",
        "    for lines in f:\n",
        "        print(lines)\n",
        "        exit"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This film is stunningly beautiful. Goldsworthy's art really benefits with the medium of film because you can see the art at its most beautiful, moving and changing and blossoming. I strongly recommend this movie to everyone. I can think of nothing else to say about it. It's just the kind of movie you HAVE TO see, because it's so visually compelling and left me very refreshed when I left the theatre.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B0jWPMaCnESz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora, vamos converter para uma matriz de ids:"
      ]
    },
    {
      "metadata": {
        "id": "MXvul8XonES0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remove pontuação, parênteses, pontos de interrogação, etc., e deixa apenas caracteres alfanuméricos\n",
        "import re\n",
        "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
        "\n",
        "def cleanSentences(string):\n",
        "    string = string.lower().replace(\"<br />\", \" \")\n",
        "    return re.sub(strip_special_chars, \"\", string.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jYs1-uHAnES3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "e88802ed-8864-4d44-b83c-7aed998a5fdd"
      },
      "cell_type": "code",
      "source": [
        "firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
        "with open(fname) as f:\n",
        "    indexCounter = 0\n",
        "    line=f.readline()\n",
        "    cleanedLine = cleanSentences(line)\n",
        "    split = cleanedLine.split()\n",
        "    for word in split:\n",
        "        try:\n",
        "            firstFile[indexCounter] = wordsList.index(word)\n",
        "        except ValueError:\n",
        "            # Vetor para palavras não conhecidas\n",
        "            firstFile[indexCounter] = 399999 \n",
        "        indexCounter = indexCounter + 1\n",
        "firstFile"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    37,    319,     14,  40709,   3366, 399999,    760,    588,\n",
              "         1913,     17, 201534,   3185,      3,    319,    113,     81,\n",
              "           86,    253, 201534,    760,     22,     47,     96,   3366,\n",
              "         1233,      5,   2803,      5,  39428,     41,   2773,   7546,\n",
              "           37,   1005,      4,   1402,     41,     86,    269,      3,\n",
              "          936,   1726,      4,    203,     59,     20,     47,    120,\n",
              "       201534,    921,      3,   1005,     81,     33,      4,    253,\n",
              "          113,     47,    100,  16674,   8537,      5,    218,    285,\n",
              "          191,  40754,     61,     41,    218, 201534,   2496,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "qUGImRuXnES7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora, vamos fazer o mesmo para cada uma das nossas 25 mil avaliações. Vamos carregar no conjunto de treinamento e obter uma matriz de 25000 x 250. Este é um processo computacionalmente intensivo, então, em vez de executar tudo, vamos carregar uma matriz de ID pré-computada.\n",
        "\n",
        "**** ATENÇÃO: Caso queira recriar a matriz de ids, lembre-se que este procedimento é computacionalmente intensivo ****"
      ]
    },
    {
      "metadata": {
        "id": "0vqufFVBnES8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9054bb10-09cc-4954-a31d-bf21894a7258"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
        "fileCounter = 0\n",
        "\n",
        "for pf in positiveFiles:\n",
        "    with open(pf, \"r\") as f:\n",
        "        indexCounter = 0\n",
        "        line=f.readline()\n",
        "        cleanedLine = cleanSentences(line)\n",
        "        split = cleanedLine.split()\n",
        "        for word in split:\n",
        "            try:\n",
        "                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
        "            except ValueError:\n",
        "                # Vetor para palavras não conhecidas\n",
        "                ids[fileCounter][indexCounter] = 399999 \n",
        "            indexCounter = indexCounter + 1\n",
        "            if indexCounter >= maxSeqLength:\n",
        "                break\n",
        "        fileCounter = fileCounter + 1 \n",
        "\n",
        "for nf in negativeFiles:\n",
        "    with open(nf, \"r\") as f:\n",
        "        indexCounter = 0\n",
        "        line=f.readline()\n",
        "        cleanedLine = cleanSentences(line)\n",
        "        split = cleanedLine.split()\n",
        "        for word in split:\n",
        "            try:\n",
        "                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
        "            except ValueError:\n",
        "                # Vetor para palavras não conhecidas\n",
        "                ids[fileCounter][indexCounter] = 399999 \n",
        "            indexCounter = indexCounter + 1\n",
        "            if indexCounter >= maxSeqLength:\n",
        "                break\n",
        "        fileCounter = fileCounter + 1 \n",
        "\n",
        "np.save('/media/datasets/DeepLearningII/Cap03/training_data/idsMatrix', ids)\n",
        "'''"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nids = np.zeros((numFiles, maxSeqLength), dtype=\\'int32\\')\\nfileCounter = 0\\n\\nfor pf in positiveFiles:\\n    with open(pf, \"r\") as f:\\n        indexCounter = 0\\n        line=f.readline()\\n        cleanedLine = cleanSentences(line)\\n        split = cleanedLine.split()\\n        for word in split:\\n            try:\\n                ids[fileCounter][indexCounter] = wordsList.index(word)\\n            except ValueError:\\n                # Vetor para palavras não conhecidas\\n                ids[fileCounter][indexCounter] = 399999 \\n            indexCounter = indexCounter + 1\\n            if indexCounter >= maxSeqLength:\\n                break\\n        fileCounter = fileCounter + 1 \\n\\nfor nf in negativeFiles:\\n    with open(nf, \"r\") as f:\\n        indexCounter = 0\\n        line=f.readline()\\n        cleanedLine = cleanSentences(line)\\n        split = cleanedLine.split()\\n        for word in split:\\n            try:\\n                ids[fileCounter][indexCounter] = wordsList.index(word)\\n            except ValueError:\\n                # Vetor para palavras não conhecidas\\n                ids[fileCounter][indexCounter] = 399999 \\n            indexCounter = indexCounter + 1\\n            if indexCounter >= maxSeqLength:\\n                break\\n        fileCounter = fileCounter + 1 \\n\\nnp.save(\\'/media/datasets/DeepLearningII/Cap03/training_data/idsMatrix\\', ids)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "1WBWnVUwnES_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Carregando a matriz de ids já computada\n",
        "ids = np.load('idsMatrix.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BdF3053OnETC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Funções Auxiliares"
      ]
    },
    {
      "metadata": {
        "id": "EXxNKMlrnETD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Abaixo, temos algumas funções auxiliares que serão úteis ao treinar a rede na etapa posterior."
      ]
    },
    {
      "metadata": {
        "id": "fcxzymGynETE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "\n",
        "def getTrainBatch():\n",
        "    labels = []\n",
        "    arr = np.zeros([batchSize, maxSeqLength])\n",
        "    for i in range(batchSize):\n",
        "        if (i % 2 == 0): \n",
        "            num = randint(1,11499)\n",
        "            labels.append([1,0])\n",
        "        else:\n",
        "            num = randint(13499,24999)\n",
        "            labels.append([0,1])\n",
        "        arr[i] = ids[num-1:num]\n",
        "    return arr, labels\n",
        "\n",
        "def getTestBatch():\n",
        "    labels = []\n",
        "    arr = np.zeros([batchSize, maxSeqLength])\n",
        "    for i in range(batchSize):\n",
        "        num = randint(11499,13499)\n",
        "        if (num <= 12499):\n",
        "            labels.append([1,0])\n",
        "        else:\n",
        "            labels.append([0,1])\n",
        "        arr[i] = ids[num-1:num]\n",
        "    return arr, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qix1nAHcnETI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Modelo RNN"
      ]
    },
    {
      "metadata": {
        "id": "8Q3kQ_Y8nETJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora, estamos prontos para começar a criar o nosso grafo Tensorflow. Primeiro, precisaremos definir alguns hiperparâmetros, como tamanho do lote, número de unidades LSTM, número de classes de saída e número de iterações de treinamento."
      ]
    },
    {
      "metadata": {
        "id": "goe_r2ZonETK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batchSize = 24\n",
        "lstmUnits = 64\n",
        "numClasses = 2\n",
        "iterations = 100000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QgWAFvbDnETM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tal como acontece com a maioria dos grafos do Tensorflow, agora precisamos especificar dois placeholders, um para as entradas na rede e outro para os rótulos. A parte mais importante sobre a definição desses placeholders é a compreensão de cada uma das suas dimensões."
      ]
    },
    {
      "metadata": {
        "id": "xk9R8PIanETP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "O placeholder de labels representa um conjunto de valores, cada um [1, 0] ou [0, 1], dependendo se cada exemplo de treinamento é positivo ou negativo. Cada linha no espaço reservado de entrada inteira representa a representação inteira de cada exemplo de treinamento que incluímos em nosso lote."
      ]
    },
    {
      "metadata": {
        "id": "3lmPQoeMnETQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![caption](https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/images/SentimentAnalysis12.png)"
      ]
    },
    {
      "metadata": {
        "id": "ov5RHkROnETT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "\n",
        "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
        "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9momyTocnETb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Uma vez que temos nosso placeholder de dados de entrada, vamos chamar a função tf.nn.lookup() para obter nossos vetores de palavras. A chamada para essa função retornará um Tensor 3-D do tamanho do lote de dimensionalidade pelo comprimento máximo da sequência por dimensões de vetor de palavras. Para visualizar este tensor 3-D, você pode simplesmente pensar em cada ponto de dados no tensor de entrada como o vetor D dimensional correspondente ao qual ele se refere."
      ]
    },
    {
      "metadata": {
        "id": "MCQO1ZBsnETd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![caption](https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/images/SentimentAnalysis13.png)"
      ]
    },
    {
      "metadata": {
        "id": "7Ir04YQKnETf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
        "data = tf.nn.embedding_lookup(wordVectors,input_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IhX0hc0xnETh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora que temos os dados no formato que queremos, vejamos como podemos alimentar esta entrada em uma rede LSTM. Usaremos a função tf.nn.rnn_cell.BasicLSTMCell. Esta função recebe um número inteiro para o número de unidades LSTM que queremos. Este é um dos hiperparâmetros que vai precisar de algum ajuste para descobrir o valor ideal. Em seguida, vamos colocar essa célula LSTM em uma camada de Dropout para ajudar a evitar o overfitting da rede."
      ]
    },
    {
      "metadata": {
        "id": "ZvAW-mP-nETi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finalmente, alimentaremos tanto a célula LSTM quanto o tensor 3-D com dados de entrada em uma função chamada tf.nn.dynamic_rnn. Esta função é responsável por \"desenrolar\" toda a rede e criar um caminho para que os dados fluam através do grafo RNN."
      ]
    },
    {
      "metadata": {
        "id": "kQWy15FInETi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "1c45b882-037f-47d1-85fd-1ec265802716"
      },
      "cell_type": "code",
      "source": [
        "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
        "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
        "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-23-db6a6fc2c55e>:1: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3VvF96IOnETk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Como observação, outra escolha de arquitetura de rede mais avançada é empilhar várias células LSTM uma sobre a outra (Stacked LSTM). Este é o lugar onde o último vetor de estado oculto do primeiro LSTM alimenta o segundo. Empilhar essas células é uma ótima maneira de ajudar o modelo a reter mais informações de dependência a longo prazo, mas também introduz mais parâmetros no modelo, possivelmente aumentando o tempo de treinamento, a necessidade de exemplos de treinamento adicionais e a chance de overfitting. Para obter mais informações sobre como você pode adicionar LSTMs empilhados ao seu modelo, confira a excelente [documentação] do Tensorflow (https://www.tensorflow.org/tutorials/recurrent#stacking_multiple_lstms)."
      ]
    },
    {
      "metadata": {
        "id": "l1wCNG_knETl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A primeira saída da função RNN dinâmica pode ser considerada como o último vetor de estado oculto. Este vetor será remodelado e depois multiplicado por uma matriz de peso final e um termo de bias para obter os valores de saída final."
      ]
    },
    {
      "metadata": {
        "id": "fLA4HwUlnETm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
        "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
        "value = tf.transpose(value, [1, 0, 2])\n",
        "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
        "prediction = (tf.matmul(last, weight) + bias)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5hMBYLBdnETp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Em seguida, definiremos as métricas corretas de previsão e precisão para acompanhar o funcionamento da rede. A formulação de predição correta funciona observando o índice do valor máximo dos 2 valores de saída e, em seguida, ver se ele coincide com os rótulos de treinamento."
      ]
    },
    {
      "metadata": {
        "id": "SQ2m7a7GnETr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0dKkTu13nET0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Definiremos uma perda padrão de entropia cruzada com uma camada de softmax colocada em cima dos valores de previsão final. Para o otimizador, usaremos Adam e a taxa de aprendizado padrão de .001."
      ]
    },
    {
      "metadata": {
        "id": "BnYJjjkPnET1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "1b8b8b0d-a5dc-4f8c-8ef0-1b0bfd3976a1"
      },
      "cell_type": "code",
      "source": [
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
        "optimizer = tf.train.AdamOptimizer().minimize(loss)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-26-773132a4e1b5>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VnWoJV6onET5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Se você quiser usar o Tensorboard para visualizar os valores de perda e precisão, você também pode executar e modificar o seguinte código."
      ]
    },
    {
      "metadata": {
        "id": "t6aLiAY7nET7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "tf.summary.scalar('Loss', loss)\n",
        "tf.summary.scalar('Accuracy', accuracy)\n",
        "merged = tf.summary.merge_all()\n",
        "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
        "writer = tf.summary.FileWriter(logdir, sess.graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gu1VyaYdnEUB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tuning dos Hiperparâmetros"
      ]
    },
    {
      "metadata": {
        "id": "A5tj-SvznEUC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Escolher os valores certos para seus hiperparâmetros é uma parte crucial do treinamento de redes neurais profundas de forma eficaz. Você vai perceber que suas curvas de perda de treinamento podem variar com sua escolha de otimizador (Adam, Adadelta, SGD, etc.), taxa de aprendizado e arquitetura de rede. Com RNNs e LSTMs em particular, alguns outros fatores importantes incluem o número de unidades LSTM e o tamanho da palavra vetores.\n",
        "\n",
        "* Taxa de Aprendizagem: RNNs são infames por serem difíceis de treinar por causa do grande número de etapas de tempo que eles têm. A taxa de aprendizagem torna-se extremamente importante, uma vez que não queremos que nossos valores de peso flutuem descontroladamente como resultado de uma grande taxa de aprendizado, nem queremos um processo de treinamento lento devido a uma baixa taxa de aprendizado. O valor padrão de 0.001 é um bom lugar para começar. Você deve aumentar esse valor se a perda de treinamento estiver mudando muito devagar e diminuir se a perda for instável.\n",
        "* Otimizador: não há uma escolha de consenso entre os pesquisadores, mas Adam tem sido amplamente popular por ter a propriedade da taxa de aprendizagem adaptável (tenha em mente que as taxas de aprendizado ótimas podem diferir com a escolha do otimizador).\n",
        "* Número de unidades LSTM: Este valor depende em grande parte do comprimento médio de seus textos de entrada. Enquanto um número maior de unidades fornece mais expressibilidade para o modelo e permite que o modelo armazene mais informações para textos mais longos, a rede levará mais tempo para treinar e será computacionalmente dispendiosa.\n",
        "* Tamanho do vetor de palavras: as dimensões para vetores de palavras geralmente variam de 50 a 300. Um tamanho maior significa que o vetor pode encapsular mais informações sobre a palavra, mas você também deve esperar um modelo mais computacionalmente caro.\n"
      ]
    },
    {
      "metadata": {
        "id": "zf6djCXLnEUD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Treinamento"
      ]
    },
    {
      "metadata": {
        "id": "AF9ZqQlhnEUE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A idéia básica do loop de treinamento é que primeiro definimos uma sessão Tensorflow. Então, carregamos um lote de avaliações e os respectivos rótulos associados. Em seguida, chamamos a função `run` da sessão. Esta função tem dois argumentos. O primeiro é chamado de argumento \"buscar\". Ele define o valor que nos interessa em computação. Queremos que o nosso otimizador seja computado, pois esse é o componente que minimiza a nossa função de perda. O segundo argumento é onde nós inserimos o `feed_dict`. Esta estrutura de dados é onde fornecemos insumos para todos os nossos espaços reservados (placeholders). Precisamos alimentar nosso lote de avaliações e nosso lote de rótulos. Este ciclo é então repetido para um número definido de iterações de treinamento."
      ]
    },
    {
      "metadata": {
        "id": "j2h3P8JnnEUF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Em vez de treinar a rede neste notebook (que levará pelo menos um par de horas), iremos carregar um modelo pré-treinado.\n",
        "\n",
        "Se você decidir treinar este notebook em sua própria máquina, note que você pode acompanhar seu progresso usando [TensorBoard] (https://www.tensorflow.org/get_started/summaries_and_tensorboard). Enquanto a seguinte célula estiver em execução, use o seu terminal para entrar no diretório que contém este notebook, entre `tensorboard --logdir = tensorboard` e visite http://localhost:6006/ com um navegador para manter o olho no seu progresso de treinamento ."
      ]
    },
    {
      "metadata": {
        "id": "NV8qY0RznEUF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "598c14ed-569a-4b0e-8159-7f6752f2ff63"
      },
      "cell_type": "code",
      "source": [
        "sess = tf.InteractiveSession()\n",
        "saver = tf.train.Saver()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(iterations):\n",
        "    # Próximo Batch de Reviews\n",
        "    nextBatch, nextBatchLabels = getTrainBatch();\n",
        "    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
        "   \n",
        "    # Gravando Sumário para o Tensorboard\n",
        "    if (i % 50 == 0):\n",
        "        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
        "        writer.add_summary(summary, i)\n",
        "\n",
        "    # Checkpoint a cada 10,000 iterações de treinamento\n",
        "    if (i % 10000 == 0 and i != 0):\n",
        "        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
        "        print(\"Salvo em %s\" % save_path)\n",
        "writer.close()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Salvo em models/pretrained_lstm.ckpt-10000\n",
            "Salvo em models/pretrained_lstm.ckpt-20000\n",
            "Salvo em models/pretrained_lstm.ckpt-30000\n",
            "Salvo em models/pretrained_lstm.ckpt-40000\n",
            "Salvo em models/pretrained_lstm.ckpt-50000\n",
            "Salvo em models/pretrained_lstm.ckpt-60000\n",
            "Salvo em models/pretrained_lstm.ckpt-70000\n",
            "Salvo em models/pretrained_lstm.ckpt-80000\n",
            "Salvo em models/pretrained_lstm.ckpt-90000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f_GDCQfcnEUK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Carregando um Modelo Pré-treinado"
      ]
    },
    {
      "metadata": {
        "id": "YwqaF_IHnEUK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As curvas de precisão e perda de nosso modelo durante o treinamento podem ser encontradas abaixo."
      ]
    },
    {
      "metadata": {
        "id": "mlRzJGXfnEUL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![caption](https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/images/SentimentAnalysis6.png)\n",
        "![caption](https://github.com/vladimiralencar/DeepLearning-LANA/raw/master/LSTM/images/SentimentAnalysis7.png)"
      ]
    },
    {
      "metadata": {
        "id": "zGcAdZkWnEUL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Olhando para as curvas de treinamento acima, parece que o treinamento do modelo está indo bem. A perda está diminuindo de forma constante, e a precisão aproxima-se de 100%. No entanto, ao analisar as curvas de treinamento, devemos também prestar especial atenção à possibilidade de o nosso modelo sofrer overfitting o conjunto de dados de treinamento. O overfitting é um fenômeno comum na aprendizagem de máquinas, onde um modelo torna-se tão adequado aos dados de treinamento que perde a capacidade de generalizar para o conjunto de testes. Isso significa que o treinamento de uma rede até alcançar 0 perda de treinamento pode não ser a melhor maneira de obter um modelo preciso que tenha bom desempenho em dados que nunca antes viu. A parada precoce (Early Stopping) é uma técnica intuitiva comumente usada com as redes LSTM para combater esta questão. A ideia básica é que treinamos o modelo em nosso conjunto de treinamento, enquanto também mede seu desempenho no conjunto de testes de vez em quando. Uma vez que o erro de teste para a sua diminuição constante e começa a aumentar em vez disso, interrompemos o treinamento, já que isso é um sinal de que a rede começou a sofrer de overfitting."
      ]
    },
    {
      "metadata": {
        "id": "H6Tav6UYnEUN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Carregar um modelo pré-treinado envolve a definição de outra sessão Tensorflow, criando um objeto Saver e, em seguida, usando esse objeto para chamar a função de restauração. Esta função recebe em 2 argumentos, um para a sessão atual e outro para o nome do modelo salvo."
      ]
    },
    {
      "metadata": {
        "id": "AfOT4QZ4nEUO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "4632a363-21aa-4d77-f7e6-21b526b513ef"
      },
      "cell_type": "code",
      "source": [
        "sess = tf.InteractiveSession()\n",
        "saver = tf.train.Saver()\n",
        "saver.restore(sess, tf.train.latest_checkpoint('models'))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from models/pretrained_lstm.ckpt-90000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "vv2wAtqxnEUT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Então, vamos carregar algumas avaliações de filmes do nosso conjunto de testes.A precisão de cada lote de teste pode ser observada quando você executa o seguinte código."
      ]
    },
    {
      "metadata": {
        "id": "aJGRJZBLnEUT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "99979c38-14ea-42be-d76b-b8ceb3ee277e"
      },
      "cell_type": "code",
      "source": [
        "iterations = 10\n",
        "for i in range(iterations):\n",
        "    nextBatch, nextBatchLabels = getTestBatch();\n",
        "    print(\"Acurácia para este batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acurácia para este batch: 70.83333134651184\n",
            "Acurácia para este batch: 87.5\n",
            "Acurácia para este batch: 95.83333134651184\n",
            "Acurácia para este batch: 66.66666865348816\n",
            "Acurácia para este batch: 91.66666865348816\n",
            "Acurácia para este batch: 79.16666865348816\n",
            "Acurácia para este batch: 75.0\n",
            "Acurácia para este batch: 87.5\n",
            "Acurácia para este batch: 79.16666865348816\n",
            "Acurácia para este batch: 79.16666865348816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2UaiFVyTnEUW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "y6Ix-_W5nEUX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conclusão"
      ]
    },
    {
      "metadata": {
        "id": "F2mo4v0gnEUY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Neste Jupyter Notebook, vimos uma abordagem de aprendizado profundo para a análise do sentimento. Analisamos os diferentes componentes envolvidos em todo o pipeline e depois examinamos o processo de construção do código Tensorflow para implementar o modelo na prática. Finalmente, treinamos e testamos o modelo para classificar as críticas dos filmes.\n",
        "\n",
        "Com a ajuda do Tensorflow, você pode criar seus próprios classificadores de sentimentos para entender as grandes quantidades de linguagem natural do mundo e usar os resultados para formar insights acionáveis. "
      ]
    }
  ]
}