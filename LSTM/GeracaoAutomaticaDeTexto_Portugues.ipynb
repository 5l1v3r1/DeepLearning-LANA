{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GeracaoAutomaticaDeTexto-Portugues-Copy1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladimiralencar/DeepLearning-LANA/blob/master/LSTM/GeracaoAutomaticaDeTexto_Portugues.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jpfguOONcQjX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Geração Automática de Texto com LSTMs - Português"
      ]
    },
    {
      "metadata": {
        "id": "xdRQKCrocQjZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As Redes Neurais Recorrentes também podem ser usadas como modelos generativos. Isso significa que além de serem usadas como modelos preditivos (fazendo previsões), elas podem aprender as sequências de um problema e, em seguida, gerar sequências plausíveis inteiramente novas para o domínio do problema. Modelos Generativos como este são úteis não apenas para estudar o quão bem um modelo aprendeu um problema, mas para saber mais sobre o próprio domínio do problema. "
      ]
    },
    {
      "metadata": {
        "id": "eHHuOH1vcQja",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Uma boa forma de praticar a criação de texto, é usando livros clássicos, os quais já temos uma boa ideia sobre a história e que não estejamos violando nenhum direito de copyright. Muitos livros clássicos já não possuem mais restrição de uso e podem ser usados gratuitamente. Um bom lugar para encontrar esses livros é no site do Projeto Gutenberg. É de lá que usaremos o livro para o qual criaremos um modelo generativo: Alice no País das Maravilhas ou o nome em inglês Alice's Adventures in Wonderland. O arquivo txt do livro pode ser baixado aqui: https://www.gutenberg.org/ebooks/11. \n",
        "Este livro tem cerca de 3.300 linhas de texto. O cabeçalho e a marca de final de arquivo foram removidos, já que não são necessários para o que vamos fazer."
      ]
    },
    {
      "metadata": {
        "id": "yl1mwyXMcQjc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Vamos aprender as dependências entre os caracteres e as probabilidades condicionais de caracteres em sequências para que possamos gerar sequências totalmente novas e originais de caracteres. Esta é uma tarefa divertida e recomendo repetir essas experiências com outros livros do Projeto Gutenberg. Essas experiências não se limitam ao texto, você também pode experimentar com outros dados ASCII, como código fonte de linguagens de programação, documentos marcados em LaTeX, HTML ou Markdown e muito mais. \n",
        "\n",
        "Faremos aqui algo muito similar ao que foi feito pelo programador, que escreveu um novo livro de Game ofthrones: http://www.businessinsider.com/ai-just-wrote-the-next-book-of-game-of-thrones-for-us-2017-8"
      ]
    },
    {
      "metadata": {
        "id": "IDTodJ00cQje",
        "colab_type": "code",
        "colab": {},
        "outputId": "dd3a3b6f-7905-456e-bf2a-645116362c25"
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy\n",
        "import sys\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "bC53zESacQjn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Carregamos os dados e convertemos para lowercase \n",
        "# Estamos usando aqui arquivo texto no formato ASCII\n",
        "filename = \"O_Alienista.txt\" # livro de Machado de Assis\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RArAOlfLcQjr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora que o livro está carregado, devemos preparar os dados para modelagem. Não podemos modelar os caracteres diretamente, em vez disso, devemos converter os caracteres em números inteiros. Podemos fazer isso facilmente, criando um conjunto de todos os caracteres distintos do livro, então criando um mapa de cada caractere para um único inteiro."
      ]
    },
    {
      "metadata": {
        "id": "B02Sj47vcQjt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Criando o mapeamento caracter/inteiro\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "COTFYyDacQjy",
        "colab_type": "code",
        "colab": {},
        "outputId": "efa2e66c-1ef8-416b-c75e-2679f07d9c75"
      },
      "cell_type": "code",
      "source": [
        "chars"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " '(',\n",
              " ')',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'x',\n",
              " 'z',\n",
              " '§',\n",
              " '°',\n",
              " 'º',\n",
              " 'à',\n",
              " 'á',\n",
              " 'â',\n",
              " 'ã',\n",
              " 'ç',\n",
              " 'é',\n",
              " 'ê',\n",
              " 'í',\n",
              " 'ò',\n",
              " 'ó',\n",
              " 'ô',\n",
              " 'õ',\n",
              " 'ú',\n",
              " 'ü',\n",
              " '—',\n",
              " '’',\n",
              " '“',\n",
              " '”']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "MbjaRG3NcQj3",
        "colab_type": "code",
        "colab": {},
        "outputId": "580fc1b2-6ee7-41cd-f5fb-010e40368377"
      },
      "cell_type": "code",
      "source": [
        "char_to_int"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " '(': 4,\n",
              " ')': 5,\n",
              " ',': 6,\n",
              " '-': 7,\n",
              " '.': 8,\n",
              " '0': 9,\n",
              " '1': 10,\n",
              " '2': 11,\n",
              " '3': 12,\n",
              " '4': 13,\n",
              " '5': 14,\n",
              " '6': 15,\n",
              " '7': 16,\n",
              " '8': 17,\n",
              " '9': 18,\n",
              " ':': 19,\n",
              " ';': 20,\n",
              " '?': 21,\n",
              " 'a': 22,\n",
              " 'b': 23,\n",
              " 'c': 24,\n",
              " 'd': 25,\n",
              " 'e': 26,\n",
              " 'f': 27,\n",
              " 'g': 28,\n",
              " 'h': 29,\n",
              " 'i': 30,\n",
              " 'j': 31,\n",
              " 'l': 32,\n",
              " 'm': 33,\n",
              " 'n': 34,\n",
              " 'o': 35,\n",
              " 'p': 36,\n",
              " 'q': 37,\n",
              " 'r': 38,\n",
              " 's': 39,\n",
              " 't': 40,\n",
              " 'u': 41,\n",
              " 'v': 42,\n",
              " 'x': 43,\n",
              " 'z': 44,\n",
              " '§': 45,\n",
              " '°': 46,\n",
              " 'º': 47,\n",
              " 'à': 48,\n",
              " 'á': 49,\n",
              " 'â': 50,\n",
              " 'ã': 51,\n",
              " 'ç': 52,\n",
              " 'é': 53,\n",
              " 'ê': 54,\n",
              " 'í': 55,\n",
              " 'ò': 56,\n",
              " 'ó': 57,\n",
              " 'ô': 58,\n",
              " 'õ': 59,\n",
              " 'ú': 60,\n",
              " 'ü': 61,\n",
              " '—': 62,\n",
              " '’': 63,\n",
              " '“': 64,\n",
              " '”': 65}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "WWQYruc6cQj8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pode haver alguns caracteres que podemos remover para limpar mais o conjunto de dados que reduzirá o vocabulário e poderá melhorar o processo de modelagem. "
      ]
    },
    {
      "metadata": {
        "id": "i9CEE3uycQj-",
        "colab_type": "code",
        "colab": {},
        "outputId": "2cefc66b-96e6-4da5-a6d4-7045828e147f"
      },
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: {:,d}\".format(n_chars))  \n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters: 99,080\n",
            "Total Vocab:  66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EfcW82pacQkE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Podemos ver que o livro tem pouco mais de 99.000 caracteres e que quando convertidos para minúsculas, existem apenas 44 caracteres distintos no vocabulário para a rede aprender, muito mais do que os 26 no alfabeto. Agora, precisamos definir os dados de treinamento para a rede. Existe muita flexibilidade em como você escolhe dividir o texto e expô-lo a rede durante o treino. Aqui dividiremos o texto do livro em subsequências com um comprimento de 100 caracteres, um comprimento arbitrário. Poderíamos facilmente dividir os dados por sentenças e ajustar as sequências mais curtas e truncar as mais longas. Cada padrão de treinamento da rede é composto de 100 passos de tempo (time steps) de um caractere (X) seguido por um caracter de saída (y). Ao criar essas sequências, deslizamos esta janela ao longo de todo o livro um caracter de cada vez, permitindo que cada caracter tenha a chance de ser aprendido a partir dos 100 caracteres que o precederam (exceto os primeiros 100 caracteres, é claro). Por exemplo, se o comprimento da sequência é 5 (para simplificar), os dois primeiros padrões de treinamento seriam os seguintes:\n",
        "\n",
        "* Palavra: CHAPTER\n",
        "* CHAPT -> E\n",
        "* HAPTE -> R"
      ]
    },
    {
      "metadata": {
        "id": "FxnSFcwQcQkG",
        "colab_type": "code",
        "colab": {},
        "outputId": "681323d3-88e4-4a76-86cb-e44620602f66"
      },
      "cell_type": "code",
      "source": [
        "# À medida que dividimos o livro em sequências, convertemos os caracteres em números inteiros usando nossa\n",
        "# tabela de pesquisa que preparamos anteriormente.\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = raw_text[i:i + seq_length]\n",
        "    seq_out = raw_text[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total de Padrões: \", n_patterns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de Padrões:  98980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OdzyL7FUcQkK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora que preparamos nossos dados de treinamento, precisamos transformá-lo para que possamos usá-lo com o Keras. Primeiro, devemos transformar a lista de sequências de entrada na forma [amostras, passos de tempo, recursos] esperados por uma rede LSTM. Em seguida, precisamos redimensionar os números inteiros para o intervalo de 0 a 1 para tornar os padrões mais fáceis de aprender pela rede LSTM que usa a função de ativação sigmoide por padrão."
      ]
    },
    {
      "metadata": {
        "id": "vqBouJmbcQkL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reshape de X para [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# Normalização\n",
        "X = X / float(n_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mZjebOS5cQkQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finalmente, precisamos converter os padrões de saída (caracteres únicos convertidos em números inteiros) usando Hot-Encoding. Isto é para que possamos configurar a rede para prever a probabilidade de cada um dos 44 caracteres diferentes no vocabulário (uma representação mais fácil) em vez de tentar forçá-lo a prever com precisão o próximo caracter. Cada valor de y é convertido em um vetor com um comprimento 66, cheio de zeros, exceto com um 1 na coluna para a letra (inteiro) que o padrão representa. Por exemplo, quando a letra n (valor inteiro 30) tiver sido transformada usando One-Hot Encoding, vai se parecer com isso:\n",
        "\n",
        "[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
      ]
    },
    {
      "metadata": {
        "id": "rNubH9OQcQkS",
        "colab_type": "code",
        "colab": {},
        "outputId": "8b27ecac-b0bb-4d61-ca92-febe35949459"
      },
      "cell_type": "code",
      "source": [
        "# One-Hot Encoding da variável de saída\n",
        "y = np_utils.to_categorical(dataY)\n",
        "y[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
              "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "sHEj28W7cQkX",
        "colab_type": "code",
        "colab": {},
        "outputId": "41f35a9d-7f80-489d-861b-a4e24d35c0f2"
      },
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(98980, 100, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "YEfXaxU2cQkc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Modelo LSTM com duas camadas de Dropout com 20%\n",
        "# O tempo de treinamento é bem longo - \n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OvAcux8PcQkh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Não há conjunto de dados de teste. Estamos modelando todo o conjunto de dados de treinamento para aprender a probabilidade de cada caracter em uma sequência. Não estamos interessados nos mais preciso modelo do conjunto de dados de treinamento (Acurácia de Classificação). Este seria um modelo que prevê cada caracter no conjunto de dados de treinamento perfeitamente. Em vez disso, estamos interessados em uma generalização do conjunto de dados que minimiza a função de perda escolhida. Estamos buscando um equilíbrio entre generalização e\n",
        "overfitting."
      ]
    },
    {
      "metadata": {
        "id": "LKBtpeKUcQki",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define o checkpoint\n",
        "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aLsjii0kcQkw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "dt = datetime.now()\n",
        "for i in range(1000000): i +1\n",
        "print(\"tempo de execução\", datetime.now() - dt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9-avEp-HcQk5",
        "colab_type": "code",
        "colab": {},
        "outputId": "260ab723-032e-4035-a645-bcc6fedff57b"
      },
      "cell_type": "code",
      "source": [
        "!ls w*.hdf5-Portuguese"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights-improvement-00-1.4191.hdf5-Portuguese\r\n",
            "weights-improvement-48-1.4750.hdf5-Portuguese\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mHlAaACwcQlB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fit do modelo"
      ]
    },
    {
      "metadata": {
        "id": "80rAkBIAcQlE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "dt = datetime.now()\n",
        "# model.fit(X, y, epochs = 20, batch_size = 128, callbacks = callbacks_list)\n",
        "#model.fit(X, y, epochs = 50, batch_size = 64, callbacks = callbacks_list)\n",
        "print(\"tempo de execução\", datetime.now() - dt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZWsfFCNjcQlH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls w*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M-rMAJSPcQlR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Depois de executar o fit, você deve ter uma série de arquivos de checkpoint no mesmo diretório onde está este Jupyter Notebook. Você pode excluí-los todos exceto aquele com o menor valor de perda. Por exemplo, neste caso, o arquivo weights-improvement-19-1.9119.hdf5 será usado. Ele contém os melhores valores de peso."
      ]
    },
    {
      "metadata": {
        "id": "yu158wQ4cQlT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Carrega os melhores pesos da rede e compila o modelo\n",
        "filename = \"weights-improvement-49-1.3344.hdf5-Portuguese\"\n",
        "filename = 'weights-improvement-00-1.4191.hdf5-Portuguese'\n",
        "model.load_weights(filename)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ugcpoSScQlY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u4lJZjNEcQlj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text():\n",
        "    # Obtém um random seed\n",
        "    start = numpy.random.randint(0, len(dataX)-1)\n",
        "\n",
        "    # Inicia a geração de texto de um ponto qualquer, definido pelo random seed \"start\"\n",
        "    pattern = dataX[start]\n",
        "    print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "\n",
        "    # Gerando caracteres\n",
        "    for i in range(1000):\n",
        "        x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "        x = x / float(n_vocab)\n",
        "        prediction = model.predict(x, verbose=0)\n",
        "        index = numpy.argmax(prediction)\n",
        "        result = int_to_char[index]\n",
        "        seq_in = [int_to_char[value] for value in pattern]\n",
        "        sys.stdout.write(result)\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "    print('\\n=======================================================================================================\\n')\n",
        "    #print (\"\\nConcluído.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XqJdYdRXcQlo",
        "colab_type": "code",
        "colab": {},
        "outputId": "47542c73-3a7c-4fe4-dc56-3a6b2159e923"
      },
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "    generate_text()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"  estamos ao serviço de sua majestade e do povo.—sebastião insinuou que\n",
            "melhor se poderia servir à co \"\n",
            "rivera do boticário com o povo eope do costo de deste entrriear de consigçncis d alegrs de consigar que n alienista fez um pesto ertar a eni entertidara que a casa verde era uma casa a dezs de sma experiência dras senhoras e aposirtar a puem a câmara e outro de serolnta de cinco aeosseso do que a cesto firar de uma experiência irricia de uma casa aorise pessoaiia de casa verdade. e aonieado de contistar que a casa verde era uma casa a de lonte a outro coi que ele via desto que ele a cesto fngaram de cinrüsiia do alienista era a puem interrogara uma experiência irriliaram alguma coisa a serria de serelhas entradas de consistar e de ter espara pue o alienista fez um pesto ertar a erase pessoalieu- o perfeito dre ter con o alienista fosse de experses e a mesma cesta experam o alienista eoa persoa esta desrocar a casa verde e de ver preso a cesta persoa e de terelha com que ele via destora a ce uma iamela, com o alienista ficou a puem interrogara o daralhro de consiguar o alienista eoa per\n",
            "=======================================================================================================\n",
            "\n",
            "\" ciúmes do alienista.\n",
            "não podia ser outra coisa; realmente, a declaração do moço fora audaciosa demai \"\n",
            "s. erta deste ao vereadores de ser expriiar de ser esta prazi sen oos casar para a casa verde era uma casa a de lonte a outro coisa para e vinha de consiguar a casa verde e de ver eeloos de itaguaí o alienista pue o alienista fezia eestes e a da latelha e aontara a casa verde e de vina, o alienista ficou de casa esidta do que a cesto firar de uma experiência irricia de uma casa aorise pessoaiia de casa verdade. e aonieado de contistar que a casa verde era uma casa a de lonte a outro coi que ele via desto que ele a cesto fngaram de cinrüsiia do alienista era a puem interrogara uma experiência irriliaram alguma coisa a serria de serelhas entradas de consistar e de ter espara pue o alienista fez um pesto ertar a erase pessoalieu- o perfeito dre ter con o alienista fosse de experses e a mesma cesta experam o alienista eoa persoa esta desrocar a casa verde e de ver preso a cesta persoa e de terelha com que ele via destora a ce uma iamela, com o alienista ficou a puem interrogara o daralhro \n",
            "=======================================================================================================\n",
            "\n",
            "\" ele que era mais decoroso ao governo mandá-lo chamar; o\n",
            "receio, porém, de que o alienista não obedec \"\n",
            "erse ao alienista eoa respondera de cinco feridos. —cois lindos contentar a parse a uma casa aediarar d fecou e aona ao aarbeiro confiara a um alienia, e a responta de interro e de ser eomsesta com que ele nesmu a casa verde era uma casa a deste materse de consiguar o alienista que o alienista fez um sento da casa verde era uma casa a de lonte a outro coi que ele via desto que ele a cesto fngaram de cinrüsiia do alienista era a puem interrogara uma experiência irriliaram alguma coisa a serria de serelhas entradas de consistar e de ter espara pue o alienista fez um pesto ertar a erase pessoalieu- o perfeito dre ter con o alienista fosse de experses e a mesma cesta experam o alienista eoa persoa esta desrocar a casa verde e de ver preso a cesta persoa e de terelha com que ele via destora a ce uma iamela, com o alienista ficou a puem interrogara o daralhro de consiguar o alienista eoa persoa esta declaração de que a câmara e oão acontecia nu aligos de consigar que não arandou a casa verde\n",
            "=======================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qQGh_6-vcQlv",
        "colab_type": "code",
        "colab": {},
        "outputId": "c1a6d539-33c6-46b2-89dc-ee1d5cc2929a"
      },
      "cell_type": "code",
      "source": [
        "for i in range(4):\n",
        "    generate_text()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\" .\n",
            "simão bacamarte refletiu ainda um instante, e disse:\n",
            "—suponho o espírito humano uma vasta concha,  \"\n",
            "e a de uodas as vinas d aores de ser espara pue ele não cacamarse, não podia de uma experiência eranderte a pue ele via destomaram o alienista. e contensou o alienista era a puem interrogara uma experiência irriliaram alguma coisa a serria de serelhas entradas de consistar e de ter espara pue o alienista fez um pesto ertar a erase pessoalieu- o perfeito dre ter con o alienista fosse de experses e a mesma cesta experam o alienista eoa persoa esta desrocar a casa verde e de ver preso a cesta persoa e de terelha com que ele via destora a ce uma iamela, com o alienista ficou a puem interrogara o daralhro de consiguar o alienista eoa persoa esta declaração de que a câmara e oão acontecia nu aligos de consigar que não arandou a casa verde. —a casa verde era uma cas lais eeles de uma casar a rua e deste ele a ceus de pua e de uma experiência eranderte de uma casa arrimenta de que ele via destora a ce uma iamela, com o alienista ficou a puem interrogara o daralhro de consiguar o alienista eoa \n",
            "=======================================================================================================\n",
            "\n",
            "\" s, que não explicavam\n",
            "nada, tal era o produto diário da imaginação pública.\n",
            "nisto chegou do rio de j \"\n",
            "aneiro e a cestoda de sinpora e aligos de portura e alia nase de dasal a esta experiência de constin e desse alos era a sua majestade. pue a câmara e oão a ce janhãr co barbeiro co que a câmara lhe dra a alngadaça de casa verde uma penhera e ceste mada. a intirração de um alienis. esta descobaram de consistar a casa verde e de ver emi de expreiar de sem oesmo a intenção de que e de longeisa pare a goferdção de que a câmara e oão achita de cinco feridos à casa verde. e a almda e aligos de consiguar o alienista eoa persoa esta declaração de que a câmara e oão acontecia nu arigos de sereltas de ciarados de uma casa aertrea e a rerpitar alnjadente de uma casa aeradito de portura a puem enterdedecea. e assim de uma experiência eranderte de uma casa arrimenta de uma casa arrimenta de que ele via destora a ce uma iamela, com o alienista ficou a puem interrogara o daralhro de consiguar o alienista eoa persoa esta declaração de que a câmara e oão acontecia nu aligos de consigar que não arandou \n",
            "=======================================================================================================\n",
            "\n",
            "\" em de\n",
            "atacar os seus próprios camaradas, e, um a um, foram passando para eles, de modo que ao cabo d \"\n",
            "e cois exprisas d aastaram a erase pue ele a ceus de pua e de uma experiência eranderte a serria de alganista e de cinco des o alienista que o alienista fez um pesto ertar a eni entertidara que a casa verde era uma casa a dezs de sma experiência dras senhoras e aposirtar a puem a câmara e outro de serolnta de cinco aeosseso do que a cesto firar de uma experiência irricia de uma casa aorise pessoaiia de casa verdade. e aonieado de contistar que a casa verde era uma casa a de lonte a outro coi que ele via desto que ele a cesto fngaram de cinrüsiia do alienista era a puem interrogara uma experiência irriliaram alguma coisa a serria de serelhas entradas de consistar e de ter espara pue o alienista fez um pesto ertar a erase pessoalieu- o perfeito dre ter con o alienista fosse de experses e a mesma cesta experam o alienista eoa persoa esta desrocar a casa verde e de ver preso a cesta persoa e de terelha com que ele via destora a ce uma iamela, com o alienista ficou a puem interrogara o dara\n",
            "=======================================================================================================\n",
            "\n",
            "\" os em casa, mas não tardava a hora dos gritos. o terror crescia;\n",
            "avizinhava-se a rebelião. a idéia d \"\n",
            "e que a câmara e oão a ce janhãr co barbeiro co que a câmara lhe dra a alngadaça de casa verde uma penhera e ceste mada. a intirração de um alienis. esta descobaram de consistar a casa verde e de ver emi de expreiar de sem oesmo a intenção de que e de longeisa pare a goferdção de que a câmara e oão achita de cinco feridos à casa verde. e a almda e aligos de consiguar o alienista eoa persoa esta declaração de que a câmara e oão acontecia nu arigos de sereltas de ciarados de uma casa aertrea e a rerpitar alnjadente de uma casa aeradito de portura a puem enterdedecea. e assim de uma experiência eranderte de uma casa arrimenta de uma casa arrimenta de que ele via destora a ce uma iamela, com o alienista ficou a puem interrogara o daralhro de consiguar o alienista eoa persoa esta declaração de que a câmara e oão acontecia nu aligos de consigar que não arandou a casa verde. —a casa verde era uma cas lais eeles de uma casar a rua e deste ele a ceus de pua e de uma experiência eranderte de uma\n",
            "=======================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cb1xRkkmcQl6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}