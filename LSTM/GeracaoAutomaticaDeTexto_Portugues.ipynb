{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GeracaoAutomaticaDeTexto-Portugues.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladimiralencar/DeepLearning-LANA/blob/master/LSTM/GeracaoAutomaticaDeTexto_Portugues.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "cyl7oi3xS739",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Geração Automática de Texto com LSTMs - Português"
      ]
    },
    {
      "metadata": {
        "id": "1nNr_MA9S73_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As Redes Neurais Recorrentes também podem ser usadas como modelos generativos. Isso significa que além de serem usadas como modelos preditivos (fazendo previsões), elas podem aprender as sequências de um problema e, em seguida, gerar sequências plausíveis inteiramente novas para o domínio do problema. Modelos Generativos como este são úteis não apenas para estudar o quão bem um modelo aprendeu um problema, mas para saber mais sobre o próprio domínio do problema. "
      ]
    },
    {
      "metadata": {
        "id": "DUjHNeirS74A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Uma boa forma de praticar a criação de texto, é usando livros clássicos, os quais já temos uma boa ideia sobre a história e que não estejamos violando nenhum direito de copyright. Muitos livros clássicos já não possuem mais restrição de uso e podem ser usados gratuitamente. Um bom lugar para encontrar esses livros é no site do Projeto Gutenberg. É de lá que usaremos o livro para o qual criaremos um modelo generativo: Alice no País das Maravilhas ou o nome em inglês Alice's Adventures in Wonderland. O arquivo txt do livro pode ser baixado aqui: https://www.gutenberg.org/ebooks/11. \n",
        "Este livro tem cerca de 3.300 linhas de texto. O cabeçalho e a marca de final de arquivo foram removidos, já que não são necessários para o que vamos fazer."
      ]
    },
    {
      "metadata": {
        "id": "W0ghRSivS74B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Vamos aprender as dependências entre os caracteres e as probabilidades condicionais de caracteres em sequências para que possamos gerar sequências totalmente novas e originais de caracteres. Esta é uma tarefa divertida e recomendo repetir essas experiências com outros livros do Projeto Gutenberg. Essas experiências não se limitam ao texto, você também pode experimentar com outros dados ASCII, como código fonte de linguagens de programação, documentos marcados em LaTeX, HTML ou Markdown e muito mais. \n",
        "\n",
        "Faremos aqui algo muito similar ao que foi feito pelo programador, que escreveu um novo livro de Game ofthrones: http://www.businessinsider.com/ai-just-wrote-the-next-book-of-game-of-thrones-for-us-2017-8"
      ]
    },
    {
      "metadata": {
        "id": "h5TO0IJmS74C",
        "colab_type": "code",
        "colab": {},
        "outputId": "4b3b640e-ff06-4122-e8e8-864dab2f36e4"
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy\n",
        "import sys\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "pJKPbdY_S74K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Carregamos os dados e convertemos para lowercase \n",
        "# Estamos usando aqui arquivo texto no formato ASCII\n",
        "filename = \"O_Alienista.txt\" # livro de Machado de Assis\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I-KeSLMtS74O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora que o livro está carregado, devemos preparar os dados para modelagem. Não podemos modelar os caracteres diretamente, em vez disso, devemos converter os caracteres em números inteiros. Podemos fazer isso facilmente, criando um conjunto de todos os caracteres distintos do livro, então criando um mapa de cada caractere para um único inteiro."
      ]
    },
    {
      "metadata": {
        "id": "9rjKNmrWS74P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Criando o mapeamento caracter/inteiro\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYVRURhJS74S",
        "colab_type": "code",
        "colab": {},
        "outputId": "35bfdd9e-640e-41a7-8bec-48eb709106ee"
      },
      "cell_type": "code",
      "source": [
        "chars"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " '(',\n",
              " ')',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'x',\n",
              " 'z',\n",
              " '§',\n",
              " '°',\n",
              " 'º',\n",
              " 'à',\n",
              " 'á',\n",
              " 'â',\n",
              " 'ã',\n",
              " 'ç',\n",
              " 'é',\n",
              " 'ê',\n",
              " 'í',\n",
              " 'ò',\n",
              " 'ó',\n",
              " 'ô',\n",
              " 'õ',\n",
              " 'ú',\n",
              " 'ü',\n",
              " '—',\n",
              " '’',\n",
              " '“',\n",
              " '”']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "4MP8CeqES74X",
        "colab_type": "code",
        "colab": {},
        "outputId": "686f7fe6-5463-40dc-9b7c-d15ed37d4967"
      },
      "cell_type": "code",
      "source": [
        "char_to_int"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " '(': 4,\n",
              " ')': 5,\n",
              " ',': 6,\n",
              " '-': 7,\n",
              " '.': 8,\n",
              " '0': 9,\n",
              " '1': 10,\n",
              " '2': 11,\n",
              " '3': 12,\n",
              " '4': 13,\n",
              " '5': 14,\n",
              " '6': 15,\n",
              " '7': 16,\n",
              " '8': 17,\n",
              " '9': 18,\n",
              " ':': 19,\n",
              " ';': 20,\n",
              " '?': 21,\n",
              " 'a': 22,\n",
              " 'b': 23,\n",
              " 'c': 24,\n",
              " 'd': 25,\n",
              " 'e': 26,\n",
              " 'f': 27,\n",
              " 'g': 28,\n",
              " 'h': 29,\n",
              " 'i': 30,\n",
              " 'j': 31,\n",
              " 'l': 32,\n",
              " 'm': 33,\n",
              " 'n': 34,\n",
              " 'o': 35,\n",
              " 'p': 36,\n",
              " 'q': 37,\n",
              " 'r': 38,\n",
              " 's': 39,\n",
              " 't': 40,\n",
              " 'u': 41,\n",
              " 'v': 42,\n",
              " 'x': 43,\n",
              " 'z': 44,\n",
              " '§': 45,\n",
              " '°': 46,\n",
              " 'º': 47,\n",
              " 'à': 48,\n",
              " 'á': 49,\n",
              " 'â': 50,\n",
              " 'ã': 51,\n",
              " 'ç': 52,\n",
              " 'é': 53,\n",
              " 'ê': 54,\n",
              " 'í': 55,\n",
              " 'ò': 56,\n",
              " 'ó': 57,\n",
              " 'ô': 58,\n",
              " 'õ': 59,\n",
              " 'ú': 60,\n",
              " 'ü': 61,\n",
              " '—': 62,\n",
              " '’': 63,\n",
              " '“': 64,\n",
              " '”': 65}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "w2EjSE1xS74c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pode haver alguns caracteres que podemos remover para limpar mais o conjunto de dados que reduzirá o vocabulário e poderá melhorar o processo de modelagem. "
      ]
    },
    {
      "metadata": {
        "id": "FhS5n-snS74d",
        "colab_type": "code",
        "colab": {},
        "outputId": "f84b6c58-a2ec-4f34-bd07-8dce08688325"
      },
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: {:,d}\".format(n_chars))  \n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters: 99,080\n",
            "Total Vocab:  66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dXMg3KVXS74h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Podemos ver que o livro tem pouco mais de 99.000 caracteres e que quando convertidos para minúsculas, existem apenas 44 caracteres distintos no vocabulário para a rede aprender, muito mais do que os 26 no alfabeto. Agora, precisamos definir os dados de treinamento para a rede. Existe muita flexibilidade em como você escolhe dividir o texto e expô-lo a rede durante o treino. Aqui dividiremos o texto do livro em subsequências com um comprimento de 100 caracteres, um comprimento arbitrário. Poderíamos facilmente dividir os dados por sentenças e ajustar as sequências mais curtas e truncar as mais longas. Cada padrão de treinamento da rede é composto de 100 passos de tempo (time steps) de um caractere (X) seguido por um caracter de saída (y). Ao criar essas sequências, deslizamos esta janela ao longo de todo o livro um caracter de cada vez, permitindo que cada caracter tenha a chance de ser aprendido a partir dos 100 caracteres que o precederam (exceto os primeiros 100 caracteres, é claro). Por exemplo, se o comprimento da sequência é 5 (para simplificar), os dois primeiros padrões de treinamento seriam os seguintes:\n",
        "\n",
        "* Palavra: CHAPTER\n",
        "* CHAPT -> E\n",
        "* HAPTE -> R"
      ]
    },
    {
      "metadata": {
        "id": "bX_cIbVfS74j",
        "colab_type": "code",
        "colab": {},
        "outputId": "2346b994-2c75-4624-ac1b-164cf6f59f5e"
      },
      "cell_type": "code",
      "source": [
        "# À medida que dividimos o livro em sequências, convertemos os caracteres em números inteiros usando nossa\n",
        "# tabela de pesquisa que preparamos anteriormente.\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = raw_text[i:i + seq_length]\n",
        "    seq_out = raw_text[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total de Padrões: \", n_patterns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de Padrões:  98980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OT76IHpNS74n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora que preparamos nossos dados de treinamento, precisamos transformá-lo para que possamos usá-lo com o Keras. Primeiro, devemos transformar a lista de sequências de entrada na forma [amostras, passos de tempo, recursos] esperados por uma rede LSTM. Em seguida, precisamos redimensionar os números inteiros para o intervalo de 0 a 1 para tornar os padrões mais fáceis de aprender pela rede LSTM que usa a função de ativação sigmoide por padrão."
      ]
    },
    {
      "metadata": {
        "id": "4tUC1n9qS74o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reshape de X para [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# Normalização\n",
        "X = X / float(n_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jVD_-gupS74s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finalmente, precisamos converter os padrões de saída (caracteres únicos convertidos em números inteiros) usando Hot-Encoding. Isto é para que possamos configurar a rede para prever a probabilidade de cada um dos 44 caracteres diferentes no vocabulário (uma representação mais fácil) em vez de tentar forçá-lo a prever com precisão o próximo caracter. Cada valor de y é convertido em um vetor com um comprimento 66, cheio de zeros, exceto com um 1 na coluna para a letra (inteiro) que o padrão representa. Por exemplo, quando a letra n (valor inteiro 30) tiver sido transformada usando One-Hot Encoding, vai se parecer com isso:\n",
        "\n",
        "[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
      ]
    },
    {
      "metadata": {
        "id": "MlcbxrukS74t",
        "colab_type": "code",
        "colab": {},
        "outputId": "455f7b31-95bd-4ba8-9639-432aa1e24601"
      },
      "cell_type": "code",
      "source": [
        "# One-Hot Encoding da variável de saída\n",
        "y = np_utils.to_categorical(dataY)\n",
        "y[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
              "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
              "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "WZVdgHi1S74z",
        "colab_type": "code",
        "colab": {},
        "outputId": "6b7ff246-56f8-4dd2-f4f9-27a309c7df75"
      },
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(98980, 100, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "Q2n2mzySS744",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Modelo LSTM com duas camadas de Dropout com 20%\n",
        "# O tempo de treinamento é bem longo - \n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "azLX8d7VS74-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Não há conjunto de dados de teste. Estamos modelando todo o conjunto de dados de treinamento para aprender a probabilidade de cada caracter em uma sequência. Não estamos interessados nos mais preciso modelo do conjunto de dados de treinamento (Acurácia de Classificação). Este seria um modelo que prevê cada caracter no conjunto de dados de treinamento perfeitamente. Em vez disso, estamos interessados em uma generalização do conjunto de dados que minimiza a função de perda escolhida. Estamos buscando um equilíbrio entre generalização e\n",
        "overfitting."
      ]
    },
    {
      "metadata": {
        "id": "VXb4XIenS74_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define o checkpoint\n",
        "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ra4LFWhSS75E",
        "colab_type": "code",
        "colab": {},
        "outputId": "d3ac8861-d5e6-4b1d-81b0-be9504f105c9"
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "dt = datetime.now()\n",
        "for i in range(1000000): i +1\n",
        "print(\"tempo de execução\", datetime.now() - dt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tempo de execução 0:00:00.121429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z5c9W2iOS75L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fit do modelo"
      ]
    },
    {
      "metadata": {
        "id": "43bvjg_5S75N",
        "colab_type": "code",
        "colab": {},
        "outputId": "e9a6539a-bb76-4bf8-e222-9193da26c644"
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "dt = datetime.now()\n",
        "# model.fit(X, y, epochs = 20, batch_size = 128, callbacks = callbacks_list)\n",
        "model.fit(X, y, epochs = 50, batch_size = 64, callbacks = callbacks_list)\n",
        "print(\"tempo de execução\", datetime.now() - dt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.9924Epoch 00000: loss improved from inf to 2.99245, saving model to weights-improvement-00-2.9924.hdf5\n",
            "98980/98980 [==============================] - 653s - loss: 2.9924   \n",
            "Epoch 2/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.8144Epoch 00001: loss improved from 2.99245 to 2.81441, saving model to weights-improvement-01-2.8144.hdf5\n",
            "98980/98980 [==============================] - 653s - loss: 2.8144   \n",
            "Epoch 3/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.6979Epoch 00002: loss improved from 2.81441 to 2.69794, saving model to weights-improvement-02-2.6979.hdf5\n",
            "98980/98980 [==============================] - 653s - loss: 2.6979   \n",
            "Epoch 4/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.5828Epoch 00003: loss improved from 2.69794 to 2.58271, saving model to weights-improvement-03-2.5827.hdf5\n",
            "98980/98980 [==============================] - 655s - loss: 2.5827   \n",
            "Epoch 5/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.4803Epoch 00004: loss improved from 2.58271 to 2.48025, saving model to weights-improvement-04-2.4802.hdf5\n",
            "98980/98980 [==============================] - 747s - loss: 2.4802   \n",
            "Epoch 6/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.3981Epoch 00005: loss improved from 2.48025 to 2.39805, saving model to weights-improvement-05-2.3980.hdf5\n",
            "98980/98980 [==============================] - 675s - loss: 2.3980   \n",
            "Epoch 7/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.3279Epoch 00006: loss improved from 2.39805 to 2.32777, saving model to weights-improvement-06-2.3278.hdf5\n",
            "98980/98980 [==============================] - 808s - loss: 2.3278   \n",
            "Epoch 8/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.2672Epoch 00007: loss improved from 2.32777 to 2.26720, saving model to weights-improvement-07-2.2672.hdf5\n",
            "98980/98980 [==============================] - 862s - loss: 2.2672   \n",
            "Epoch 9/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.2152Epoch 00008: loss improved from 2.26720 to 2.21524, saving model to weights-improvement-08-2.2152.hdf5\n",
            "98980/98980 [==============================] - 650s - loss: 2.2152   \n",
            "Epoch 10/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.1670Epoch 00009: loss improved from 2.21524 to 2.16691, saving model to weights-improvement-09-2.1669.hdf5\n",
            "98980/98980 [==============================] - 650s - loss: 2.1669   \n",
            "Epoch 11/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.1194Epoch 00010: loss improved from 2.16691 to 2.11933, saving model to weights-improvement-10-2.1193.hdf5\n",
            "98980/98980 [==============================] - 651s - loss: 2.1193   \n",
            "Epoch 12/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.0817Epoch 00011: loss improved from 2.11933 to 2.08168, saving model to weights-improvement-11-2.0817.hdf5\n",
            "98980/98980 [==============================] - 657s - loss: 2.0817   \n",
            "Epoch 13/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.0446Epoch 00012: loss improved from 2.08168 to 2.04446, saving model to weights-improvement-12-2.0445.hdf5\n",
            "98980/98980 [==============================] - 654s - loss: 2.0445   \n",
            "Epoch 14/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.2534Epoch 00013: loss did not improve\n",
            "98980/98980 [==============================] - 656s - loss: 2.2568   \n",
            "Epoch 15/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 13.7604Epoch 00014: loss did not improve\n",
            "98980/98980 [==============================] - 653s - loss: 13.7562   \n",
            "Epoch 16/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.0058Epoch 00015: loss improved from 2.04446 to 2.00573, saving model to weights-improvement-15-2.0057.hdf5\n",
            "98980/98980 [==============================] - 655s - loss: 2.0057   \n",
            "Epoch 17/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.9505Epoch 00016: loss improved from 2.00573 to 1.95054, saving model to weights-improvement-16-1.9505.hdf5\n",
            "98980/98980 [==============================] - 652s - loss: 1.9505   \n",
            "Epoch 18/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.9189Epoch 00017: loss improved from 1.95054 to 1.91903, saving model to weights-improvement-17-1.9190.hdf5\n",
            "98980/98980 [==============================] - 655s - loss: 1.9190   \n",
            "Epoch 19/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.8906Epoch 00018: loss improved from 1.91903 to 1.89059, saving model to weights-improvement-18-1.8906.hdf5\n",
            "98980/98980 [==============================] - 656s - loss: 1.8906   \n",
            "Epoch 20/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.8657Epoch 00019: loss improved from 1.89059 to 1.86565, saving model to weights-improvement-19-1.8656.hdf5\n",
            "98980/98980 [==============================] - 651s - loss: 1.8656   \n",
            "Epoch 21/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.8412Epoch 00020: loss improved from 1.86565 to 1.84124, saving model to weights-improvement-20-1.8412.hdf5\n",
            "98980/98980 [==============================] - 675s - loss: 1.8412   \n",
            "Epoch 22/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.8182Epoch 00021: loss improved from 1.84124 to 1.81810, saving model to weights-improvement-21-1.8181.hdf5\n",
            "98980/98980 [==============================] - 669s - loss: 1.8181   \n",
            "Epoch 23/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.7940Epoch 00022: loss improved from 1.81810 to 1.79409, saving model to weights-improvement-22-1.7941.hdf5\n",
            "98980/98980 [==============================] - 654s - loss: 1.7941   \n",
            "Epoch 24/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.7738Epoch 00023: loss improved from 1.79409 to 1.77373, saving model to weights-improvement-23-1.7737.hdf5\n",
            "98980/98980 [==============================] - 657s - loss: 1.7737   \n",
            "Epoch 25/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.7562Epoch 00024: loss improved from 1.77373 to 1.75612, saving model to weights-improvement-24-1.7561.hdf5\n",
            "98980/98980 [==============================] - 676s - loss: 1.7561   \n",
            "Epoch 26/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.7358Epoch 00025: loss improved from 1.75612 to 1.73594, saving model to weights-improvement-25-1.7359.hdf5\n",
            "98980/98980 [==============================] - 656s - loss: 1.7359   \n",
            "Epoch 27/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.7206Epoch 00026: loss improved from 1.73594 to 1.72068, saving model to weights-improvement-26-1.7207.hdf5\n",
            "98980/98980 [==============================] - 657s - loss: 1.7207   \n",
            "Epoch 28/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.7019Epoch 00027: loss improved from 1.72068 to 1.70200, saving model to weights-improvement-27-1.7020.hdf5\n",
            "98980/98980 [==============================] - 653s - loss: 1.7020   \n",
            "Epoch 29/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.6831Epoch 00028: loss improved from 1.70200 to 1.68299, saving model to weights-improvement-28-1.6830.hdf5\n",
            "98980/98980 [==============================] - 651s - loss: 1.6830   \n",
            "Epoch 30/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.6714Epoch 00029: loss improved from 1.68299 to 1.67137, saving model to weights-improvement-29-1.6714.hdf5\n",
            "98980/98980 [==============================] - 651s - loss: 1.6714   \n",
            "Epoch 31/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.6539Epoch 00030: loss improved from 1.67137 to 1.65381, saving model to weights-improvement-30-1.6538.hdf5\n",
            "98980/98980 [==============================] - 652s - loss: 1.6538   \n",
            "Epoch 32/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.6425Epoch 00031: loss improved from 1.65381 to 1.64250, saving model to weights-improvement-31-1.6425.hdf5\n",
            "98980/98980 [==============================] - 652s - loss: 1.6425   \n",
            "Epoch 33/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.6258Epoch 00032: loss improved from 1.64250 to 1.62604, saving model to weights-improvement-32-1.6260.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "98980/98980 [==============================] - 651s - loss: 1.6260   \n",
            "Epoch 34/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.6108Epoch 00033: loss improved from 1.62604 to 1.61078, saving model to weights-improvement-33-1.6108.hdf5\n",
            "98980/98980 [==============================] - 651s - loss: 1.6108   \n",
            "Epoch 35/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.6012Epoch 00034: loss improved from 1.61078 to 1.60121, saving model to weights-improvement-34-1.6012.hdf5\n",
            "98980/98980 [==============================] - 653s - loss: 1.6012   \n",
            "Epoch 36/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5887Epoch 00035: loss improved from 1.60121 to 1.58865, saving model to weights-improvement-35-1.5887.hdf5\n",
            "98980/98980 [==============================] - 656s - loss: 1.5887   \n",
            "Epoch 37/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5806Epoch 00036: loss improved from 1.58865 to 1.58049, saving model to weights-improvement-36-1.5805.hdf5\n",
            "98980/98980 [==============================] - 655s - loss: 1.5805   \n",
            "Epoch 38/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5692Epoch 00037: loss improved from 1.58049 to 1.56930, saving model to weights-improvement-37-1.5693.hdf5\n",
            "98980/98980 [==============================] - 662s - loss: 1.5693   \n",
            "Epoch 39/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5594Epoch 00038: loss improved from 1.56930 to 1.55936, saving model to weights-improvement-38-1.5594.hdf5\n",
            "98980/98980 [==============================] - 657s - loss: 1.5594   \n",
            "Epoch 40/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5489Epoch 00039: loss improved from 1.55936 to 1.54879, saving model to weights-improvement-39-1.5488.hdf5\n",
            "98980/98980 [==============================] - 652s - loss: 1.5488   \n",
            "Epoch 41/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5393Epoch 00040: loss improved from 1.54879 to 1.53948, saving model to weights-improvement-40-1.5395.hdf5\n",
            "98980/98980 [==============================] - 651s - loss: 1.5395   \n",
            "Epoch 42/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5283Epoch 00041: loss improved from 1.53948 to 1.52834, saving model to weights-improvement-41-1.5283.hdf5\n",
            "98980/98980 [==============================] - 652s - loss: 1.5283   \n",
            "Epoch 43/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5233Epoch 00042: loss improved from 1.52834 to 1.52339, saving model to weights-improvement-42-1.5234.hdf5\n",
            "98980/98980 [==============================] - 656s - loss: 1.5234   \n",
            "Epoch 44/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5112Epoch 00043: loss improved from 1.52339 to 1.51134, saving model to weights-improvement-43-1.5113.hdf5\n",
            "98980/98980 [==============================] - 657s - loss: 1.5113   \n",
            "Epoch 45/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5090Epoch 00044: loss improved from 1.51134 to 1.50899, saving model to weights-improvement-44-1.5090.hdf5\n",
            "98980/98980 [==============================] - 657s - loss: 1.5090   \n",
            "Epoch 46/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5010Epoch 00045: loss improved from 1.50899 to 1.50104, saving model to weights-improvement-45-1.5010.hdf5\n",
            "98980/98980 [==============================] - 658s - loss: 1.5010   \n",
            "Epoch 47/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.4886Epoch 00046: loss improved from 1.50104 to 1.48879, saving model to weights-improvement-46-1.4888.hdf5\n",
            "98980/98980 [==============================] - 653s - loss: 1.4888   \n",
            "Epoch 48/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.4803Epoch 00047: loss improved from 1.48879 to 1.48016, saving model to weights-improvement-47-1.4802.hdf5\n",
            "98980/98980 [==============================] - 653s - loss: 1.4802   \n",
            "Epoch 49/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.4750Epoch 00048: loss improved from 1.48016 to 1.47499, saving model to weights-improvement-48-1.4750.hdf5\n",
            "98980/98980 [==============================] - 655s - loss: 1.4750   \n",
            "Epoch 50/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.4807Epoch 00049: loss did not improve\n",
            "98980/98980 [==============================] - 657s - loss: 1.4806   \n",
            "tempo de execução 9:14:18.978356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rpeRRwaFS75U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Continuar treinando o modelo a partir do checkpoint"
      ]
    },
    {
      "metadata": {
        "id": "MPJ1v1YaS75V",
        "colab_type": "code",
        "colab": {},
        "outputId": "c7360555-c9c5-454e-d5ab-093fd6cd93d5"
      },
      "cell_type": "code",
      "source": [
        "# Continuar treinando o modelo a partir do checkpoint\n",
        "\n",
        "# Carrega os melhores pesos da rede e compila o modelo\n",
        "filename = 'weights/' +  \"weights-improvement-07-1.3928.hdf5-Portuguese\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
        "\n",
        "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5-Portuguese\"\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "from datetime import datetime\n",
        "dt = datetime.now()\n",
        "# fit the model\n",
        "model.fit(X, y, epochs = 10, batch_size = 64, callbacks = callbacks_list)\n",
        "print(\"tempo de execução\", datetime.now() - dt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.4154Epoch 00000: loss improved from inf to 1.41530, saving model to weights-improvement-00-1.4153.hdf5-Portuguese\n",
            "98980/98980 [==============================] - 651s - loss: 1.4153   \n",
            "Epoch 2/10\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.4022Epoch 00001: loss improved from 1.41530 to 1.40228, saving model to weights-improvement-01-1.4023.hdf5-Portuguese\n",
            "98980/98980 [==============================] - 650s - loss: 1.4023   \n",
            "Epoch 3/10\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.4027Epoch 00002: loss did not improve\n",
            "98980/98980 [==============================] - 651s - loss: 1.4026   \n",
            "Epoch 4/10\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.3913Epoch 00003: loss improved from 1.40228 to 1.39147, saving model to weights-improvement-03-1.3915.hdf5-Portuguese\n",
            "98980/98980 [==============================] - 652s - loss: 1.3915   \n",
            "Epoch 5/10\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.3881Epoch 00004: loss improved from 1.39147 to 1.38806, saving model to weights-improvement-04-1.3881.hdf5-Portuguese\n",
            "98980/98980 [==============================] - 650s - loss: 1.3881   \n",
            "Epoch 6/10\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.3892Epoch 00005: loss did not improve\n",
            "98980/98980 [==============================] - 664s - loss: 1.3892   \n",
            "Epoch 7/10\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.3769Epoch 00006: loss improved from 1.38806 to 1.37695, saving model to weights-improvement-06-1.3770.hdf5-Portuguese\n",
            "98980/98980 [==============================] - 653s - loss: 1.3770   \n",
            "Epoch 8/10\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.3816Epoch 00007: loss did not improve\n",
            "98980/98980 [==============================] - 658s - loss: 1.3816   \n",
            "Epoch 9/10\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.3758Epoch 00008: loss improved from 1.37695 to 1.37576, saving model to weights-improvement-08-1.3758.hdf5-Portuguese\n",
            "98980/98980 [==============================] - 654s - loss: 1.3758   \n",
            "Epoch 10/10\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 4.3380Epoch 00009: loss did not improve\n",
            "98980/98980 [==============================] - 651s - loss: 4.3375   \n",
            "tempo de execução 1:49:00.916299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gVuW7UNXS75c",
        "colab_type": "code",
        "colab": {},
        "outputId": "9cace758-9444-42bb-b7de-0db045ee00bb"
      },
      "cell_type": "code",
      "source": [
        "# tempo de execução 9:14:18 - 50 epoch\n",
        "# tempo de execução 1:50:49 - 10 epoch\n",
        "#!ls w*.-Portuguese\n",
        "!ls  w*.hdf5-Portuguese"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights-improvement-00-1.4153.hdf5-Portuguese\r\n",
            "weights-improvement-01-1.4023.hdf5-Portuguese\r\n",
            "weights-improvement-03-1.3915.hdf5-Portuguese\r\n",
            "weights-improvement-04-1.3881.hdf5-Portuguese\r\n",
            "weights-improvement-06-1.3770.hdf5-Portuguese\r\n",
            "weights-improvement-08-1.3758.hdf5-Portuguese\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G9Sw_ty6S75e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!ls weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GMqV3xLQS75i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!cp weights-improvement-48-1.4750.hdf5 weights-improvement-48-1.4750.hdf5-Portuguese"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V9MnCc7MS75k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Depois de executar o fit, você deve ter uma série de arquivos de checkpoint no mesmo diretório onde está este Jupyter Notebook. Você pode excluí-los todos exceto aquele com o menor valor de perda. Por exemplo, neste caso, o arquivo weights-improvement-19-1.9119.hdf5 será usado. Ele contém os melhores valores de peso."
      ]
    },
    {
      "metadata": {
        "id": "R1qjzs_lS75l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Carrega os melhores pesos da rede e compila o modelo\n",
        "filename = \"weights-improvement-08-1.3758.hdf5-Portuguese\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e_p7kpcnS75n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r0m3VnU6S75q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text():\n",
        "    # Obtém um random seed\n",
        "    start = numpy.random.randint(0, len(dataX)-1)\n",
        "\n",
        "    # Inicia a geração de texto de um ponto qualquer, definido pelo random seed \"start\"\n",
        "    pattern = dataX[start]\n",
        "    print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "\n",
        "    # Gerando caracteres\n",
        "    for i in range(1000):\n",
        "        x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "        x = x / float(n_vocab)\n",
        "        prediction = model.predict(x, verbose=0)\n",
        "        index = numpy.argmax(prediction)\n",
        "        result = int_to_char[index]\n",
        "        seq_in = [int_to_char[value] for value in pattern]\n",
        "        sys.stdout.write(result)\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "    print('\\n=======================================================================================================\\n')\n",
        "\n",
        "    #print (\"\\nConcluído.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6z6DdnOAS75s",
        "colab_type": "code",
        "colab": {},
        "outputId": "a4a6862b-db0d-40c0-c786-36ac2e94dae4"
      },
      "cell_type": "code",
      "source": [
        "generate_text()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"  da câmara, e da matriz;—ou por meio de matraca. \n",
            "7\n",
            "eis em que consistia este segundo uso. contratav \"\n",
            "a-se o dasiica eosa de eizer que o alienista não o alienista foi o pue prdia ser ee uma peruersiria do alienista eos almdossos de poder a sua majestade. a puem entiruava a casa verde e de terris lais esa uma coisa mais de manhão com a ele, não a de janhia, e alnda o alienista puoiaio que vossa renhora e dessibado a putra de ser emrredo de casa espicado e desiguarses e conseger ao povo eoses de mongems, e deserninhar a parse a casa verde é um col de longin e desebrar aos outros, e a alia do res de janeiro. sem de uma peruecaram as alianiste. o povo ene nais a meio destribada d de todas as alianistos de simão bacamarte que o alienista fosse con erande a almga e aligos dos outros, o alienista não o alienista foi o pue prdie fogarda do inuerno com a responta de sua majestade. um dos prineiras, a casa verde é um dos pontos para o ponferoo que o alienista fosse con o alienista fosse continto e alma a dessrra da rerponta de itaguaí e asosria e a dessiga de que o alienista eosse continto e alm\n",
            "=======================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BzJlNEAQS75v",
        "colab_type": "code",
        "colab": {},
        "outputId": "beb000b5-0f58-4eb2-a1ae-13d32fb0affb"
      },
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    generate_text()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\" u esse grito\n",
            "justamente no dia em que simão bacamarte fizera recolher à casa verde um homem que traz \"\n",
            "ia com o alienista eosse continto e alma aodaça a um alienista de casa esa uma casa mais de uma perueca do boticário. e dizia ele não podia ser rodos os casos de um alienisto era o alienista contintou o alienista que o alienista fosse con erande a almga e aligos dos outros, o alienista não o alienista foi o pue prdie fogarda do inuerno com a responta de sua majestade. um dos prineiras, a casa verde é um dos pontos para o ponferoo que o alienista fosse con o alienista fosse continto e alma a dessrra da rerponta de itaguaí e asosria e a dessiga de que o alienista eosse continto e alma aodaça a casa verde, o alienista ficou a puem e de um cos mais consrigades mais aluitres de dois monensos da casa verde e de porta movícia de sua majestade. a puem incomtinte a esta dxperiência eo alienista que o alienista fosse con erande a almga e aligos dos outros, o alienista não o alienista foi o pue prdie fogarda do inuerno com a responta de sua majestade. um dos prineiras, a casa verde é um dos ponto\n",
            "=======================================================================================================\n",
            "\n",
            "\"  por caridade do que por\n",
            "interesse científico.\n",
            "que, na verdade, a paciência do alienista era ainda m \"\n",
            "ais a rersia do marido, dizia ele a ceusa para outra coisa a intimiar dos peus outros eras tenhoras e aoisas e a dasa verde e de ter o dessigar o territo pue tes comsas e a dessiga de sua majestade e do povo porfírio era um cos dases a experiência do alienista que o alienista fosse con erande a almga e aligos dos outros, o alienista não o alienista foi o pue prdie fogarda do inuerno com a responta de sua majestade. um dos prineiras, a casa verde é um dos pontos para o ponferoo que o alienista fosse con o alienista fosse continto e alma a dessrra da rerponta de itaguaí e asosria e a dessiga de que o alienista eosse continto e alma aodaça a casa verde, o alienista ficou a puem e de um cos mais consrigades mais aluitres de dois monensos da casa verde e de porta movícia de sua majestade. a puem incomtinte a esta dxperiência eo alienista que o alienista fosse con erande a almga e aligos dos outros, o alienista não o alienista foi o pue prdie fogarda do inuerno com a responta de sua majestad\n",
            "=======================================================================================================\n",
            "\n",
            "\" gava um texto antigo, ora ruminava uma questão, e ia\n",
            "muitas vezes de um cabo a outro do jantar sem d \"\n",
            "eseobrira de uma casa verta e acrigação do boticário. e até que outira a alma do costa erpevava a um sertiterto contrar o que podia ficas e de uma peruema do alienista. e continuou sempentas pue não era casa verde um dos paientes de sua majestade. um dos prineiras, a casa verde é um conterso de dezs e a de uma uenhas de um cor das cortes e assimentras e castiga de sua majestade, a puem enterdeitamdo o alienista que o alienista fosse con o alienista fosse continto e alma a dessrra da rerponta de itaguaí e asosria e a dessiga de que o alienista eosse continto e alma aodaça a casa verde, o alienista ficou a puem e de um cos mais consrigades mais aluitres de dois monensos da casa verde e de porta movícia de sua majestade. a puem incomtinte a esta dxperiência eo alienista que o alienista fosse con erande a almga e aligos dos outros, o alienista não o alienista foi o pue prdie fogarda do inuerno com a responta de sua majestade. um dos prineiras, a casa verde é um dos pontos para o ponferoo q\n",
            "=======================================================================================================\n",
            "\n",
            "\" uma de gozo, outra de\n",
            "abatimento. a de gozo foi por ver que, ao cabo de longas e pacientes investiga \"\n",
            "ções prinaip d secantoados de cinco derres a esa. aligado de cizir que o alienista não o alienista não o almeços a alegaram ao cotta, e a rerpita do mosso alienista, e ainda assim a intenção de um alienid. e alnda o alienista eosse continto este prroo ao alienista, e aontra uma experiência era caca rmdo ao ticário e de uma penhura de que ele virera a casa verde, sem desenual de cinrüsiia que o alienista fosse con o alienista fosse continto e alma a dessrra da rerponta de itaguaí e asosria e a dessiga de que o alienista eosse continto e alma aodaça a casa verde, o alienista ficou a puem e de um cos mais consrigades mais aluitres de dois monensos da casa verde e de porta movícia de sua majestade. a puem incomtinte a esta dxperiência eo alienista que o alienista fosse con erande a almga e aligos dos outros, o alienista não o alienista foi o pue prdie fogarda do inuerno com a responta de sua majestade. um dos prineiras, a casa verde é um dos pontos para o ponferoo que o alienista fosse con\n",
            "=======================================================================================================\n",
            "\n",
            "\" ia logo às doses máximas,—graduava-as, conforme o estado, a idade,\n",
            "o temperamento, a posição social  \"\n",
            "en que confirmar o descrrirírio da câmara, e alnda o dasalho e aoosentar o povo eo que confirmar e con eran mortos de ser a rua maiestade. um dos aligos e de modos com a eeclaração de que a pualidade pre o alienista fosse con ele a cessa maneira de que o alienista não o alienado não é que o perfeito de diente modesto, o perfeito dqsiluífio da casa verde e de terris lais esa uma coisa mais de manhão com a ele, não a de janhia, e alnda o alienista puoiaio que vossa renhora e dessibado a putra de ser emrredo de casa espicado e desiguarses e conseger ao povo eoses de mongems, e deserninhar a parse a casa verde é um col de longin e desebrar aos outros, e a alia do res de janeiro. sem de uma peruecaram as alianiste. o povo ene nais a meio destribada d de todas as alianistos de simão bacamarte que o alienista fosse con erande a almga e aligos dos outros, o alienista não o alienista foi o pue prdie fogarda do inuerno com a responta de sua majestade. um dos prineiras, a casa verde é um dos pont\n",
            "=======================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VolGvfkkS75y",
        "colab_type": "code",
        "colab": {},
        "outputId": "edad2ae6-0dc4-4884-8d1b-06acf63888ec"
      },
      "cell_type": "code",
      "source": [
        "generate_text()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"  e que ele não podia deixar na rua um\n",
            "mentecapto. a última pessoa que intercedeu por ele (porque dep \"\n",
            "ois de casa a mais pidenda de um alcare de sereltou a casa verde era uma casa aorica, e a câmara lhe dasa de alguma casa verde. — a pazão do boticário. e dizer que era um cos damar, e a dâmara lhe deste lajs de eizer que o alienista fosse conceuso do alienista por angar comses a esperiar dos peus enteresses de ser emtregar com a alna, o perso de que a câmara lhe dasa de ser eme a casa verde era uma casa aoricado, mas acmanan-lhe o alienista a alguns senpos, a casa verde era uma casa aoricado, mas acmanan-lhe o alienista a alguns senpos, a casa verde era uma casa aoricado, mas acmanan-lhe o alienista a alguns senpos, a casa verde era uma casa aoricado, mas acmanan-lhe o alienista a alguns senpos, a casa verde era uma casa aoricado, mas acmanan-lhe o alienista a alguns senpos, a casa verde era uma casa aoricado, mas acmanan-lhe o alienista a alguns senpos, a casa verde era uma casa aoricado, mas acmanan-lhe o alienista a alguns senpos, a casa verde era uma casa aoricado, mas acmanan-lhe \n",
            "=======================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "49LWeLluS751",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}