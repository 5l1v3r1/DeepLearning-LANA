{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN01.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladimiralencar/DeepLearning-LANA/blob/master/CNN/CNN01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "b_H4FH2IWNzL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks (CNNs)"
      ]
    },
    {
      "metadata": {
        "id": "Ym2rnibeXqpj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Carregando imagens"
      ]
    },
    {
      "metadata": {
        "id": "jx3xpC_ZWc0H",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bcaf4a91-9c9e-434c-a25c-f4310d1ac992"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir images\n",
        "!cp *.png images\n",
        "!cp *.jpg images\n",
        "!cp *.gif images\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0ad11b01-adcd-4d05-9a17-e96782c4e60c\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-0ad11b01-adcd-4d05-9a17-e96782c4e60c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘images’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jwPopPLhXSaw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1d10c8ea-ff93-457f-963e-c3899bda024a"
      },
      "cell_type": "code",
      "source": [
        "#!rm images\n",
        "!ls images"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "arquitetura.png       cnn_grid.png    conv_maxpool2.png  mnist.png\n",
            "cifar.png\t      cnn.jpg\t      conv_maxpool.png\n",
            "cnn_architecture.png  conv-layer.gif  lenet5.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ztKblqi0b2tg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ]
    },
    {
      "metadata": {
        "id": "92zg5ku5WNzO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A Rede Neural Convolucional (CNN) é uma tecnologia de rede neural que impactou profundamente a área de Visão Computacional. Fukushima (1980) introduziu o conceito original de uma rede neural convolutiva, e LeCun, Bottou, Bengio & Haffner (1998) melhoraram muito este trabalho. A partir desta pesquisa, Yan LeCun apresentou a famosa arquitetura de rede neural LeNet-5. "
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "aiW2Q037WNzT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Existem similaridades e diferenças entre as CNNs e outros modelos de DNNs:\n",
        "\n",
        "* Normalmente usamos a classificação, embora a regressão ainda seja uma opção.\n",
        "* A entrada para a rede neural agora é 3D (altura, largura, cor)\n",
        "* Os dados não são transformados; sem zscores ou variáveis dummy.\n",
        "* O tempo de processamento é muito maior.\n",
        "* Agora temos diferentes camadas: camadas densas (como antes), camadas de convolução e camadas de Max Pooling.\n",
        "* Os dados não chegarão mais como arquivos CSV. O TensorFlow fornece alguns utilitários para ir diretamente da imagem para o input de uma rede neural.\n"
      ]
    },
    {
      "metadata": {
        "id": "8oidLrtiWNzX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Embora a Visão Computacional use principalmente CNNs, essa tecnologia possui aplicações em outras áreas. É possível utilizar CNNs em dados não visuais, mas nesse caso você deve encontrar uma maneira de codificar seus dados para que possa imitar as propriedades dos dados visuais. As CNNs são semelhantes à arquitetura de Mapas Auto Organizáveis (SOM - Self Organizing Maps). A ordem dos elementos vetoriais é crucial para o treinamento. Em contraste, a maioria das redes neurais que não são CNNs ou SOMs tratam seus dados de entrada como um vetor de valores, e a ordem em que você organiza as características de entrada neste vetor é irrelevante. Para esses tipos de redes neurais, você não pode alterar a ordem depois de ter treinado a rede. Em outras palavras, CNNs e SOMs não seguem o tratamento padrão de vetores de entrada."
      ]
    },
    {
      "metadata": {
        "id": "_QURrJSRWNzZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Normalmente, uma rede neural feedforward comum cria todas as possíveis conexões de peso entre duas camadas. Na terminologia de aprendizagem profunda, nos referimos a essas camadas como camadas densas. Além de não representar todo o peso possível, as redes neurais convolutivas também compartilharão pesos."
      ]
    },
    {
      "metadata": {
        "id": "cnMB-mexWNza",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MLPs x CNNs\n",
        "\n",
        "Existem algumas diferenças importantes entre modelos MLP e CNN:\n",
        "\n",
        "MLP:\n",
        "\n",
        "* Somente utiliza camadas totalmente conectadas\n",
        "* Somente aceita vetores como input\n",
        "\n",
        "CNN\n",
        "\n",
        "* Permite fazer tudo que é feito com MLPs\n",
        "* Utiliza camadas esparsas\n",
        "* Aceita matrizes como input"
      ]
    },
    {
      "metadata": {
        "id": "qOG499tlWNzc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Datasets Para Visão Computacional\n",
        "\n",
        "Existem muitos conjuntos de dados para visão computacional. Dois dos mais populares são o conjunto de dados MNIST e os conjuntos de dados de imagem CIFAR.\n",
        "\n",
        "## MNIST Digits Data Set\n",
        "\n",
        "O [MNIST Digits Data Set](http://yann.lecun.com/exdb/mnist/) é muito popular na comunidade de pesquisa da rede neural. São dígitos escritos à mão que foram escaneados e gravados em um dataset eletrônico.\n",
        "\n",
        "![MINIST](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/CNN/images/mnist.png?raw=true\"MINIST\")\n",
        "\n",
        "\n",
        "## CIFAR Data Set\n",
        "\n",
        "Os datasets [CIFAR-10 and CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) também são frequentemente usados pela comunidade de pesquisa de rede neural.\n",
        "\n",
        "![CIFAR](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/CNN/images/cifar.png?raw=true \"CIFAR\")\n",
        "\n",
        "O conjunto de dados CIFAR-10 contém imagens divididas em 10 classes. O conjunto de dados CIFAR-100 contém 100 classes em uma hierarquia."
      ]
    },
    {
      "metadata": {
        "id": "XIkvyUqGWNzc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Arquitetura LeNET-5 (LeCun, 1998)\n",
        "\n",
        "Podemos usar a arquitetura LeNET-5 principalmente para a classificação de imagens gráficas. Este tipo de rede é semelhante à rede feedforward que examinamos nos capítulos anteriores: fluxo de dados da entrada para a saída. No entanto, a rede LeNET-5 contém vários tipos de camadas diferentes, como mostra a figura abaixo:\n",
        "\n",
        "**A LeNET-5 Network (LeCun, 1998)**\n",
        "![LENET5](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/CNN/images/lenet5.png?raw=true  \"LENET5\")\n",
        "\n",
        "Existem várias diferenças importantes entre uma rede neural feedforward e uma rede LeNET-5:\n",
        "\n",
        "* Os vetores passam através de redes feedforward; Os cubos 3D passam através das redes LeNET-5. \n",
        "* As redes LeNET-5 contêm uma variedade de tipos de camadas. \n",
        "* Visão Computacional é a principal aplicação do LeNET-5.\n",
        "\n",
        "No entanto, também existem muitas semelhanças entre as DNNs e redes CNNs como a arquitetura LeNET-5. A semelhança mais importante é que podemos treinar o LeNET-5 com as mesmas técnicas baseadas em backpropagation. Qualquer algoritmo de otimização pode treinar os pesos de uma rede feedforward ou LeNET-5. O LeNET-5 frequentemente usa treinamento de backpropagation. Os seguintes tipos de camadas compreendem as redes neurais originais LeNET-5:\n",
        "\n",
        "* **Dense Layers** - Camadas totalmente conectadas.  \n",
        "* **Convolution Layers** - Usadas para \"escanear\" as imagens. \n",
        "* **Max Pooling Layers** - Usadas para reduzir as imagens. \n",
        "* **Dropout Layer** - Usadas para regularização da rede. \n",
        "\n",
        "Outras estruturas de rede neural adicionarão tipos de camada adicionais relacionados à visão computacional. A adição de novos tipos de camada é um meio comum de aumentar a pesquisa existente na rede neural. "
      ]
    },
    {
      "metadata": {
        "id": "DDCpjJSiWNze",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature Learning e Classificação\n",
        "\n",
        "![CNN](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/CNN/images/cnn.jpg?raw=true \"CNN\")"
      ]
    },
    {
      "metadata": {
        "id": "U50tMMqUWNzf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Camadas de Convolução\n",
        "\n",
        "A primeira camada que examinaremos é a camada convolucional. A camada convolucional é o bloco de construção central de uma rede convolutiva que faz a maior parte do processamento computacional pesado. Vamos primeiro discutir o que a camada convolucional calcula sem analogias cérebro / neurônio. Os parâmetros da camada convolucional consistem em um conjunto de filtros. Cada filtro é pequeno espacialmente (ao longo da largura e altura), mas se estende através da profundidade total do volume de entrada. Por exemplo, um filtro típico em uma primeira camada de uma ConvNet pode ter tamanho 5x5x3 (isto é, 5 pixels de largura e altura e 3, pois as imagens têm a profundidade 3, os canais de cores). Durante a passagem para a frente, deslocamos (mais precisamente, convolvemos) cada filtro na largura e altura do volume de entrada e os produtos de ponto de computação entre as entradas do filtro e a entrada em qualquer posição. Ao deslizar o filtro pela largura e altura do volume de entrada, produziremos um mapa de ativação bidimensional que dê as respostas desse filtro em todas as posições espaciais. Intuitivamente, a rede aprenderá filtros que se ativam quando vêem algum tipo de recurso visual, como uma borda de alguma orientação ou uma mancha de alguma cor na primeira camada, ou, eventualmente, padrões inteiros de favo de mel ou de roda em camadas mais altas da rede. Agora, teremos um conjunto completo de filtros em cada camada convolucional (por exemplo, 12 filtros), e cada um deles produzirá um mapa de ativação bidimensional separado. Vamos empilhar esses mapas de ativação ao longo da dimensão de profundidade e produzir o volume de saída.\n",
        "\n",
        "Se você é fã das analogias cérebro / neurônio, cada entrada no volume 3D também pode ser interpretada como uma saída de um neurônio que olha apenas uma pequena região na entrada e compartilha parâmetros com todos os neurônios à esquerda e espacialmente à direita (já que esses números resultam da aplicação do mesmo filtro). Agora discutimos os detalhes das conectividades dos neurônios, seu arranjo no espaço e seu esquema de compartilhamento de parâmetros.\n",
        "\n",
        "Ao lidar com entradas de alta dimensão como as imagens, não é prático conectar neurônios a todos os neurônios no volume anterior. Em vez disso, conectaremos cada neurônio a apenas uma região local do volume de entrada. A extensão espacial desta conectividade é um hiperparâmetro chamado campo receptivo do neurônio (esse é o tamanho do filtro). A extensão da conectividade ao longo do eixo de profundidade é sempre igual à profundidade do volume de entrada. É importante ressaltar novamente essa assimetria na forma como tratamos as dimensões espaciais (largura e altura) e a dimensão da profundidade: as conexões são locais no espaço (ao longo da largura e altura), mas sempre cheias em toda a profundidade do volume de entrada.\n",
        "\n",
        "Por exemplo, suponha que o volume de entrada tenha tamanho [32x32x3] (por exemplo, uma imagem RGB CIFAR-10). Se o campo receptivo (ou o tamanho do filtro) for 5x5, cada neurônio na camada convolucional terá pesos para uma região [5x5x3] no volume de entrada, para um total de 5 x 5 x 3 = 75 pesos (e +1 Parâmetro de bias). Observe que a extensão da conectividade ao longo do eixo da profundidade deve ser 3, uma vez que esta é a profundidade do volume de entrada.\n",
        "\n",
        "Começaremos por analisar os hiperparâmetros que você deve especificar para uma camada convolucional na maioria das estruturas de rede neural que suportam a CNN:\n",
        "\n",
        "* Número de Filtros\n",
        "* Tamanho do filtro (Filter Size)\n",
        "* Stride\n",
        "* Padding\n",
        "* Função de Ativação\n",
        "\n",
        "Nós explicamos a conectividade de cada neurônio na camada convolucional ao volume de entrada, mas ainda não discutimos quantos neurônios existem no volume de saída ou como eles estão dispostos. Três hiperparâmetros controlam o tamanho do volume de saída: a profundidade (depth), o passo (stride) e o preenchimento (padding). \n",
        "\n",
        "Primeiro, a profundidade do volume de saída é um hiperparâmetro: corresponde ao número de filtros que gostaríamos de usar, cada um aprendendo a procurar algo diferente na entrada. Por exemplo, se a primeira camada convolucional receber como entrada a imagem bruta, então diferentes neurônios ao longo da dimensão de profundidade podem se ativar na presença de várias arestas orientadas ou bolhas de cor. Vamos nos referir a um conjunto de neurônios que estão olhando para a mesma região da entrada como uma coluna de profundidade (algumas pessoas também preferem o termo fibra).\n",
        "Em segundo lugar, devemos especificar o passo com o qual deslizamos o filtro. Quando o passo é 1, movemos os filtros um pixel de cada vez. Quando o stride é 2 (ou incomumente 3 ou mais, embora isso seja raro na prática), então os filtros saltam 2 pixels de cada vez, enquanto os desliza. Isso produzirá volumes de saída menores espacialmente.\n",
        "Como veremos em breve, às vezes será conveniente adicionar o volume de entrada com zeros ao redor da borda. O tamanho desse preenchimento é um hiperparâmetro. O bom recurso do preenchimento é que nos permitirá controlar o tamanho espacial dos volumes de saída (mais comumente, como veremos em breve, vamos usá-lo para preservar exatamente o tamanho espacial do volume de entrada, de modo que a largura de entrada e saída e a altura são as mesmas).\n",
        "\n",
        "![CONV_LAYER](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/CNN/images/conv-layer.gif?raw=true \"CONV_LAYER\")\n",
        "\n",
        "Exemplo de arquitetura ConvNet para o CIFAR-10: [INPUT - CONV - RELU - POOL - FC]. Em mais detalhes:\n",
        "\n",
        "* INPUT [32x32x3] manterá os valores de pixel em estado bruto da imagem, neste caso uma imagem de largura 32, altura 32 e com três canais de cores R, G, B.\n",
        "\n",
        "* A camada CONV calculará a saída de neurônios que estão conectados a regiões locais na entrada, cada um calculando um dot product entre seus pesos e uma pequena região em que estão conectados no volume de entrada. Isso pode resultar em um volume como [32x32x12] se decidimos usar 12 filtros.\n",
        "\n",
        "* A camada RELU aplicará uma função de ativação elementar, como max(0, x) com limira igual a zero. Isso deixa o tamanho do volume inalterado ([32x32x12]).\n",
        "\n",
        "* A camada POOL executará uma operação de downsampling ao longo das dimensões espaciais (largura, altura), resultando em volume como [16x16x12].\n",
        "\n",
        "* A camada FC (ou seja, totalmente conectada) calculará os escores da classe, resultando em volume de tamanho [1x1x10], onde cada um dos 10 números corresponde a uma pontuação de classe, como entre as 10 categorias do CIFAR-10. Tal como acontece com as Redes Neurais comuns e como o nome indica, cada neurônio nesta camada será conectado a todos os números no volume anterior.\n",
        "\n",
        "O objetivo principal de uma camada convolucional é detectar características como bordas, linhas, bolhas de cor e outros elementos visuais. Os filtros podem detectar esses recursos. Quanto mais filtros nós damos a uma camada convolucional, mais recursos ela pode detectar.\n",
        "\n",
        "Um filtro é um objeto em forma de quadrado que escaneia a imagem. Uma grade pode representar os pixels individuais de uma grade. Você pode pensar na camada convolucional como uma grade menor que varre da esquerda para a direita sobre cada linha da imagem. Há também um hiperparâmetro que especifica tanto a largura quanto a altura do filtro em forma de quadrado. \n",
        "\n",
        "Uma camada convolucional tem pesos entre ela e a camada anterior ou grade de imagem. Cada pixel em cada camada convolucional é um peso. Portanto, o número de pesos entre uma camada convolucional e sua camada anterior ou campo de imagem é o seguinte:\n",
        "\n",
        "```\n",
        "[FilterSize] * [FilterSize] * [# de Filtros]\n",
        "```\n",
        "\n",
        "Por exemplo, se o tamanho do filtro fosse 5 (5x4) para 10 filtros, haveria 250 pesos.\n",
        "\n",
        "Você precisa entender como os filtros convolucionais varrem a saída da camada anterior ou a grade da imagem. A figura abaixo ilustra a varredura:\n",
        "\n",
        "![GRID](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/CNN/images/cnn_grid.png?raw=true  \"GRID\")\n",
        "\n",
        "A figura acima mostra um filtro convolucional com um tamanho de 4 e um tamanho de preenchimento de 1. O tamanho do preenchimento é responsável pelo limite de zeros na área que o filtro varre. Mesmo que a imagem seja realmente 8x7, o preenchimento extra fornece um tamanho de imagem virtual de 9x8 para que o filtro varre. O passo especifica o número de posições nas quais os filtros convolucionais vão parar. Os filtros convolucionais se movem para a direita, avançando pelo número de células especificadas no passo. Uma vez que a extrema direita é alcançada, o filtro convolucional volta para o extremo esquerdo, então ele se move para baixo pelo montante do passo e continua a direita novamente.\n",
        "\n",
        "Existem algumas restrições em relação ao tamanho do passo. Obviamente, o passo não pode ser 0. O filtro convolucional nunca se moveria se o passo fosse ajustado para 0. Além disso, nem o passo, nem o tamanho do filtro convolucional podem ser maiores do que a grade anterior. Existem restrições adicionais sobre o (s) passo (s), preenchimento (p) e a largura do filtro (f) para uma imagem de largura (w). Especificamente, o filtro convolucional deve ser capaz de começar no limite superior esquerdo ou superior, mover um certo número de passos e aterrar na fronteira extrema ou inferior. A seguinte equação mostra o número de etapas de um operador convolucional deve atravessar a imagem:\n",
        "\n",
        "$ steps = \\frac{w - f + 2p}{s+1} $\n",
        "\n",
        "O número de etapas deve ser um número inteiro. Em outras palavras, não pode ter casas decimais. O propósito do preenchimento (p) deve ser ajustado para tornar esta equação tornar-se um valor inteiro."
      ]
    },
    {
      "metadata": {
        "id": "olRZwFvuWNzg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Compartilhamento de Parâmetros\n",
        "\n",
        "A arquitetura que ganhou o desafio ImageNet em 2012 recebeu imagens de tamanho [227x227x3]. Na primeira Camada Convolucional, utilizou neurônios com tamanho de campo receptivo F = 11, passo (stride) S = 4 e sem preenchimento (zero padding) P = 0. Desde (227 - 11) / 4 + 1 = 55, e uma vez que a camada Conv teve uma profundidade de K = 96, o volume de saída da camada Conv teve tamanho [55x55x96]. Cada um dos 55 * 55 * 96 neurônios neste volume foi conectado a uma região de tamanho [11x11x3] no volume de entrada. Além disso, todos os 96 neurônios em cada coluna de profundidade estão conectados à mesma região [11x11x3] da entrada, mas é claro com pesos diferentes. Se você ler o documento atual da pesquisa (na seção de links úteis) verá que as imagens de entrada eram 224x224, o que certamente é incorreto porque (224 - 11) / 4 + 1 é claramente não um número inteiro. Isso confundiu muitas pessoas na história dos ConvNets e pouco se sabe sobre o que aconteceu. O melhor palpite é que Alex usou zero-padding de 3 pixels extras que ele não menciona no papel.\n",
        "\n",
        "O esquema de compartilhamento de parâmetros é usado em Camadas Convolucionais para controlar o número de parâmetros. Usando o exemplo acima, vemos que existem 55 * 55 * 96 = 290.400 neurônios na primeira camada Conv, e cada um tem 11 x 11 x 3 = 363 pesos e 1 bias. Juntos, isso acrescenta a 290400 * 364 = 105.705.600 parâmetros na primeira camada do ConvNet sozinho. Claramente, esse número é muito alto.\n",
        "\n",
        "Acontece que podemos reduzir drasticamente o número de parâmetros fazendo uma suposição razoável: se um recurso é útil para calcular em alguma posição espacial (x, y), então também deve ser útil para calcular em uma posição diferente (x2 , Y2). Em outras palavras, indicando uma única fatia de profundidade bidimensional como uma fatia de profundidade (por exemplo, um volume de tamanho [55x55x96] tem 96 fatias de profundidade, cada um de tamanho [55x55]), vamos restringir os neurônios em cada fatia de profundidade para usar os mesmos pesos e bias. Com este esquema de compartilhamento de parâmetros, a primeira camada Conv em nosso exemplo agora teria apenas 96 conjuntos de pesos exclusivos (um para cada trecho de profundidade), para um total de 96 * 11 * 11 * 3 = 34.848 pesos exclusivos ou 34.944 parâmetros (+96 bias). Alternativamente, todos os 55 x 55 neurônios em cada fatia de profundidade agora estarão usando os mesmos parâmetros. Na prática, durante a pós-propagação, cada neurônio no volume calculará o gradiente para os pesos, mas esses gradientes serão adicionados em cada fatia de profundidade e atualizarão apenas um único conjunto de pesos por fatia."
      ]
    },
    {
      "metadata": {
        "id": "Zke0wxMXWNzi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Camadas de Max Pooling \n",
        "\n",
        "As camadas Max Pooling reduzem a caixa 3D para uma nova com dimensões menores. Normalmente, você sempre pode colocar uma camada de Max Pooling imediatamente após a camada convolucional. O LENET mostra a camada de Max Pooling imediatamente após as camadas C1 e C3. Essas camadas de Max Pooling diminuem progressivamente o tamanho das dimensões das caixas 3D passando por elas. Esta técnica pode evitar o overfitting (Krizhevsky, Sutskever & Hinton, 2012).\n",
        "\n",
        "É comum inserir periodicamente uma camada de Max Pooling (também chamada de agrupamento) entre as camadas Convolucionais sucessivas em uma arquitetura ConvNet. Sua função é reduzir progressivamente o tamanho espacial da representação para reduzir a quantidade de parâmetros e computação na rede e, portanto, também controlar o overfitting. A Camada de agrupamento opera independentemente em cada fatia de profundidade da entrada e redimensiona-a espacialmente, usando a operação MAX. A forma mais comum é uma camada de agrupamento com filtros de tamanho 2x2 aplicados com um passo de 2 amostras descendentes de cada fatia de profundidade na entrada por 2 ao longo de largura e altura, descartando 75% das ativações. Toda operação MAX, neste caso, receberia um máximo de 4 números (pequena região 2x2 em uma fatia de profundidade). A dimensão da profundidade permanece inalterada. Mais geralmente, a camada de pooling:\n",
        "\n",
        "![MaxPool](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/CNN/images/conv_maxpool2.png?raw=true  \"MaxPool\")\n",
        "\n",
        "Uma camada de Max Pooling possui os seguintes hiperparâmetros:\n",
        "\n",
        "* Spatial Extent (f)\n",
        "* Stride (s)\n",
        "\n",
        "Ao contrário das camadas convolucionais, as camadas Max Pooling não usam preenchimento. Além disso, as camadas de Max Pooling não possuem pesos, então o treinamento não as afeta. Essas camadas simplesmente diminuem sua entrada na caixa 3D. A saída da caixa 3D por uma camada de Max Pooling terá uma largura igual a esta equação:\n",
        "\n",
        "$ w_2 = \\frac{w_1 - f}{s + 1} $\n",
        "\n",
        "A altura da caixa 3D produzida pela camada de Max Pooling é calculada de forma semelhante com esta equação:\n",
        "\n",
        "$ h_2 = \\frac{h_1 - f}{s + 1} $\n",
        "\n",
        "A profundidade da caixa 3D produzida pela camada max-pool é igual à profundidade que a caixa 3D recebeu como entrada. A configuração mais comum para hiper-parâmetros de uma camada de pool máximo são f = 2 e s = 2. A extensão espacial (f) especifica que as caixas de 2x2 serão dimensionadas para pixels individuais. Destes quatro pixels, o pixel com o valor máximo representará o pixel 2x2 na nova grade. Como os quadrados do tamanho 4 são substituídos pelo tamanho 1, 75% das informações do pixel são perdidas. A figura a seguir mostra esta transformação como uma grade 6x6 torna-se um 3x3:\n",
        "\n",
        "![MaxPool](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/CNN/images/conv_maxpool.png?raw=true  \"MaxPool\")\n",
        "\n",
        "Claro, o diagrama acima mostra cada pixel como um único número. Uma imagem em escala de cinza teria essa característica. Para uma imagem RGB, geralmente tomamos a média dos três números para determinar qual pixel tem o valor máximo."
      ]
    },
    {
      "metadata": {
        "id": "Y_z4DYJ5WNzm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Camadas Densas\n",
        "\n",
        "O tipo de camada final em uma rede LeNET-5 é uma camada densa. Esse tipo de camada é exatamente o mesmo tipo de camada que já vimos em redes neurais feedforward. Uma camada densa conecta cada elemento (neurônio) na caixa 3D de saída da camada anterior para cada neurônio na camada densa. O vetor resultante é passado através de uma função de ativação. As redes LeNET-5 normalmente usarão uma ativação ReLU. No entanto, podemos usar uma função de ativação sigmoid. Essa técnica é menos comum. Uma camada densa geralmente contém os seguintes hiperparâmetros:\n",
        "\n",
        "* Neuron Count \n",
        "* Activation Function\n",
        "\n",
        "A contagem de neurônios especifica o número de neurônios que compõem essa camada. A função de ativação indica o tipo de função de ativação a ser utilizada. As camadas densas podem empregar muitos tipos diferentes de funções de ativação, como ReLU, sigmoide ou tangente hiperbólica. As redes LeNET-5 normalmente contêm várias camadas densas como suas camadas finais. A camada final densa em uma LeNET-5 realmente executa a classificação. Deve haver um neurônio de saída para cada classe, ou tipo de imagem, para classificar. Por exemplo, se a rede distingue entre cães, gatos e aves, haverá três neurônios de saída. Você pode aplicar uma função final de softmax para a camada final para tratar os neurônios de saída como probabilidades. O Softmax permite que cada neurônio forneça a probabilidade de a imagem representar cada classe. Como os neurônios de saída são agora probabilidades, o softmax garante que eles somam 1,0 (100%). \n",
        "\n",
        "Esse diagrama resume bem a arquitetura de uma CNN:\n",
        "\n",
        "![CNN](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/CNN/images/cnn.jpg?raw=true  \"CNN\")"
      ]
    },
    {
      "metadata": {
        "id": "zhUALeqHWNzo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Construindo uma CNN com TensorFlow e Keras\n",
        "\n",
        "O Keras fornece classes de acesso incorporadas para o MNIST. É importante notar que os dados MNIST chegam já separados em dois conjuntos:\n",
        "\n",
        "* **treino** \n",
        "* **teste** "
      ]
    },
    {
      "metadata": {
        "id": "AbFEVUGYWNzs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "0dbe91f3-f4bd-47ad-af44-f3bbe0097eef"
      },
      "cell_type": "code",
      "source": [
        "# Checando a memória da GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jan 19 11:39:39 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 396.44                 Driver Version: 396.44                    |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    70W / 149W |    170MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uskCOhEvWNzy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4ab65081-09c9-4267-d201-251eb9a4e8f0"
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# Treino e Teste\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Print do Shape\n",
        "print(\"Shape x_train: {}\".format(x_train.shape))\n",
        "print(\"Shape y_train: {}\".format(y_train.shape))\n",
        "print()\n",
        "print(\"Shape x_test: {}\".format(x_test.shape))\n",
        "print(\"Shape y_test: {}\".format(y_test.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape x_train: (60000, 28, 28)\n",
            "Shape y_train: (60000,)\n",
            "\n",
            "Shape x_test: (10000, 28, 28)\n",
            "Shape y_test: (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zDIuRfjaWNz5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Visualizando os Dígitos"
      ]
    },
    {
      "metadata": {
        "id": "wuRIswbTWNz6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1019
        },
        "outputId": "e55ebc1e-b07c-4ef8-a628-67e20f7cb8fa"
      },
      "cell_type": "code",
      "source": [
        "# Display como texto\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Shape do dataset: {}\".format(x_train.shape))\n",
        "print(\"Labels: {}\".format(y_train))\n",
        "\n",
        "# Display de um único dígito\n",
        "single = x_train[0]\n",
        "print(\"\\nShape de um Único Dígito: {}\".format(single.shape))\n",
        "print(\"\\nDígito: {}\".format(y_train[0]))\n",
        "\n",
        "#display(pd.DataFrame(single.reshape(28,28)))\n",
        "df = pd.DataFrame(single.reshape(28,28))\n",
        "df.loc[:,4:23] # onde pixel > 0"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape do dataset: (60000, 28, 28)\n",
            "Labels: [5 0 4 ... 5 6 8]\n",
            "\n",
            "Shape de um Único Dígito: (28, 28)\n",
            "\n",
            "Dígito: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>126</td>\n",
              "      <td>136</td>\n",
              "      <td>175</td>\n",
              "      <td>26</td>\n",
              "      <td>166</td>\n",
              "      <td>255</td>\n",
              "      <td>247</td>\n",
              "      <td>127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>36</td>\n",
              "      <td>94</td>\n",
              "      <td>154</td>\n",
              "      <td>170</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>225</td>\n",
              "      <td>172</td>\n",
              "      <td>253</td>\n",
              "      <td>242</td>\n",
              "      <td>195</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>238</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>93</td>\n",
              "      <td>82</td>\n",
              "      <td>82</td>\n",
              "      <td>56</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>219</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>198</td>\n",
              "      <td>182</td>\n",
              "      <td>247</td>\n",
              "      <td>241</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>80</td>\n",
              "      <td>156</td>\n",
              "      <td>107</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>205</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "      <td>154</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>154</td>\n",
              "      <td>253</td>\n",
              "      <td>90</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>139</td>\n",
              "      <td>253</td>\n",
              "      <td>190</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>190</td>\n",
              "      <td>253</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>241</td>\n",
              "      <td>225</td>\n",
              "      <td>160</td>\n",
              "      <td>108</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>81</td>\n",
              "      <td>240</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>119</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>45</td>\n",
              "      <td>186</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>150</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>93</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>249</td>\n",
              "      <td>253</td>\n",
              "      <td>249</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>46</td>\n",
              "      <td>130</td>\n",
              "      <td>183</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>207</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39</td>\n",
              "      <td>148</td>\n",
              "      <td>229</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>250</td>\n",
              "      <td>182</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>114</td>\n",
              "      <td>221</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>201</td>\n",
              "      <td>78</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>66</td>\n",
              "      <td>213</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>198</td>\n",
              "      <td>81</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>171</td>\n",
              "      <td>219</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>195</td>\n",
              "      <td>80</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>55</td>\n",
              "      <td>172</td>\n",
              "      <td>226</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>244</td>\n",
              "      <td>133</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>136</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>212</td>\n",
              "      <td>135</td>\n",
              "      <td>132</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     4    5    6    7    8    9    10   11   12   13   14   15   16   17   18  \\\n",
              "0     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
              "1     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
              "2     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
              "3     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
              "4     0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
              "5     0    0    0    0    0    0    0    0    3   18   18   18  126  136  175   \n",
              "6     0    0    0    0   30   36   94  154  170  253  253  253  253  253  225   \n",
              "7     0    0    0   49  238  253  253  253  253  253  253  253  253  251   93   \n",
              "8     0    0    0   18  219  253  253  253  253  253  198  182  247  241    0   \n",
              "9     0    0    0    0   80  156  107  253  253  205   11    0   43  154    0   \n",
              "10    0    0    0    0    0   14    1  154  253   90    0    0    0    0    0   \n",
              "11    0    0    0    0    0    0    0  139  253  190    2    0    0    0    0   \n",
              "12    0    0    0    0    0    0    0   11  190  253   70    0    0    0    0   \n",
              "13    0    0    0    0    0    0    0    0   35  241  225  160  108    1    0   \n",
              "14    0    0    0    0    0    0    0    0    0   81  240  253  253  119   25   \n",
              "15    0    0    0    0    0    0    0    0    0    0   45  186  253  253  150   \n",
              "16    0    0    0    0    0    0    0    0    0    0    0   16   93  252  253   \n",
              "17    0    0    0    0    0    0    0    0    0    0    0    0    0  249  253   \n",
              "18    0    0    0    0    0    0    0    0    0    0   46  130  183  253  253   \n",
              "19    0    0    0    0    0    0    0    0   39  148  229  253  253  253  250   \n",
              "20    0    0    0    0    0    0   24  114  221  253  253  253  253  201   78   \n",
              "21    0    0    0    0   23   66  213  253  253  253  253  198   81    2    0   \n",
              "22    0    0   18  171  219  253  253  253  253  195   80    9    0    0    0   \n",
              "23   55  172  226  253  253  253  253  244  133   11    0    0    0    0    0   \n",
              "24  136  253  253  253  212  135  132   16    0    0    0    0    0    0    0   \n",
              "25    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
              "26    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
              "27    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
              "\n",
              "     19   20   21   22   23  \n",
              "0     0    0    0    0    0  \n",
              "1     0    0    0    0    0  \n",
              "2     0    0    0    0    0  \n",
              "3     0    0    0    0    0  \n",
              "4     0    0    0    0    0  \n",
              "5    26  166  255  247  127  \n",
              "6   172  253  242  195   64  \n",
              "7    82   82   56   39    0  \n",
              "8     0    0    0    0    0  \n",
              "9     0    0    0    0    0  \n",
              "10    0    0    0    0    0  \n",
              "11    0    0    0    0    0  \n",
              "12    0    0    0    0    0  \n",
              "13    0    0    0    0    0  \n",
              "14    0    0    0    0    0  \n",
              "15   27    0    0    0    0  \n",
              "16  187    0    0    0    0  \n",
              "17  249   64    0    0    0  \n",
              "18  207    2    0    0    0  \n",
              "19  182    0    0    0    0  \n",
              "20    0    0    0    0    0  \n",
              "21    0    0    0    0    0  \n",
              "22    0    0    0    0    0  \n",
              "23    0    0    0    0    0  \n",
              "24    0    0    0    0    0  \n",
              "25    0    0    0    0    0  \n",
              "26    0    0    0    0    0  \n",
              "27    0    0    0    0    0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "-r_DWCLaWNz_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "9d280105-b283-479d-f39c-b364c081fa50"
      },
      "cell_type": "code",
      "source": [
        "# Display como imagem\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "digit = 101 \n",
        "digit = 0\n",
        "a = x_train[digit]\n",
        "plt.imshow(a, cmap = 'gray', interpolation = 'nearest')\n",
        "print(\"Imagem (#{}) é o dígito '{}'\".format(digit, y_train[digit]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imagem (#0) é o dígito '5'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAD9hJREFUeJzt3XuMVGWax/FvN4asEBVxssKwYwju\n5HG12pBh1cF4gVWGwegqlwkhXogSmWxkYrIZE0f9Q0hwzCC6QdgJk9n1toHgLQM4o4KA8ocJKyqm\ne5g8O04mJkJPUIaWm6JA7R9d3dNV9Hmr6vSpqgPv75N0rHOeOqceq/tHnXPeqnrbisUiInJma291\nAyLSeAq6SAQUdJEIKOgiEVDQRWJQLBYb/gMUB/50dnYWK9fl5Ue9qbfTta9QBtvSDq+Z2VPA90sP\ncr+7v5d037a2trIHKRaLtLW1pXrcRlNv6ai3+mXdV7FYTNxZqkN3M7se+K67TwYWACtS9iYiTZD2\nHP0G4DcA7v4H4HwzOzezrkQkU2el3G4M8P6A5c9K6w4OdufOzk4KhULZujy/I0+9paPe6tesvtIG\nvVLwRKOjo6NsOa/nTKDe0lJv9WvAOXpiLe2h+156X8H7fBvoTrkvEWmwtEHfBMwBMLPvAXvd/VBm\nXYlIplIF3d3fBd43s3fpveJ+X6ZdiUimUo+j1/UgGkfPhHpLJ6+95X4cXUROLwq6SAQUdJEIKOgi\nEVDQRSKgoItEQEEXiYCCLhIBBV0kAgq6SAQUdJEIKOgiEVDQRSKgoItEQEEXiYCCLhIBBV0kAgq6\nSAQUdJEIKOgiEVDQRSKgoItEQEEXiYCCLhIBBV0kAgq6SAQUdJEIKOgiEVDQRSJwVqsbkMYYNmxY\nsH7eeedl/pijR4/uv71o0aLE+40YMSK4HzML1u+7LzxL9xNPPHHKujVr1gAwb9684LZfffVVsP74\n448H64sXLw7WWyVV0M1sCvAS8PvSqk53/0lWTYlItobyiv6Ou8/JrBMRaRido4tEoK1YLNa9UenQ\n/T+Bj4HRwGJ335x0/66urmKhUEjbo4jUpi2xkDLo44BrgBeBCcA24B/d/etBH6StrexBisUibW2J\nPbXUmdJbsy/G7d+/nwsuuKB/OU8X4+bNm8fatWv7b4c082Jc1n9rxWIxcWepztHdfQ+wrrT4JzP7\nCzAO+HOa/YlIY6U6Rzez283sp6XbY4ALgT1ZNiYi2Ul71X0DsMbMbgWGA/+WdNges4suuihYHz58\neLB+9dVXn7Lurrvu6r99zTXXJG47atSo4L5nz54drKfx2WefZbKfTz/9NFhfsWJFsD5z5sxT1s2d\nOxeAQ4cOBbf96KOPgvV33nknWM+rtIfuh4BbMu5FRBpEw2siEVDQRSKgoItEQEEXiYCCLhKBVO+M\nq/tBztB3xk2cODFY37p1a7Be77vT2tvbOXnyZF3bNEs9vVW73z333BOsHz58uOa+AF599VVmzZoF\nQHd3d/C+Bw4cCNbdva7HDmnmO+P0ii4SAQVdJAIKukgEFHSRCCjoIhFQ0EUioKCLREDj6BXq6W3g\n1xsPZseOHcH6hAkTau4LmjuOXq33np6esuUZM2bw+uuv9y9PnTo1cduvvw5/ojnrb7/J69+bxtFF\nJFMKukgEFHSRCCjoIhFQ0EUioKCLREBBF4mAxtErZNnbbbfdFqzffPPNwfqHH35Ytrxy5cqyGVCq\nfe1xyK5du4L16667Llg/cuRI2XLl83bZZZclbnv//fcH971w4cJgvV55/XvTOLqIZEpBF4mAgi4S\nAQVdJAIKukgEFHSRCCjoIhHQOHqFZvZ27rnnBuuVU/yePHmS9va//du8evXqxG0XLFgQ3Pcdd9wR\nrK9duzZYr6Tfaf2aOY5e07TJZlYA1gNPuftKM/sO8AIwDOgG7nT3Y1k0KyLZq3robmYjgaeBLQNW\nLwFWufu1wMdAeGoNEWmpWs7RjwE3AXsHrJsCbCjd3gjcmG1bIpKlqofu7n4cOG5mA1ePHHCovg8Y\nG9pHZ2cnhUKhbF0zrg2klefesvrOuDVr1gypPpg8P2957a1ZfdV0jl5F1asJHR0dZct5vTgCuhjX\nRxfjGq8BF+MSa2mH1w6b2dml2+MoP6wXkZxJG/S3gNml27OBN7JpR0Qaoeqhu5lNApYD44FvzGwO\ncDvwrJn9GPgEeK6RTZ6pDh48WPc2Aw/Pvvjii9SPfe+99wbr69atC9bzOk+7DK6Wi3Hv03uVvdK0\nzLsRkYbQW2BFIqCgi0RAQReJgIIuEgEFXSQC+phqhdOpt5EjRybed+PGjcF9XX/99cH6jBkzgvVN\nmzYFe8uTvPamr3sWkUwp6CIRUNBFIqCgi0RAQReJgIIuEgEFXSQCGkevcKb0dvHFFwfrH3zwQbDe\n09MTrG/btq1sef78+Tz33N8+rbxz587EbVetWhXcd9Z/k3n9nWocXUQypaCLREBBF4mAgi4SAQVd\nJAIKukgEFHSRCGgcvUIsvc2cOTNYf+aZZ4L1c845p2y5vb295q+Afuihh4L1559/Pljv7u6u6XH6\n5PV3qnF0EcmUgi4SAQVdJAIKukgEFHSRCCjoIhFQ0EUioHH0CuqtV6FQCNaffPLJsuVp06axefPm\n/uUbbrgh9WOvXr06WF+6dGmwvmfPnrLlvP5OmzmOXnXaZAAzKwDrgafcfaWZPQtMAvaX7rLM3X87\n1EZFpDGqBt3MRgJPA1sqSj9z99ca0pWIZKqWc/RjwE3A3gb3IiINUvM5upk9Cnw+4NB9DDAc2Acs\ncvfPk7bt6uoqVjvnE5EhG9o5+iBeAPa7+y4zexB4FFiUdOeOjo6y5bxeHAH11kcX4xqvARfjEmup\ngu7uA8/XNwC/TLMfEWmOVOPoZvaKmU0oLU4BujLrSEQyV/Uc3cwmAcuB8cA3wB56r8I/CBwFDgN3\nu/u+xAfROHom8tTbqFGjypYPHDjA+eef3798yy23JG5b7bPu1f4ft27dGqxPmzatbDlPz9tAuRpH\nd/f36X3VrvTKEHoSkSbSW2BFIqCgi0RAQReJgIIuEgEFXSQC+phqBfWWTj29HTt2LFg/66zwYNDx\n48eD9enTp5ctb9u2jalTpwLw9ttvV2+wSfR1zyKSKQVdJAIKukgEFHSRCCjoIhFQ0EUioKCLRCDt\nN8zIGe7yyy8P1ufMmXPKuiVLlvTfvuKKKxK3rTZOXs3u3buD9e3bt9e0LiZ6RReJgIIuEgEFXSQC\nCrpIBBR0kQgo6CIRUNBFIqBx9DOUmQXrixYlTqwDwKxZs4L1MWPGnLLu4Ycfrt5YDU6cOBGsd3d3\nB+snT56saV1M9IouEgEFXSQCCrpIBBR0kQgo6CIRUNBFIqCgi0RA4+g5NthY9cB18+bNS9y22jj5\n+PHjU/c1VDt37gzWly5dGqxv2LAhy3aiUFPQzewXwLWl+/8ceA94ARgGdAN3unv4W/lFpGWqHrqb\n2VSg4O6TgR8C/wEsAVa5+7XAx8A9De1SRIaklnP07cCPSrd7gJHAFKDv+GkjcGPmnYlIZuqae83M\nFtJ7CD/d3f++tO5i4AV3vzppu66urmKhUBhqryISljj3Ws0X48zsVmAB8APgj7XsvE9HR0fZ8pky\nWWCjVV6M6+7uZuzYsf3LeboY197eXvMHR5p9MS5Pv9OBGjDJYmKtpuE1M5sOPAzMcPcvgMNmdnap\nPA7YO9QmRaRxqr6im9l5wDLgRnf/a2n1W8Bs4H9K/32jYR2exi688MJg/dJLLw3WV65cecq6LVu2\n9N++5JJL0jWWgR07dpQtT548uWzdsmXLErddv359cN+xf6S0EWo5dJ8LfAt4ccBnnOcDvzazHwOf\nAM81pj0RyULVoLv7r4BfDVKaln07ItIIegusSAQUdJEIKOgiEVDQRSKgoItEoK63wKZ+kLa2sgfJ\n6zuV4NTeRo8enXjf1atXB/c1ceLEYH3ChAl19VbPu8+qeffdd4P15cuXB+tvvvlm2fLRo0cZMWJE\n//KXX36ZvrmM5fXvrQHvjEvcmV7RRSKgoItEQEEXiYCCLhIBBV0kAgq6SAQUdJEInPFf93zVVVcF\n6w888MAp615++eX+21deeWXituPGjUvfWAaOHj2aWFuxYkVw28ceeyxYP3LkSN395GnsXMrpFV0k\nAgq6SAQUdJEIKOgiEVDQRSKgoItEQEEXicAZP44+c+bMuuvVtqnV7t27g/XXXnstWD9+/HjZ8iOP\nPFI2/h36zHhPT08NHUos9IouEgEFXSQCCrpIBBR0kQgo6CIRUNBFIqCgi0Sgpu91N7NfANfSO+7+\nc+BfgUnA/tJdlrn7bxMf5DT+Xvc8UW/p5LW3Zn6ve9U3zJjZVKDg7pPN7ALgQ2Ar8DN3D7/jQ0Ry\noZZ3xm0H/rd0uwcYCQxrWEcikrm6pmQys4X0HsKfAMYAw4F9wCJ3/zxpu66urmKhUBhiqyJSReKh\ne81BN7NbgYeAHwD/DOx3911m9iDwD+6+KPFBdI6eCfWWTl57y9U5OoCZTQceBn7o7l8AWwaUNwC/\nHFKHItJQVYfXzOw8YBlws7v/tbTuFTPrmwp0CtDVsA5FZMhqeUWfC3wLeNHM+tY9A6wzs6PAYeDu\nxrQnIlnQ/OgV1Fs66q1+mh9dRDKloItEQEEXiYCCLhIBBV0kAgq6SAQUdJEIKOgiEVDQRSKgoItE\nQEEXiYCCLhIBBV0kAgq6SASa8jFVEWktvaKLREBBF4mAgi4SAQVdJAIKukgEFHSRCCjoIhGoaaaW\nLJnZU8D3gSJwv7u/1+weBmNmU4CXgN+XVnW6+09a1xGYWQFYDzzl7ivN7DvAC/ROctkN3Onux3LS\n27PUMZV2g3urnOb7PXLwvA11+vGhaGrQzex64LulKZj/CfhvYHIze6jiHXef0+omAMxsJPA05dNf\nLQFWuftLZvYYcA8tmA4roTfIwVTaCdN8b6HFz1urpx9v9qH7DcBvANz9D8D5ZnZuk3s4XRwDbgL2\nDlg3hd657gA2Ajc2uac+g/WWF9uBH5Vu903zPYXWP2+D9dW06cebfeg+Bnh/wPJnpXUHm9xHkkvN\nbAMwGljs7ptb1Yi7HweOD5gGC2DkgEPOfcDYpjdGYm8Ai8zs36lhKu0G9nYCOFJaXAD8Dpje6uct\noa8TNOk5a/XFuDzNk/NHYDFwKzAf+C8zG97aloLy9NxB7znwg+7+L8Au4NFWNlOa5nsBUDmdd0uf\nt4q+mvacNfsVfS+9r+B9vk3vxZGWc/c9wLrS4p/M7C/AOODPrevqFIfN7Gx3/5Le3nJz6OzuuZlK\nu3KabzPLxfPWyunHm/2KvgmYA2Bm3wP2uvuhJvcwKDO73cx+Wro9BrgQ2NPark7xFjC7dHs28EYL\neymTl6m0B5vmmxw8b62efrzpH1M1s8eB64CTwH3u/lFTG0hgZucAa4BRwHB6z9F/18J+JgHLgfHA\nN/T+o3M78Czwd8AnwN3u/k1OensaeBDon0rb3fe1oLeF9B4C/9+A1fOBX9PC5y2hr2foPYRv+HOm\nz6OLRKDVF+NEpAkUdJEIKOgiEVDQRSKgoItEQEEXiYCCLhKB/wcGHQ6X7PrItwAAAABJRU5ErkJg\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "SECFM0jcWN0F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Preparando os Dados"
      ]
    },
    {
      "metadata": {
        "id": "d1NHVWkjWN0G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3d2e7c34-2698-44cc-9601-5927bfb51bf3"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import backend as backend\n",
        "\n",
        "# Input das dimensões das imagens\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# Reshape dos dados de treino e de teste e input_shape\n",
        "if backend.image_data_format() == 'channels_first': # canal de cores\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "# Conversão para float32 \n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Escala\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Print\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(\"Exemplos de Treino: {}\".format(x_train.shape[0]))\n",
        "print(\"Exemplos de Teste: {}\".format(x_test.shape[0]))\n",
        "print(\"Input Shape: {}\".format(input_shape))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "Exemplos de Treino: 60000\n",
            "Exemplos de Teste: 10000\n",
            "Input Shape: (28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nJNKTZU0WN0Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "1533ec30-ded7-495d-b029-b82e1e80ed6f"
      },
      "cell_type": "code",
      "source": [
        "# Display de um único dígito NORMALIZADO\n",
        "single = x_train[0]\n",
        "print(\"\\nShape de um Único Dígito: {}\".format(single.shape))\n",
        "print(\"\\nDígito: {}\".format(y_train[0]))\n",
        "\n",
        "#display(pd.DataFrame(single.reshape(28,28)))\n",
        "\n",
        "pd.options.display.float_format  = '{:,.2f}'.format\n",
        "df = pd.DataFrame(single.reshape(28,28)) \n",
        "\n",
        "df2 = df.copy()\n",
        "for lin in range(28):\n",
        "    for col in range(28):\n",
        "        if df2.loc[lin,col] == 0.0:\n",
        "            df2.loc[lin,col] = 0     \n",
        "\n",
        "df2.loc[5:24,4:23] # onde pixel > 0"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Shape de um Único Dígito: (28, 28, 1)\n",
            "\n",
            "Dígito: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.65</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.22</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.53</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     4    5    6    7    8    9    10   11   12   13   14   15   16   17   18  \\\n",
              "5  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.07 0.07 0.07 0.49 0.53 0.69   \n",
              "6  0.00 0.00 0.00 0.00 0.12 0.14 0.37 0.60 0.67 0.99 0.99 0.99 0.99 0.99 0.88   \n",
              "7  0.00 0.00 0.00 0.19 0.93 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.36   \n",
              "8  0.00 0.00 0.00 0.07 0.86 0.99 0.99 0.99 0.99 0.99 0.78 0.71 0.97 0.95 0.00   \n",
              "9  0.00 0.00 0.00 0.00 0.31 0.61 0.42 0.99 0.99 0.80 0.04 0.00 0.17 0.60 0.00   \n",
              "10 0.00 0.00 0.00 0.00 0.00 0.05 0.00 0.60 0.99 0.35 0.00 0.00 0.00 0.00 0.00   \n",
              "11 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.55 0.99 0.75 0.01 0.00 0.00 0.00 0.00   \n",
              "12 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.75 0.99 0.27 0.00 0.00 0.00 0.00   \n",
              "13 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.14 0.95 0.88 0.63 0.42 0.00 0.00   \n",
              "14 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.32 0.94 0.99 0.99 0.47 0.10   \n",
              "15 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.18 0.73 0.99 0.99 0.59   \n",
              "16 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.06 0.36 0.99 0.99   \n",
              "17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.98 0.99   \n",
              "18 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.18 0.51 0.72 0.99 0.99   \n",
              "19 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.15 0.58 0.90 0.99 0.99 0.99 0.98   \n",
              "20 0.00 0.00 0.00 0.00 0.00 0.00 0.09 0.45 0.87 0.99 0.99 0.99 0.99 0.79 0.31   \n",
              "21 0.00 0.00 0.00 0.00 0.09 0.26 0.84 0.99 0.99 0.99 0.99 0.78 0.32 0.01 0.00   \n",
              "22 0.00 0.00 0.07 0.67 0.86 0.99 0.99 0.99 0.99 0.76 0.31 0.04 0.00 0.00 0.00   \n",
              "23 0.22 0.67 0.89 0.99 0.99 0.99 0.99 0.96 0.52 0.04 0.00 0.00 0.00 0.00 0.00   \n",
              "24 0.53 0.99 0.99 0.99 0.83 0.53 0.52 0.06 0.00 0.00 0.00 0.00 0.00 0.00 0.00   \n",
              "\n",
              "     19   20   21   22   23  \n",
              "5  0.10 0.65 1.00 0.97 0.50  \n",
              "6  0.67 0.99 0.95 0.76 0.25  \n",
              "7  0.32 0.32 0.22 0.15 0.00  \n",
              "8  0.00 0.00 0.00 0.00 0.00  \n",
              "9  0.00 0.00 0.00 0.00 0.00  \n",
              "10 0.00 0.00 0.00 0.00 0.00  \n",
              "11 0.00 0.00 0.00 0.00 0.00  \n",
              "12 0.00 0.00 0.00 0.00 0.00  \n",
              "13 0.00 0.00 0.00 0.00 0.00  \n",
              "14 0.00 0.00 0.00 0.00 0.00  \n",
              "15 0.11 0.00 0.00 0.00 0.00  \n",
              "16 0.73 0.00 0.00 0.00 0.00  \n",
              "17 0.98 0.25 0.00 0.00 0.00  \n",
              "18 0.81 0.01 0.00 0.00 0.00  \n",
              "19 0.71 0.00 0.00 0.00 0.00  \n",
              "20 0.00 0.00 0.00 0.00 0.00  \n",
              "21 0.00 0.00 0.00 0.00 0.00  \n",
              "22 0.00 0.00 0.00 0.00 0.00  \n",
              "23 0.00 0.00 0.00 0.00 0.00  \n",
              "24 0.00 0.00 0.00 0.00 0.00  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "qlouQTg3WN0W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "outputId": "3fa01b98-58c7-4be8-f782-b6fbc5dd96ad"
      },
      "cell_type": "code",
      "source": [
        "# Display de um único dígito DIFERENTE DE ZERO ACESSO = 1\n",
        "single = x_train[0]\n",
        "print(\"\\nShape de um Único Dígito: {}\".format(single.shape))\n",
        "print(\"\\nDígito: {}\".format(y_train[0]))\n",
        "\n",
        "#display(pd.DataFrame(single.reshape(28,28)))\n",
        "\n",
        "pd.options.display.float_format  = '{:,.0f}'.format\n",
        "df = pd.DataFrame(single.reshape(28,28)) \n",
        "\n",
        "count1 = 0\n",
        "df2 = df.copy()\n",
        "for lin in range(28):\n",
        "    for col in range(28):\n",
        "        if df2.loc[lin,col] != 0.0:\n",
        "            count1 += 1\n",
        "            df2.loc[lin,col] = 1     \n",
        "\n",
        "print('numero de 1s:',count1)\n",
        "print('numero de 0s:', (28*28) - count1)  \n",
        "df2.loc[5:24,4:23] # onde pixel > 0"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Shape de um Único Dígito: (28, 28, 1)\n",
            "\n",
            "Dígito: 5\n",
            "numero de 1s: 166\n",
            "numero de 0s: 618\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    4   5   6   7   8   9   10  11  12  13  14  15  16  17  18  19  20  21  \\\n",
              "5    0   0   0   0   0   0   0   0   1   1   1   1   1   1   1   1   1   1   \n",
              "6    0   0   0   0   1   1   1   1   1   1   1   1   1   1   1   1   1   1   \n",
              "7    0   0   0   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   \n",
              "8    0   0   0   1   1   1   1   1   1   1   1   1   1   1   0   0   0   0   \n",
              "9    0   0   0   0   1   1   1   1   1   1   1   0   1   1   0   0   0   0   \n",
              "10   0   0   0   0   0   1   1   1   1   1   0   0   0   0   0   0   0   0   \n",
              "11   0   0   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   \n",
              "12   0   0   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0   \n",
              "13   0   0   0   0   0   0   0   0   1   1   1   1   1   1   0   0   0   0   \n",
              "14   0   0   0   0   0   0   0   0   0   1   1   1   1   1   1   0   0   0   \n",
              "15   0   0   0   0   0   0   0   0   0   0   1   1   1   1   1   1   0   0   \n",
              "16   0   0   0   0   0   0   0   0   0   0   0   1   1   1   1   1   0   0   \n",
              "17   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   1   1   0   \n",
              "18   0   0   0   0   0   0   0   0   0   0   1   1   1   1   1   1   1   0   \n",
              "19   0   0   0   0   0   0   0   0   1   1   1   1   1   1   1   1   0   0   \n",
              "20   0   0   0   0   0   0   1   1   1   1   1   1   1   1   1   0   0   0   \n",
              "21   0   0   0   0   1   1   1   1   1   1   1   1   1   1   0   0   0   0   \n",
              "22   0   0   1   1   1   1   1   1   1   1   1   1   0   0   0   0   0   0   \n",
              "23   1   1   1   1   1   1   1   1   1   1   0   0   0   0   0   0   0   0   \n",
              "24   1   1   1   1   1   1   1   1   0   0   0   0   0   0   0   0   0   0   \n",
              "\n",
              "    22  23  \n",
              "5    1   1  \n",
              "6    1   1  \n",
              "7    1   0  \n",
              "8    0   0  \n",
              "9    0   0  \n",
              "10   0   0  \n",
              "11   0   0  \n",
              "12   0   0  \n",
              "13   0   0  \n",
              "14   0   0  \n",
              "15   0   0  \n",
              "16   0   0  \n",
              "17   0   0  \n",
              "18   0   0  \n",
              "19   0   0  \n",
              "20   0   0  \n",
              "21   0   0  \n",
              "22   0   0  \n",
              "23   0   0  \n",
              "24   0   0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "i2sYiqvlWN0g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Definindo a CNN"
      ]
    },
    {
      "metadata": {
        "id": "k57W7PGtWN0j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "de6098c2-727b-4679-be5c-a56eef22c5e6"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "# Definindo os hiperparâmetros\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# Converte vetores de classe para matrizes de classe binária\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Modelo CNN\n",
        "# https://keras.io/layers/convolutional/#conv2d\n",
        "model = Sequential()                      # 2x2  padding = abrangue todo filtro a entrada  # Input Shape: (28, 28, 1)\n",
        "input_shape = (28, 28, 1)\n",
        "model.add(Conv2D(filters = 16, kernel_size = 2, padding = 'same', activation = 'relu', input_shape = input_shape))\n",
        "model.add(MaxPooling2D(pool_size = 2)) # 2x2 - reduz a dimensionalidade e reduz o overfitting\n",
        "model.add(Conv2D(filters = 32, kernel_size = 2, padding = 'same', activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = 2))\n",
        "model.add(Conv2D(filters = 64, kernel_size = 2, padding = 'same', activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = 2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500, activation = 'relu'))\n",
        "model.add(Dense(10, activation = 'softmax'))\n",
        "\n",
        "# Compilação do modelo\n",
        "model.compile(loss = keras.losses.categorical_crossentropy, \n",
        "              optimizer = keras.optimizers.Adadelta(), \n",
        "              metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 16)        80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 32)        2080      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 64)          8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               288500    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5010      \n",
            "=================================================================\n",
            "Total params: 303,926\n",
            "Trainable params: 303,926\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nlpLKR7DWN0q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Nota: observe que aumentamos a quantidade de filtros. Isso se deve ao fato que queremos aumentar a profundidade do nosso array, ao mesmo tempo que reduzimos altura e largura com a camada Max Pooling. O processo de convolução tem exatamente o objetivo de tornar o array mais profundo, enquanto a camada Max Pooling reduz o espaço dimensional. Basicamente fazemos isso: convertemos um quadrado representando uma imagem em vários pequenos quadrados representando cada um características das imagens. No final, entregamos esse array a uma camada totalmente conectada e fazemos a classificação.\n",
        "![CNN](images/cnn_architecture.png \"CNN\")"
      ]
    },
    {
      "metadata": {
        "id": "_InSjrBjWN0r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Treinamento da CNN"
      ]
    },
    {
      "metadata": {
        "id": "B87dfXo5WN0s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15c9c778-1167-4b70-e88c-79f137f7010a"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "RdNzPCmmWN0w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0081de6a-16c7-4091-d7f4-192120d28822"
      },
      "cell_type": "code",
      "source": [
        "import keras as kr\n",
        "kr.__version__"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "SXAU8ttuWN0y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "b8c00844-bae7-45d5-914c-1ce6ab4377f0"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from datetime import datetime\n",
        "t1 = datetime.now()\n",
        "\n",
        "# Fit do modelo\n",
        "model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, verbose = 2, validation_data = (x_test, y_test))\n",
        "print(\"tempo decorrido:\", datetime.now() - t1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            " - 8s - loss: 0.2922 - acc: 0.9074 - val_loss: 0.0554 - val_acc: 0.9825\n",
            "Epoch 2/12\n",
            " - 4s - loss: 0.0603 - acc: 0.9809 - val_loss: 0.0448 - val_acc: 0.9869\n",
            "Epoch 3/12\n",
            " - 4s - loss: 0.0412 - acc: 0.9870 - val_loss: 0.0312 - val_acc: 0.9897\n",
            "Epoch 4/12\n",
            " - 4s - loss: 0.0324 - acc: 0.9896 - val_loss: 0.0537 - val_acc: 0.9827\n",
            "Epoch 5/12\n",
            " - 4s - loss: 0.0255 - acc: 0.9918 - val_loss: 0.0306 - val_acc: 0.9906\n",
            "Epoch 6/12\n",
            " - 4s - loss: 0.0217 - acc: 0.9928 - val_loss: 0.0381 - val_acc: 0.9884\n",
            "Epoch 7/12\n",
            " - 4s - loss: 0.0169 - acc: 0.9948 - val_loss: 0.0369 - val_acc: 0.9885\n",
            "Epoch 8/12\n",
            " - 4s - loss: 0.0140 - acc: 0.9955 - val_loss: 0.0419 - val_acc: 0.9874\n",
            "Epoch 9/12\n",
            " - 4s - loss: 0.0115 - acc: 0.9964 - val_loss: 0.0328 - val_acc: 0.9893\n",
            "Epoch 10/12\n",
            " - 4s - loss: 0.0105 - acc: 0.9967 - val_loss: 0.0315 - val_acc: 0.9918\n",
            "Epoch 11/12\n",
            " - 4s - loss: 0.0082 - acc: 0.9974 - val_loss: 0.0312 - val_acc: 0.9918\n",
            "Epoch 12/12\n",
            " - 4s - loss: 0.0065 - acc: 0.9981 - val_loss: 0.0318 - val_acc: 0.9923\n",
            "tempo decorrido: 0:00:57.391607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zppSTV3vWN0_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07f352f7-1c40-4e8a-86a3-400ef47a8595"
      },
      "cell_type": "code",
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28, 1), (60000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "Xq_4IMcwWN1B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "((60000, 28, 28, 1), (60000, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1DHT6VzOWN1L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a69b884f-5c43-4195-b989-2f695c8c574d"
      },
      "cell_type": "code",
      "source": [
        "x_test.shape, y_test.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 28, 28, 1), (10000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "o21im1MYWN1W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "465c622c-2bdb-45f0-a674-27cb19e937d9"
      },
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "\n",
        "# Fórmula: número de parâmetros em uma camada convolucional\n",
        "# O número de parâmetros em uma camada convolucional depende dos valores fornecidos dos filtros, \n",
        "#                                                                                      kernel_size e input_shape. \n",
        "# Vamos definir algumas variáveis:\n",
        "\n",
        "# K - o número de filtros na camada convolucional\n",
        "# F - altura e largura dos filtros convolucionais\n",
        "# D_in - a profundidade da camada anterior\n",
        "\n",
        "# Observe que K = filtros e F = kernel_size. Da mesma forma, D_in é o último valor na tupla input_shape.\n",
        "\n",
        "# Uma vez que existem pesos F * F * D por filtro e a camada convolucional é composta por filtros K, \n",
        "# o número total de pesos na camada convolucional é:\n",
        "\n",
        "# K * F * F * D_in. \n",
        "\n",
        "# Uma vez que existe um termo de bias por filtro, a camada convolucional tem bias igual ao valor de K. \n",
        "# Assim, o número de parâmetros na camada convolucional é dado por K * F * F * D_in + K."
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 16)        80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 32)        2080      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 64)          8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               288500    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5010      \n",
            "=================================================================\n",
            "Total params: 303,926\n",
            "Trainable params: 303,926\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZV3NOghyWN1a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "417a92e0-d0df-4c81-a2bf-e0c66139a6c2"
      },
      "cell_type": "code",
      "source": [
        "for layer in model.layers:\n",
        "    print(layer.name, layer.output_shape ) # layer.inbound_nodes layser.outbound_nodes"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv2d_1 (None, 28, 28, 16)\n",
            "max_pooling2d_1 (None, 14, 14, 16)\n",
            "conv2d_2 (None, 14, 14, 32)\n",
            "max_pooling2d_2 (None, 7, 7, 32)\n",
            "conv2d_3 (None, 7, 7, 64)\n",
            "max_pooling2d_3 (None, 3, 3, 64)\n",
            "flatten_1 (None, 576)\n",
            "dense_1 (None, 500)\n",
            "dense_2 (None, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Av3vU_0jWN1g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Avaliando a Acurácia\n",
        "\n",
        "Note, se você estiver usando uma GPU, você pode obter a mensagem de erro ** ResourceExhaustedError **. Isso ocorre porque a GPU pode não ter RAM suficiente para prever todo o conjunto de dados ao mesmo tempo."
      ]
    },
    {
      "metadata": {
        "id": "c9DA-AabWN1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9c8840ee-a38a-4e75-e9b8-e1a601fd0845"
      },
      "cell_type": "code",
      "source": [
        "# Avaliação nos dados de teste\n",
        "# Neste caso, estamos avaliando o modelo no dataset de teste inteiro. \n",
        "# Isso pode não caber na memória da GPU\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# Print\n",
        "print('Perda em Teste: {}'.format(score[0]))\n",
        "print('Acurácia em Teste: {}'.format(score[1]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perda em Teste: 0.03181745680046615\n",
            "Acurácia em Teste: 0.9923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "er8_adp7WN1k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As GPUs são mais utilizadas durante o treinamento do que nas previsões.\n",
        "\n",
        "Para as previsões, desative a GPU ou apenas preveja em uma amostra menor. \n",
        "\n",
        "Se a sua GPU tiver memória suficiente, o código de previsão acima pode funcionar bem. Caso contrário, basta uma previsão em uma amostra com o seguinte código:"
      ]
    },
    {
      "metadata": {
        "id": "psVAqUklWN1o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4993762d-4a3f-4bb1-b9ad-a697df785ee0"
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Para avaliação do modelo usando GPU, usamos amostras dos dados de teste\n",
        "small_x = x_test[1:100]\n",
        "small_y = y_test[1:100]\n",
        "small_y2 = np.argmax(small_y, axis = 1)\n",
        "pred = model.predict(small_x)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "score = metrics.accuracy_score(small_y2, pred)\n",
        "print('Acurácia: {}'.format(score))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acurácia: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P8oue5AqWN1t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "1a2007e7-2461-4e62-c054-54a440674ff9"
      },
      "cell_type": "code",
      "source": [
        "print(small_y[:8])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}