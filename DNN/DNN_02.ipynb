{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNN-02.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladimiralencar/DeepLearning-LANA/blob/master/DNN/DNN_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "aqeFi4kbdkot",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Treinamento de uma Deep Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "JG4hPIPML98r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  Carregando dados"
      ]
    },
    {
      "metadata": {
        "id": "Tu1ujOjFL9op",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 902
        },
        "outputId": "2a8d11e7-e458-41ff-99e9-5868f00a90d8"
      },
      "cell_type": "code",
      "source": [
        "#!mkdir images\n",
        "#!mkdir data\n",
        "#!cp *.csv data\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!cp *.jpg images\n",
        "!cp *.png images\n",
        "!cp *.gif"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8339c301-6a94-45e6-967c-39f0e761eceb\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-8339c301-6a94-45e6-967c-39f0e761eceb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving saddle_point_evaluation_optimizers.gif to saddle_point_evaluation_optimizers.gif\n",
            "Saving z-score.png to z-score.png\n",
            "Saving one-hot.png to one-hot.png\n",
            "Saving tanh.png to tanh.png\n",
            "Saving higher-dimensions-classification.png to higher-dimensions-classification.png\n",
            "Saving binary-classification.png to binary-classification.png\n",
            "Saving abstract_nn.png to abstract_nn.png\n",
            "Saving ann_class_reg.png to ann_class_reg.png\n",
            "Saving ann.png to ann.png\n",
            "Saving bias_value.png to bias_value.png\n",
            "Saving bias_weight.png to bias_weight.png\n",
            "Saving contours_evaluation_optimizers.gif to contours_evaluation_optimizers.gif\n",
            "Saving deriv_sigmoid.png to deriv_sigmoid.png\n",
            "Saving deriv.png to deriv.png\n",
            "Saving errors.png to errors.png\n",
            "Saving kfold.png to kfold.png\n",
            "Saving momentum.png to momentum.png\n",
            "Saving relu.png to relu.png\n",
            "Saving roc.png to roc.png\n",
            "Saving sgd_error.png to sgd_error.png\n",
            "Saving sigmoid.png to sigmoid.png\n",
            "Saving spec_cut.png to spec_cut.png\n",
            "Saving t1vst2.png to t1vst2.png\n",
            "Saving train_val.png to train_val.png\n",
            "Saving training_val.png to training_val.png\n",
            "cp: cannot stat '*.jpg': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ANln6kWPM9EC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "32750edc-b304-4b40-da7a-6e7660d59049"
      },
      "cell_type": "code",
      "source": [
        "!ls images"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "abstract_nn.png\t\t\t      one-hot.png\n",
            "ann_class_reg.png\t\t      relu.png\n",
            "ann.png\t\t\t\t      roc.png\n",
            "bias_value.png\t\t\t      sgd_error.png\n",
            "bias_weight.png\t\t\t      sigmoid.png\n",
            "binary-classification.png\t      spec_cut.png\n",
            "deriv.png\t\t\t      t1vst2.png\n",
            "deriv_sigmoid.png\t\t      tanh.png\n",
            "errors.png\t\t\t      training_val.png\n",
            "higher-dimensions-classification.png  train_val.png\n",
            "kfold.png\t\t\t      z-score.png\n",
            "momentum.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "43TXDuj5dkou",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Apesar das vantagens computacionais e de ser um modelo mais aproximado do cérebro, o treinamento de redes neurais profundas era uma tarefa de difícil execução até uma década atrás, devido a dois complicadores principais. O primeiro deles é conhecido como a dissipação dos gradientes (vanishing gradients), que é causado por unidades que estão saturadas. Durante a aplicação do algoritmo de retropropagação, o gradiente da ativação de cada unidade oculta é calculado e utilizado como um termo multiplicativo para atualização dos pesos correspondentes. Entretanto se a unidade em questão está próxima da saturação, então a derivada parcial resultante será um valor próximo de zero. Isso faz com que o gradiente com relação às pré-ativações seja também próximo de zero, porque seu cálculo envolve a multiplicação pela derivada parcial. Isso significa que o gradiente retropropagado para as camadas anteriores será cada vez mais próximo de zero, de tal forma que quanto mais próximo à camada de entrada, maior a quantidade de gradientes próximos de zero. Como resultado, à medida que o gradiente do erro é retropropagado seu valor decresce exponencialmente, de tal forma que o aprendizado nas camadas mais próximas à entrada se torna muito lento. Esse problema afeta não apenas o aprendizado em redes profundas alimentadas adiante (Feed Forward), mas também em redes recorrentes (RNNs). O segundo complicador é consequência do número maior de parâmetros para adequar (os parâmetros de uma rede neural são seus pesos, e quanto mais camadas, mais pesos). De fato, quanto maior a quantidade de parâmetros a ajustar em um modelo, maior o risco de overfitting."
      ]
    },
    {
      "metadata": {
        "id": "Qz_za_KDdkov",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "O aprendizado de modelos por meio do treinamento de redes neurais profundas tem sido tradicionalmente tratado como um processo empírico. De fato, soluções envolvem o uso de diversos procedimentos que foram surgindo ao longo dos anos e que em conjunto passaram a contar como fatores importantes no sucesso da aprendizagem profunda (além do aumento do poder computacional, da disponibilidade de grandes conjuntos de dados e de melhores arquiteturas de rede). Vamos agora, descrever o procedimentos cujo propósito é treinar redes neurais profundas de forma adequada, seja diminuindo o tempo de treinamento, seja diminuindo o erro de generalização dos modelos resultantes."
      ]
    },
    {
      "metadata": {
        "id": "W4VBlMUUdkow",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Construindo um Vetor de Features (atributos)\n",
        "\n",
        "As redes neurais exigem que suas entradas sejam um número fixo de colunas. Isso é muito semelhante aos dados de uma planilha. Esta entrada deve ser completamente numérica.\n",
        "\n",
        "É importante representar os dados de forma a que a rede neural possa se formar a partir deles. Vejamos quais são algumas das formas mais básicas de transformar dados para uma rede neural, começando pelos possíveis tipos de dados:\n",
        "\n",
        "* Dados do Tipo Caracter (strings)\n",
        "    * **Nominal** - Itens discretos individuais, sem ordem. Por exemplo: cor, código postal, forma.\n",
        "    * **Ordinal** - Itens discretos individuais que podem ser ordenados. Por exemplo: nível de classificação, título do trabalho, tamanho de uma roupa (pequeno, médio, grande)\n",
        "    \n",
        "    \n",
        "* Dados Numéricos\n",
        "    * **Interval** - Valores numéricos, início não definido. Por exemplo, a temperatura. Você nunca diria \"ontem foi duas vezes mais quente do que hoje\".\n",
        "    * **Ratio** - Valores numéricos, início claramente definido. Por exemplo, a velocidade. Você diria que \"O primeiro carro está indo duas vezes mais rápido do que o segundo\".\n",
        "\n",
        "Abaixo você vai encontrar várias funções úteis para codificar o vetor de recursos para vários tipos de dados. \n",
        "\n",
        "* **encode_text_dummy** - Encoding de campos de texto, como as espécies de íris como um único campo para cada classe. Três classes se tornariam \"0,0,1\" \"0,1,0\" e \"1,0,0\". Podemos fazer o encoding dos preditores não-alvo dessa maneira. Bom para dados nominais.\n",
        "* **encode_text_index** - Encoding dos campos de texto, como um único campo numérico como \"0\" \"1\" e \"2\". Bom para dados nominais.\n",
        "* **encode_numeric_zscore** - Encoding dos valores numéricos com um escore z. As redes neurais lidam bem com os campos \"centrados\", o zscore geralmente é um bom ponto de partida para o intervalo / proporção.\n",
        "\n",
        "* Os valores ordinais podem ser codificados com as funções dummy ou index. Mais tarde, veremos um meio mais avançado de encoding *\n",
        "\n",
        "Tratamento de dados missing:\n",
        "\n",
        "* **missing_median** - Preenche todos os valores faltantes com o valor médio.\n",
        "\n",
        "Criando o vetor final de recursos:\n",
        "\n",
        "* **to_xy** - Uma vez que todos os campos são numéricos, esta função pode fornecer as matrizes x e y que são usadas para caber na rede neural.\n",
        "\n",
        "Outras funções de utilidade:\n",
        "\n",
        "* **hms_string** - Imprima uma sequência de tempo decorrido.\n",
        "* **chart_regression** - Exibe um gráfico para mostrar o quão bem uma regressão executa."
      ]
    },
    {
      "metadata": {
        "id": "t-V-j1Vfdkoy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Normalização"
      ]
    },
    {
      "metadata": {
        "id": "H1XXZ4Nydkoz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Normalmente, os dados são dimensionados para um intervalo específico em um processo chamado normalização. "
      ]
    },
    {
      "metadata": {
        "id": "i_NELkGEdko1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Uma transformação usual durante o treinamento de uma RNA é normalizar o conjunto de dados de treinamento de acordo com a distribuição normal padrão (i.e., média igual a zero e variância igual a 1) para evitar problemas de comparação devido às diferentes escalas usadas nos dados. Ocorre que durante a passagem dos exemplos normalizados através das\n",
        "camadas da rede, esses valores são novamente transformados (por meio das pré-ativações e ativações), podendo fazer com que os dados de entrada de algumas camadas ocultas fiquem desnormalizados novamente. Esse problema, conhecido como mudança de co-variável interna (internal covariate shift), é tanto mais grave quanto mais profunda for a rede a ser treinada. Esse problema aumenta o tempo de treinamento porque implica na definição de uma taxa de aprendizagem pequena, além de propiciar a dissipação dos gradientes.\n",
        "\n",
        "A Normalização em Lote (Batch Normalization) é um mecanismo proposto recentemente para resolver o problema acima, e que consiste em normalizar os dados fornecidos a cada camada oculta. A normalização é aplicada em cada mini-lote (como já fizemos em exemplos anteriores), para aumentar a eficiência durante a aplicação da transformação. De acordo com os experimentos realizados pelos autores da técnica (link na seção de links úteis), ela também produz um efeito de regularização sobre o treinamento, em alguns casos eliminando a necessidade de aplicar o desligamento (Dropout). Uma aceleração significativa do tempo de treinamento também foi observada, resultante da diminuição de 14 vezes na quantidade de passos de treinamento necessários, quando comparada ao tempo de treinamento sem o uso da normalização."
      ]
    },
    {
      "metadata": {
        "id": "ln6h8Qbcdko2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Existem muitas maneiras diferentes de normalizar os dados. Vejamos algumas delas:"
      ]
    },
    {
      "metadata": {
        "id": "Auuxbpw9dko3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### One Hot Encoding"
      ]
    },
    {
      "metadata": {
        "id": "GaR58N38dko4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Se você tem um valor categórico, como a espécie de uma íris, a marca de um automóvel ou a etiqueta de dígitos no conjunto de dados MNIST, você deve usar uma codificação One-Hot Encoding. Para codificar dessa maneira, você usaria um neurônio de saída para cada classe no problema. Lembra do conjunto de dados MNSIT que já usamos em outras oportunidades, onde você tem imagens para dígitos entre 0 e 9? Este problema é mais comumente codificado como dez neurônios de saída com uma função de ativação softmax que dá a probabilidade de a entrada ser um desses dígitos. Usando a codificação One-Hot, os dez dígitos podem ser codificados da seguinte maneira:"
      ]
    },
    {
      "metadata": {
        "id": "sWweOHHadko4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![One-Hot Encoding](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/DNN/images/one-hot.png?raw=true \"One-Hot Encoding\")"
      ]
    },
    {
      "metadata": {
        "id": "GJv24mUmdko5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One-Hot Encoding sempre deve ser usado quando as classes não têm ordenação. Outro exemplo desse tipo de codificação é a marca de um automóvel. Normalmente, a lista de fabricantes de automóveis não está ordenada, a menos que exista algum significado que você deseja transmitir. Por exemplo, você pode encomendar as montadoras pelo número de anos de negócios. No entanto, esta classificação só deve ser feita se o número de anos de negócios tiver significado no seu problema. Se realmente não há ordem, então One-Hot deve sempre ser usado. Como você pode facilmente converter os dígitos, você pode se perguntar por que usamos uma codificação One-Hot. No entanto, a ordem dos dígitos não significa que o programa possa reconhecê-los. O fato de que \"1\" e \"2\" estarem numericamente próximos um do outro não ajuda o programa a reconhecer a imagem. Portanto, não devemos usar um neurônio de saída único que simplesmente exiba o dígito reconhecido. Os dígitos 0-9 são categorias, e não valores numéricos reais. \n",
        "\n",
        "As categorias de codificação com um único valor numérico prejudicam o processo de decisões da rede neural. Tanto a entrada como a saída podem usar One-Hot Encoding. A lista acima usou 0 's e 1' s. Normalmente você usará a unidade linear retificada (ReLU) e a ativação do softmax. No entanto, se você estiver trabalhando com uma função de ativação hiperbólica tangente, você deve utilizar um valor de -1 para os 0 para coincidir com o intervalo da hiperbólica tangente de -1 a 1.\n",
        "\n",
        "Se você tem um número extremamente grande de classes, One-Hot Encoding pode tornar-se complicado porque você deve ter um neurônio para cada classe. Nesses casos, você tem várias opções. Primeiro, você pode encontrar uma maneira de pedir suas categorias. Com esta ordem, suas categorias agora podem ser codificadas como um valor numérico, que seria a posição da categoria atual dentro da lista ordenada. Outra abordagem para lidar com um número extremamente grande de categorias é a codificação de frequência de documento inversa (TF-IDF) porque cada classe se torna essencialmente a probabilidade de ocorrência dessa classe em relação às demais. Desta forma, o TF-IDF permite que o programa mapeie um grande número de classes para um único neurônio. "
      ]
    },
    {
      "metadata": {
        "id": "K-wDUtqjdko7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Range"
      ]
    },
    {
      "metadata": {
        "id": "tAnq7dfxdko9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Se você tem um número real ou uma lista ordenada de categorias, você pode escolher a normalização de range, pois simplesmente mapeia o intervalo de dados de entrada no alcance de sua função de ativação. Sigmoid, ReLU e softmax usam um intervalo entre 0 e 1, enquanto a tangente hiperbólica usa um intervalo entre -1 e 1."
      ]
    },
    {
      "metadata": {
        "id": "cUybeQeEdko_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Para executar a normalização, é necessário que os valores altos e baixos dos dados sejam normalizados. Da mesma forma, você precisa de valores altos e baixos para normalizar (geralmente 0 e 1). Às vezes você precisará desfazer a normalização realizada em um número e retorná-lo para um estado desnormalizado."
      ]
    },
    {
      "metadata": {
        "id": "qzeEkaY3dkpA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Uma maneira muito simples de pensar sobre a normalização de range é porcentagem. Considere a seguinte analogia. Você vê um anúncio afirmando que você receberá uma redução de 10 reais em um produto, e você deve decidir se esse negócio vale a pena. Se você está comprando uma camiseta, esta oferta provavelmente é um bom negócio. No entanto, se você está comprando um carro, 10 reais realmente não importam. A situação muda se você descobrir que o comerciante ofereceu um desconto de 10%. Assim, o valor agora é mais significativo. Não importa se você está comprando uma camiseta, um carro ou mesmo uma casa, o desconto de 10% tem ramificações claras sobre o problema porque transcende as moedas. Em outras palavras, a porcentagem é um tipo de normalização. Assim como na analogia, a normalização para um intervalo ajuda a rede neural a avaliar todas as entradas com igual significado."
      ]
    },
    {
      "metadata": {
        "id": "W4J_bbEedkpA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Escore Z"
      ]
    },
    {
      "metadata": {
        "id": "n8thloIUdkpC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A normalização de escore Z é a normalização mais comum para um número real ou uma lista ordenada. Para quase todas as aplicações, a normalização do z-score deve ser usada no lugar da normalização de range. Esse tipo de normalização baseia-se no conceito estatístico de escores z. Os escores Z fornecem ainda mais informações do que percentagens. \n",
        "\n",
        "Considere o seguinte exemplo: \n",
        "\n",
        "O estudante A obteve 85% dos pontos em seu exame. O estudante B obteve 75% dos pontos em seu exame. Qual aluno teve melhor performance? Se o professor está apenas relatando a porcentagem de pontos corretos, o aluno A ganhou uma melhor pontuação. No entanto, você pode mudar sua resposta se você soubesse que a pontuação média (média) para o exame do aluno A era de 95%. Da mesma forma, você pode reconsiderar sua posição se descobrisse que a classe do aluno B tinha uma pontuação média de 65%. O estudante B ficou acima da média no exame. Embora o aluno A tenha obtido uma pontuação melhor, ela ficou abaixo da média. Para reportar verdadeiramente uma pontuação curva (um escore z), você deve ter o escore médio e o desvio padrão. "
      ]
    },
    {
      "metadata": {
        "id": "xE-w1uVjdkpD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Calculamos o z-score, subtraindo a média do valor de x e dividindo pelo desvio padrão. O z-score é um valor numérico onde 0 representa uma pontuação que é exatamente a média. Um escore z positivo está acima da média. Um escore z negativo está abaixo da média. \n",
        "\n",
        "![Z-Score](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/DNN/images/z-score.png?raw=true  \"Z-Score\")\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RPz6BTFSdkpF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Funções Auxiliares\n",
        "\n",
        "É uma boa prática criar suas próprias funções de limpeza e transformação dos dados que serão processados pelo modelo de rede neural. Use esses exemplos como referência."
      ]
    },
    {
      "metadata": {
        "id": "3n6riN5MdkpG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Encoding dos valores de texto para variáveis nominais\n",
        "def encode_text_dummy(df, name):\n",
        "    dummies = pd.get_dummies(df[name])\n",
        "    for x in dummies.columns:\n",
        "        dummy_name = \"{}-{}\".format(name, x)\n",
        "        df[dummy_name] = dummies[x]\n",
        "    df.drop(name, axis=1, inplace=True)\n",
        "\n",
        "\n",
        "# Encoding dos valores de texto para uma única variável dummy. As novas colunas (que não substituem o antigo) terão 1\n",
        "# em todos os locais onde a coluna original (nome) corresponde a cada um dos valores-alvo. Uma coluna é adicionada para\n",
        "# cada valor alvo.\n",
        "def encode_text_single_dummy(df, name, target_values):\n",
        "    for tv in target_values:\n",
        "        l = list(df[name].astype(str))\n",
        "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
        "        name2 = \"{}-{}\".format(name, tv)\n",
        "        df[name2] = l\n",
        "\n",
        "\n",
        "# Encoding dos valores de texto para índices (ou seja, [1], [2], [3] para vermelho, verde, azul por exemplo).\n",
        "def encode_text_index(df, name):\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    df[name] = le.fit_transform(df[name])\n",
        "    return le.classes_\n",
        "\n",
        "\n",
        "# Normalização Z-score\n",
        "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
        "    if mean is None:\n",
        "        mean = df[name].mean()\n",
        "\n",
        "    if sd is None:\n",
        "        sd = df[name].std()\n",
        "\n",
        "    df[name] = (df[name] - mean) / sd\n",
        "\n",
        "\n",
        "# Converte todos os valores faltantes na coluna especificada para a mediana\n",
        "def missing_median(df, name):\n",
        "    med = df[name].median()\n",
        "    df[name] = df[name].fillna(med)\n",
        "\n",
        "\n",
        "# Converte todos os valores faltantes na coluna especificada para o padrão\n",
        "def missing_default(df, name, default_value):\n",
        "    df[name] = df[name].fillna(default_value)\n",
        "\n",
        "\n",
        "# Converte um dataframe Pandas para as entradas x, y que o TensorFlow precisa\n",
        "def to_xy(df, target):\n",
        "    result = []\n",
        "    for x in df.columns:\n",
        "        if x != target:\n",
        "            result.append(x)\n",
        "    # Descobre o tipo da coluna de destino. \n",
        "    target_type = df[target].dtypes\n",
        "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
        "    # Encoding para int. TensorFlow gosta de 32 bits.\n",
        "    if target_type in (np.int64, np.int32):\n",
        "        # Classificação\n",
        "        dummies = pd.get_dummies(df[target])\n",
        "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
        "    else:\n",
        "        # Regressão\n",
        "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
        "\n",
        "# String de tempo bem formatado\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
        "\n",
        "\n",
        "# Chart de Regressão\n",
        "def chart_regression(pred,y,sort=True):\n",
        "    t = pd.DataFrame({'pred' : pred, 'y' : y })  # y.flatten()\n",
        "    if sort:\n",
        "        t.sort_values(by=['y'],inplace=True)\n",
        "    a = plt.plot(t['y'].tolist(),label='expected')\n",
        "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
        "    plt.ylabel('output')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Remove todas as linhas onde a coluna especificada em +/- desvios padrão\n",
        "def remove_outliers(df, name, sd):\n",
        "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
        "    df.drop(drop_rows, axis=0, inplace=True)\n",
        "\n",
        "\n",
        "# Normalização Range\n",
        "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1, data_low=None, data_high=None):\n",
        "    if data_low is None:\n",
        "        data_low = min(df[name])\n",
        "        data_high = max(df[name])\n",
        "\n",
        "    df[name] = ((df[name] - data_low) / (data_high - data_low)) * (normalized_high - normalized_low) + normalized_low"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "SuB26HMvdkpM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Treinamento com um Conjunto de Validação e Early Stopping\n",
        "\n",
        "** O Overfitting ** ocorre quando uma rede neural é treinada no ponto em que começa a memorizar em vez de generalizar.\n",
        "\n",
        "![Treinamento x Erro de Validação](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/DNN/images/training_val.png?raw=true  \"Treinamento x Erro de Validação\")\n",
        "\n",
        "É importante segmentar o conjunto de dados original em vários conjuntos de dados:\n",
        "\n",
        "* **Treinamento**\n",
        "* **Validação**\n",
        "* **Teste**\n",
        "\n",
        "Existem várias maneiras diferentes de que esses conjuntos possam ser construídos. Os seguintes programas demonstram alguns deles.\n",
        "\n",
        "O primeiro método é um conjunto de treinamento e validação. Os dados de treinamento são usados para treinar a rede neural até que o conjunto de validação não melhore mais. Isso tenta parar em um ponto de treinamento quase ótimo. Este método apenas dará previsões precisas de \"fora de amostra\" para o conjunto de validação, isto geralmente é 20% ou mais dos dados. As previsões para os dados de treinamento serão excessivamente otimistas, pois esses foram os dados em que a rede neural foi treinada.\n",
        "\n",
        "![Treinamento com um conjundo de validação](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/DNN/images/train_val.png?raw=true   \"Treinamento com um conjundo de validação\")\n"
      ]
    },
    {
      "metadata": {
        "id": "-mcrybMkdkpO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Early Stopping no Keras: https://keras.io/callbacks/"
      ]
    },
    {
      "metadata": {
        "id": "M9FdsOz2K-Dj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "0187493c-dce8-4069-aa1e-1bb76f53313c"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Path\n",
        "path = \"./data/\"\n",
        "    \n",
        "# Arquivo\n",
        "filename = os.path.join(path,\"iris.csv\")   \n",
        "\n",
        "# Leitura do arquivo em um dataframe\n",
        "df = pd.read_csv(filename,na_values=['NA','?'])\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_l</th>\n",
              "      <th>sepal_w</th>\n",
              "      <th>petal_l</th>\n",
              "      <th>petal_w</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal_l  sepal_w  petal_l  petal_w      species\n",
              "0      5.1      3.5      1.4      0.2  Iris-setosa\n",
              "1      4.9      3.0      1.4      0.2  Iris-setosa\n",
              "2      4.7      3.2      1.3      0.2  Iris-setosa\n",
              "3      4.6      3.1      1.5      0.2  Iris-setosa\n",
              "4      5.0      3.6      1.4      0.2  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "d_Js_i-GdkpR",
        "colab_type": "code",
        "outputId": "d49577ac-a906-4d04-e58e-c07321dac66f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "# Encoding dos valores de texto para índices\n",
        "species = encode_text_index(df, \"species\")\n",
        "\n",
        "# Conversão do input\n",
        "x,y = to_xy(df,\"species\")\n",
        "\n",
        "# Divisão em dados de treino e de teste\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 42)\n",
        "\n",
        "# Construindo a Rede Neural\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_dim = x.shape[1], kernel_initializer = 'normal', activation = 'relu'))\n",
        "model.add(Dense(1, kernel_initializer = 'normal'))\n",
        "model.add(Dense(y.shape[1], activation = 'softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
        "\n",
        "# Early stoppping                             min_delta = Melhora na perda, patience = num epochs para esperar\n",
        "monitor = EarlyStopping(monitor = 'val_loss', min_delta = 1e-3, patience = 5, verbose = 1, mode = 'auto')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 10)                50        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 11        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 6         \n",
            "=================================================================\n",
            "Total params: 67\n",
            "Trainable params: 67\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E4zsP3QNKyo2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 18326
        },
        "outputId": "462011ee-36c2-4e9f-a3c3-707dcfdf5e53"
      },
      "cell_type": "code",
      "source": [
        "# Fit do modelo\n",
        "model.fit(x, y, validation_data = (x_test, y_test), callbacks = [monitor], verbose = 2, epochs = 1000)\n",
        "\n",
        "# epochs = 600, loss: 0.1043 - val_loss (perda durante a validadção): 0.0841\n",
        "# epochs = 564,loss: 0.1168 - val_loss: 0.0924"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 150 samples, validate on 38 samples\n",
            "Epoch 1/1000\n",
            " - 1s - loss: 1.0957 - val_loss: 1.0900\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 1.0914 - val_loss: 1.0865\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 1.0877 - val_loss: 1.0838\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 1.0841 - val_loss: 1.0810\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 1.0806 - val_loss: 1.0778\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 1.0768 - val_loss: 1.0746\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 1.0729 - val_loss: 1.0710\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 1.0691 - val_loss: 1.0671\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 1.0639 - val_loss: 1.0621\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 1.0588 - val_loss: 1.0565\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 1.0531 - val_loss: 1.0501\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 1.0468 - val_loss: 1.0432\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 1.0398 - val_loss: 1.0357\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 1.0324 - val_loss: 1.0273\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 1.0240 - val_loss: 1.0172\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 1.0153 - val_loss: 1.0067\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 1.0063 - val_loss: 0.9954\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 0.9963 - val_loss: 0.9849\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 0.9863 - val_loss: 0.9739\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 0.9760 - val_loss: 0.9612\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 0.9645 - val_loss: 0.9484\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 0.9537 - val_loss: 0.9352\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 0.9417 - val_loss: 0.9222\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 0.9295 - val_loss: 0.9097\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 0.9187 - val_loss: 0.8975\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 0.9062 - val_loss: 0.8838\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 0.8937 - val_loss: 0.8700\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 0.8817 - val_loss: 0.8566\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 0.8703 - val_loss: 0.8431\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 0.8578 - val_loss: 0.8302\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 0.8465 - val_loss: 0.8167\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 0.8343 - val_loss: 0.8043\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 0.8238 - val_loss: 0.7922\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 0.8120 - val_loss: 0.7788\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 0.8011 - val_loss: 0.7666\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 0.7905 - val_loss: 0.7552\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 0.7801 - val_loss: 0.7443\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 0.7700 - val_loss: 0.7334\n",
            "Epoch 39/1000\n",
            " - 0s - loss: 0.7608 - val_loss: 0.7234\n",
            "Epoch 40/1000\n",
            " - 0s - loss: 0.7508 - val_loss: 0.7128\n",
            "Epoch 41/1000\n",
            " - 0s - loss: 0.7419 - val_loss: 0.7028\n",
            "Epoch 42/1000\n",
            " - 0s - loss: 0.7333 - val_loss: 0.6930\n",
            "Epoch 43/1000\n",
            " - 0s - loss: 0.7251 - val_loss: 0.6844\n",
            "Epoch 44/1000\n",
            " - 0s - loss: 0.7166 - val_loss: 0.6753\n",
            "Epoch 45/1000\n",
            " - 0s - loss: 0.7087 - val_loss: 0.6671\n",
            "Epoch 46/1000\n",
            " - 0s - loss: 0.7007 - val_loss: 0.6593\n",
            "Epoch 47/1000\n",
            " - 0s - loss: 0.6934 - val_loss: 0.6513\n",
            "Epoch 48/1000\n",
            " - 0s - loss: 0.6860 - val_loss: 0.6441\n",
            "Epoch 49/1000\n",
            " - 0s - loss: 0.6795 - val_loss: 0.6372\n",
            "Epoch 50/1000\n",
            " - 0s - loss: 0.6727 - val_loss: 0.6298\n",
            "Epoch 51/1000\n",
            " - 0s - loss: 0.6660 - val_loss: 0.6228\n",
            "Epoch 52/1000\n",
            " - 0s - loss: 0.6594 - val_loss: 0.6164\n",
            "Epoch 53/1000\n",
            " - 0s - loss: 0.6534 - val_loss: 0.6096\n",
            "Epoch 54/1000\n",
            " - 0s - loss: 0.6472 - val_loss: 0.6034\n",
            "Epoch 55/1000\n",
            " - 0s - loss: 0.6415 - val_loss: 0.5975\n",
            "Epoch 56/1000\n",
            " - 0s - loss: 0.6357 - val_loss: 0.5916\n",
            "Epoch 57/1000\n",
            " - 0s - loss: 0.6302 - val_loss: 0.5859\n",
            "Epoch 58/1000\n",
            " - 0s - loss: 0.6248 - val_loss: 0.5803\n",
            "Epoch 59/1000\n",
            " - 0s - loss: 0.6198 - val_loss: 0.5750\n",
            "Epoch 60/1000\n",
            " - 0s - loss: 0.6144 - val_loss: 0.5700\n",
            "Epoch 61/1000\n",
            " - 0s - loss: 0.6095 - val_loss: 0.5648\n",
            "Epoch 62/1000\n",
            " - 0s - loss: 0.6047 - val_loss: 0.5598\n",
            "Epoch 63/1000\n",
            " - 0s - loss: 0.6000 - val_loss: 0.5552\n",
            "Epoch 64/1000\n",
            " - 0s - loss: 0.5955 - val_loss: 0.5509\n",
            "Epoch 65/1000\n",
            " - 0s - loss: 0.5909 - val_loss: 0.5462\n",
            "Epoch 66/1000\n",
            " - 0s - loss: 0.5867 - val_loss: 0.5414\n",
            "Epoch 67/1000\n",
            " - 0s - loss: 0.5826 - val_loss: 0.5370\n",
            "Epoch 68/1000\n",
            " - 0s - loss: 0.5783 - val_loss: 0.5329\n",
            "Epoch 69/1000\n",
            " - 0s - loss: 0.5742 - val_loss: 0.5290\n",
            "Epoch 70/1000\n",
            " - 0s - loss: 0.5702 - val_loss: 0.5251\n",
            "Epoch 71/1000\n",
            " - 0s - loss: 0.5665 - val_loss: 0.5211\n",
            "Epoch 72/1000\n",
            " - 0s - loss: 0.5627 - val_loss: 0.5173\n",
            "Epoch 73/1000\n",
            " - 0s - loss: 0.5591 - val_loss: 0.5136\n",
            "Epoch 74/1000\n",
            " - 0s - loss: 0.5556 - val_loss: 0.5099\n",
            "Epoch 75/1000\n",
            " - 0s - loss: 0.5525 - val_loss: 0.5062\n",
            "Epoch 76/1000\n",
            " - 0s - loss: 0.5489 - val_loss: 0.5030\n",
            "Epoch 77/1000\n",
            " - 0s - loss: 0.5456 - val_loss: 0.5000\n",
            "Epoch 78/1000\n",
            " - 0s - loss: 0.5427 - val_loss: 0.4968\n",
            "Epoch 79/1000\n",
            " - 0s - loss: 0.5396 - val_loss: 0.4937\n",
            "Epoch 80/1000\n",
            " - 0s - loss: 0.5366 - val_loss: 0.4909\n",
            "Epoch 81/1000\n",
            " - 0s - loss: 0.5338 - val_loss: 0.4880\n",
            "Epoch 82/1000\n",
            " - 0s - loss: 0.5310 - val_loss: 0.4852\n",
            "Epoch 83/1000\n",
            " - 0s - loss: 0.5284 - val_loss: 0.4823\n",
            "Epoch 84/1000\n",
            " - 0s - loss: 0.5256 - val_loss: 0.4796\n",
            "Epoch 85/1000\n",
            " - 0s - loss: 0.5230 - val_loss: 0.4771\n",
            "Epoch 86/1000\n",
            " - 0s - loss: 0.5207 - val_loss: 0.4743\n",
            "Epoch 87/1000\n",
            " - 0s - loss: 0.5181 - val_loss: 0.4717\n",
            "Epoch 88/1000\n",
            " - 0s - loss: 0.5158 - val_loss: 0.4692\n",
            "Epoch 89/1000\n",
            " - 0s - loss: 0.5135 - val_loss: 0.4669\n",
            "Epoch 90/1000\n",
            " - 0s - loss: 0.5114 - val_loss: 0.4645\n",
            "Epoch 91/1000\n",
            " - 0s - loss: 0.5093 - val_loss: 0.4623\n",
            "Epoch 92/1000\n",
            " - 0s - loss: 0.5070 - val_loss: 0.4603\n",
            "Epoch 93/1000\n",
            " - 0s - loss: 0.5051 - val_loss: 0.4584\n",
            "Epoch 94/1000\n",
            " - 0s - loss: 0.5032 - val_loss: 0.4565\n",
            "Epoch 95/1000\n",
            " - 0s - loss: 0.5015 - val_loss: 0.4547\n",
            "Epoch 96/1000\n",
            " - 0s - loss: 0.4996 - val_loss: 0.4531\n",
            "Epoch 97/1000\n",
            " - 0s - loss: 0.4980 - val_loss: 0.4514\n",
            "Epoch 98/1000\n",
            " - 0s - loss: 0.4966 - val_loss: 0.4497\n",
            "Epoch 99/1000\n",
            " - 0s - loss: 0.4950 - val_loss: 0.4482\n",
            "Epoch 100/1000\n",
            " - 0s - loss: 0.4935 - val_loss: 0.4467\n",
            "Epoch 101/1000\n",
            " - 0s - loss: 0.4923 - val_loss: 0.4452\n",
            "Epoch 102/1000\n",
            " - 0s - loss: 0.4908 - val_loss: 0.4439\n",
            "Epoch 103/1000\n",
            " - 0s - loss: 0.4895 - val_loss: 0.4426\n",
            "Epoch 104/1000\n",
            " - 0s - loss: 0.4882 - val_loss: 0.4412\n",
            "Epoch 105/1000\n",
            " - 0s - loss: 0.4871 - val_loss: 0.4399\n",
            "Epoch 106/1000\n",
            " - 0s - loss: 0.4859 - val_loss: 0.4386\n",
            "Epoch 107/1000\n",
            " - 0s - loss: 0.4847 - val_loss: 0.4374\n",
            "Epoch 108/1000\n",
            " - 0s - loss: 0.4836 - val_loss: 0.4362\n",
            "Epoch 109/1000\n",
            " - 0s - loss: 0.4825 - val_loss: 0.4351\n",
            "Epoch 110/1000\n",
            " - 0s - loss: 0.4814 - val_loss: 0.4340\n",
            "Epoch 111/1000\n",
            " - 0s - loss: 0.4804 - val_loss: 0.4329\n",
            "Epoch 112/1000\n",
            " - 0s - loss: 0.4794 - val_loss: 0.4318\n",
            "Epoch 113/1000\n",
            " - 0s - loss: 0.4783 - val_loss: 0.4307\n",
            "Epoch 114/1000\n",
            " - 0s - loss: 0.4772 - val_loss: 0.4296\n",
            "Epoch 115/1000\n",
            " - 0s - loss: 0.4764 - val_loss: 0.4285\n",
            "Epoch 116/1000\n",
            " - 0s - loss: 0.4753 - val_loss: 0.4274\n",
            "Epoch 117/1000\n",
            " - 0s - loss: 0.4741 - val_loss: 0.4263\n",
            "Epoch 118/1000\n",
            " - 0s - loss: 0.4731 - val_loss: 0.4253\n",
            "Epoch 119/1000\n",
            " - 0s - loss: 0.4721 - val_loss: 0.4241\n",
            "Epoch 120/1000\n",
            " - 0s - loss: 0.4709 - val_loss: 0.4230\n",
            "Epoch 121/1000\n",
            " - 0s - loss: 0.4699 - val_loss: 0.4218\n",
            "Epoch 122/1000\n",
            " - 0s - loss: 0.4687 - val_loss: 0.4207\n",
            "Epoch 123/1000\n",
            " - 0s - loss: 0.4675 - val_loss: 0.4194\n",
            "Epoch 124/1000\n",
            " - 0s - loss: 0.4663 - val_loss: 0.4182\n",
            "Epoch 125/1000\n",
            " - 0s - loss: 0.4651 - val_loss: 0.4168\n",
            "Epoch 126/1000\n",
            " - 0s - loss: 0.4637 - val_loss: 0.4155\n",
            "Epoch 127/1000\n",
            " - 0s - loss: 0.4624 - val_loss: 0.4141\n",
            "Epoch 128/1000\n",
            " - 0s - loss: 0.4609 - val_loss: 0.4127\n",
            "Epoch 129/1000\n",
            " - 0s - loss: 0.4595 - val_loss: 0.4112\n",
            "Epoch 130/1000\n",
            " - 0s - loss: 0.4579 - val_loss: 0.4098\n",
            "Epoch 131/1000\n",
            " - 0s - loss: 0.4564 - val_loss: 0.4084\n",
            "Epoch 132/1000\n",
            " - 0s - loss: 0.4549 - val_loss: 0.4068\n",
            "Epoch 133/1000\n",
            " - 0s - loss: 0.4532 - val_loss: 0.4053\n",
            "Epoch 134/1000\n",
            " - 0s - loss: 0.4517 - val_loss: 0.4037\n",
            "Epoch 135/1000\n",
            " - 0s - loss: 0.4501 - val_loss: 0.4022\n",
            "Epoch 136/1000\n",
            " - 0s - loss: 0.4486 - val_loss: 0.4006\n",
            "Epoch 137/1000\n",
            " - 0s - loss: 0.4469 - val_loss: 0.3992\n",
            "Epoch 138/1000\n",
            " - 0s - loss: 0.4454 - val_loss: 0.3977\n",
            "Epoch 139/1000\n",
            " - 0s - loss: 0.4439 - val_loss: 0.3963\n",
            "Epoch 140/1000\n",
            " - 0s - loss: 0.4425 - val_loss: 0.3949\n",
            "Epoch 141/1000\n",
            " - 0s - loss: 0.4411 - val_loss: 0.3935\n",
            "Epoch 142/1000\n",
            " - 0s - loss: 0.4398 - val_loss: 0.3922\n",
            "Epoch 143/1000\n",
            " - 0s - loss: 0.4384 - val_loss: 0.3908\n",
            "Epoch 144/1000\n",
            " - 0s - loss: 0.4373 - val_loss: 0.3895\n",
            "Epoch 145/1000\n",
            " - 0s - loss: 0.4358 - val_loss: 0.3882\n",
            "Epoch 146/1000\n",
            " - 0s - loss: 0.4345 - val_loss: 0.3869\n",
            "Epoch 147/1000\n",
            " - 0s - loss: 0.4332 - val_loss: 0.3856\n",
            "Epoch 148/1000\n",
            " - 0s - loss: 0.4319 - val_loss: 0.3844\n",
            "Epoch 149/1000\n",
            " - 0s - loss: 0.4306 - val_loss: 0.3831\n",
            "Epoch 150/1000\n",
            " - 0s - loss: 0.4295 - val_loss: 0.3819\n",
            "Epoch 151/1000\n",
            " - 0s - loss: 0.4282 - val_loss: 0.3806\n",
            "Epoch 152/1000\n",
            " - 0s - loss: 0.4269 - val_loss: 0.3794\n",
            "Epoch 153/1000\n",
            " - 0s - loss: 0.4256 - val_loss: 0.3782\n",
            "Epoch 154/1000\n",
            " - 0s - loss: 0.4245 - val_loss: 0.3770\n",
            "Epoch 155/1000\n",
            " - 0s - loss: 0.4232 - val_loss: 0.3758\n",
            "Epoch 156/1000\n",
            " - 0s - loss: 0.4219 - val_loss: 0.3747\n",
            "Epoch 157/1000\n",
            " - 0s - loss: 0.4206 - val_loss: 0.3735\n",
            "Epoch 158/1000\n",
            " - 0s - loss: 0.4196 - val_loss: 0.3723\n",
            "Epoch 159/1000\n",
            " - 0s - loss: 0.4181 - val_loss: 0.3711\n",
            "Epoch 160/1000\n",
            " - 0s - loss: 0.4169 - val_loss: 0.3700\n",
            "Epoch 161/1000\n",
            " - 0s - loss: 0.4156 - val_loss: 0.3688\n",
            "Epoch 162/1000\n",
            " - 0s - loss: 0.4144 - val_loss: 0.3676\n",
            "Epoch 163/1000\n",
            " - 0s - loss: 0.4132 - val_loss: 0.3664\n",
            "Epoch 164/1000\n",
            " - 0s - loss: 0.4119 - val_loss: 0.3652\n",
            "Epoch 165/1000\n",
            " - 0s - loss: 0.4108 - val_loss: 0.3640\n",
            "Epoch 166/1000\n",
            " - 0s - loss: 0.4094 - val_loss: 0.3629\n",
            "Epoch 167/1000\n",
            " - 0s - loss: 0.4082 - val_loss: 0.3617\n",
            "Epoch 168/1000\n",
            " - 0s - loss: 0.4067 - val_loss: 0.3605\n",
            "Epoch 169/1000\n",
            " - 0s - loss: 0.4056 - val_loss: 0.3593\n",
            "Epoch 170/1000\n",
            " - 0s - loss: 0.4042 - val_loss: 0.3581\n",
            "Epoch 171/1000\n",
            " - 0s - loss: 0.4029 - val_loss: 0.3569\n",
            "Epoch 172/1000\n",
            " - 0s - loss: 0.4016 - val_loss: 0.3557\n",
            "Epoch 173/1000\n",
            " - 0s - loss: 0.4005 - val_loss: 0.3545\n",
            "Epoch 174/1000\n",
            " - 0s - loss: 0.3992 - val_loss: 0.3532\n",
            "Epoch 175/1000\n",
            " - 0s - loss: 0.3980 - val_loss: 0.3520\n",
            "Epoch 176/1000\n",
            " - 0s - loss: 0.3964 - val_loss: 0.3508\n",
            "Epoch 177/1000\n",
            " - 0s - loss: 0.3952 - val_loss: 0.3496\n",
            "Epoch 178/1000\n",
            " - 0s - loss: 0.3939 - val_loss: 0.3484\n",
            "Epoch 179/1000\n",
            " - 0s - loss: 0.3926 - val_loss: 0.3472\n",
            "Epoch 180/1000\n",
            " - 0s - loss: 0.3912 - val_loss: 0.3459\n",
            "Epoch 181/1000\n",
            " - 0s - loss: 0.3899 - val_loss: 0.3447\n",
            "Epoch 182/1000\n",
            " - 0s - loss: 0.3886 - val_loss: 0.3434\n",
            "Epoch 183/1000\n",
            " - 0s - loss: 0.3872 - val_loss: 0.3422\n",
            "Epoch 184/1000\n",
            " - 0s - loss: 0.3859 - val_loss: 0.3409\n",
            "Epoch 185/1000\n",
            " - 0s - loss: 0.3845 - val_loss: 0.3396\n",
            "Epoch 186/1000\n",
            " - 0s - loss: 0.3832 - val_loss: 0.3383\n",
            "Epoch 187/1000\n",
            " - 0s - loss: 0.3818 - val_loss: 0.3370\n",
            "Epoch 188/1000\n",
            " - 0s - loss: 0.3806 - val_loss: 0.3358\n",
            "Epoch 189/1000\n",
            " - 0s - loss: 0.3791 - val_loss: 0.3345\n",
            "Epoch 190/1000\n",
            " - 0s - loss: 0.3778 - val_loss: 0.3332\n",
            "Epoch 191/1000\n",
            " - 0s - loss: 0.3764 - val_loss: 0.3319\n",
            "Epoch 192/1000\n",
            " - 0s - loss: 0.3755 - val_loss: 0.3307\n",
            "Epoch 193/1000\n",
            " - 0s - loss: 0.3735 - val_loss: 0.3294\n",
            "Epoch 194/1000\n",
            " - 0s - loss: 0.3723 - val_loss: 0.3281\n",
            "Epoch 195/1000\n",
            " - 0s - loss: 0.3712 - val_loss: 0.3269\n",
            "Epoch 196/1000\n",
            " - 0s - loss: 0.3697 - val_loss: 0.3256\n",
            "Epoch 197/1000\n",
            " - 0s - loss: 0.3684 - val_loss: 0.3243\n",
            "Epoch 198/1000\n",
            " - 0s - loss: 0.3669 - val_loss: 0.3230\n",
            "Epoch 199/1000\n",
            " - 0s - loss: 0.3656 - val_loss: 0.3217\n",
            "Epoch 200/1000\n",
            " - 0s - loss: 0.3643 - val_loss: 0.3205\n",
            "Epoch 201/1000\n",
            " - 0s - loss: 0.3628 - val_loss: 0.3192\n",
            "Epoch 202/1000\n",
            " - 0s - loss: 0.3613 - val_loss: 0.3178\n",
            "Epoch 203/1000\n",
            " - 0s - loss: 0.3599 - val_loss: 0.3165\n",
            "Epoch 204/1000\n",
            " - 0s - loss: 0.3587 - val_loss: 0.3151\n",
            "Epoch 205/1000\n",
            " - 0s - loss: 0.3572 - val_loss: 0.3138\n",
            "Epoch 206/1000\n",
            " - 0s - loss: 0.3559 - val_loss: 0.3126\n",
            "Epoch 207/1000\n",
            " - 0s - loss: 0.3544 - val_loss: 0.3113\n",
            "Epoch 208/1000\n",
            " - 0s - loss: 0.3529 - val_loss: 0.3099\n",
            "Epoch 209/1000\n",
            " - 0s - loss: 0.3516 - val_loss: 0.3086\n",
            "Epoch 210/1000\n",
            " - 0s - loss: 0.3506 - val_loss: 0.3073\n",
            "Epoch 211/1000\n",
            " - 0s - loss: 0.3489 - val_loss: 0.3060\n",
            "Epoch 212/1000\n",
            " - 0s - loss: 0.3477 - val_loss: 0.3047\n",
            "Epoch 213/1000\n",
            " - 0s - loss: 0.3460 - val_loss: 0.3033\n",
            "Epoch 214/1000\n",
            " - 0s - loss: 0.3447 - val_loss: 0.3020\n",
            "Epoch 215/1000\n",
            " - 0s - loss: 0.3434 - val_loss: 0.3007\n",
            "Epoch 216/1000\n",
            " - 0s - loss: 0.3420 - val_loss: 0.2995\n",
            "Epoch 217/1000\n",
            " - 0s - loss: 0.3407 - val_loss: 0.2981\n",
            "Epoch 218/1000\n",
            " - 0s - loss: 0.3392 - val_loss: 0.2970\n",
            "Epoch 219/1000\n",
            " - 0s - loss: 0.3378 - val_loss: 0.2959\n",
            "Epoch 220/1000\n",
            " - 0s - loss: 0.3365 - val_loss: 0.2947\n",
            "Epoch 221/1000\n",
            " - 0s - loss: 0.3353 - val_loss: 0.2930\n",
            "Epoch 222/1000\n",
            " - 0s - loss: 0.3336 - val_loss: 0.2916\n",
            "Epoch 223/1000\n",
            " - 0s - loss: 0.3327 - val_loss: 0.2903\n",
            "Epoch 224/1000\n",
            " - 0s - loss: 0.3309 - val_loss: 0.2890\n",
            "Epoch 225/1000\n",
            " - 0s - loss: 0.3297 - val_loss: 0.2880\n",
            "Epoch 226/1000\n",
            " - 0s - loss: 0.3282 - val_loss: 0.2867\n",
            "Epoch 227/1000\n",
            " - 0s - loss: 0.3270 - val_loss: 0.2853\n",
            "Epoch 228/1000\n",
            " - 0s - loss: 0.3255 - val_loss: 0.2840\n",
            "Epoch 229/1000\n",
            " - 0s - loss: 0.3243 - val_loss: 0.2826\n",
            "Epoch 230/1000\n",
            " - 0s - loss: 0.3232 - val_loss: 0.2814\n",
            "Epoch 231/1000\n",
            " - 0s - loss: 0.3215 - val_loss: 0.2803\n",
            "Epoch 232/1000\n",
            " - 0s - loss: 0.3204 - val_loss: 0.2793\n",
            "Epoch 233/1000\n",
            " - 0s - loss: 0.3194 - val_loss: 0.2780\n",
            "Epoch 234/1000\n",
            " - 0s - loss: 0.3176 - val_loss: 0.2764\n",
            "Epoch 235/1000\n",
            " - 0s - loss: 0.3163 - val_loss: 0.2752\n",
            "Epoch 236/1000\n",
            " - 0s - loss: 0.3150 - val_loss: 0.2739\n",
            "Epoch 237/1000\n",
            " - 0s - loss: 0.3138 - val_loss: 0.2727\n",
            "Epoch 238/1000\n",
            " - 0s - loss: 0.3123 - val_loss: 0.2715\n",
            "Epoch 239/1000\n",
            " - 0s - loss: 0.3109 - val_loss: 0.2704\n",
            "Epoch 240/1000\n",
            " - 0s - loss: 0.3097 - val_loss: 0.2694\n",
            "Epoch 241/1000\n",
            " - 0s - loss: 0.3085 - val_loss: 0.2681\n",
            "Epoch 242/1000\n",
            " - 0s - loss: 0.3076 - val_loss: 0.2665\n",
            "Epoch 243/1000\n",
            " - 0s - loss: 0.3060 - val_loss: 0.2653\n",
            "Epoch 244/1000\n",
            " - 0s - loss: 0.3054 - val_loss: 0.2647\n",
            "Epoch 245/1000\n",
            " - 0s - loss: 0.3035 - val_loss: 0.2632\n",
            "Epoch 246/1000\n",
            " - 0s - loss: 0.3020 - val_loss: 0.2619\n",
            "Epoch 247/1000\n",
            " - 0s - loss: 0.3009 - val_loss: 0.2608\n",
            "Epoch 248/1000\n",
            " - 0s - loss: 0.2996 - val_loss: 0.2595\n",
            "Epoch 249/1000\n",
            " - 0s - loss: 0.2983 - val_loss: 0.2583\n",
            "Epoch 250/1000\n",
            " - 0s - loss: 0.2973 - val_loss: 0.2571\n",
            "Epoch 251/1000\n",
            " - 0s - loss: 0.2964 - val_loss: 0.2560\n",
            "Epoch 252/1000\n",
            " - 0s - loss: 0.2951 - val_loss: 0.2550\n",
            "Epoch 253/1000\n",
            " - 0s - loss: 0.2935 - val_loss: 0.2536\n",
            "Epoch 254/1000\n",
            " - 0s - loss: 0.2922 - val_loss: 0.2525\n",
            "Epoch 255/1000\n",
            " - 0s - loss: 0.2910 - val_loss: 0.2515\n",
            "Epoch 256/1000\n",
            " - 0s - loss: 0.2897 - val_loss: 0.2504\n",
            "Epoch 257/1000\n",
            " - 0s - loss: 0.2889 - val_loss: 0.2491\n",
            "Epoch 258/1000\n",
            " - 0s - loss: 0.2875 - val_loss: 0.2484\n",
            "Epoch 259/1000\n",
            " - 0s - loss: 0.2864 - val_loss: 0.2472\n",
            "Epoch 260/1000\n",
            " - 0s - loss: 0.2850 - val_loss: 0.2461\n",
            "Epoch 261/1000\n",
            " - 0s - loss: 0.2841 - val_loss: 0.2450\n",
            "Epoch 262/1000\n",
            " - 0s - loss: 0.2825 - val_loss: 0.2435\n",
            "Epoch 263/1000\n",
            " - 0s - loss: 0.2818 - val_loss: 0.2424\n",
            "Epoch 264/1000\n",
            " - 0s - loss: 0.2803 - val_loss: 0.2413\n",
            "Epoch 265/1000\n",
            " - 0s - loss: 0.2795 - val_loss: 0.2411\n",
            "Epoch 266/1000\n",
            " - 0s - loss: 0.2783 - val_loss: 0.2398\n",
            "Epoch 267/1000\n",
            " - 0s - loss: 0.2784 - val_loss: 0.2382\n",
            "Epoch 268/1000\n",
            " - 0s - loss: 0.2756 - val_loss: 0.2374\n",
            "Epoch 269/1000\n",
            " - 0s - loss: 0.2750 - val_loss: 0.2367\n",
            "Epoch 270/1000\n",
            " - 0s - loss: 0.2736 - val_loss: 0.2353\n",
            "Epoch 271/1000\n",
            " - 0s - loss: 0.2725 - val_loss: 0.2339\n",
            "Epoch 272/1000\n",
            " - 0s - loss: 0.2715 - val_loss: 0.2330\n",
            "Epoch 273/1000\n",
            " - 0s - loss: 0.2702 - val_loss: 0.2319\n",
            "Epoch 274/1000\n",
            " - 0s - loss: 0.2691 - val_loss: 0.2311\n",
            "Epoch 275/1000\n",
            " - 0s - loss: 0.2681 - val_loss: 0.2301\n",
            "Epoch 276/1000\n",
            " - 0s - loss: 0.2668 - val_loss: 0.2290\n",
            "Epoch 277/1000\n",
            " - 0s - loss: 0.2658 - val_loss: 0.2279\n",
            "Epoch 278/1000\n",
            " - 0s - loss: 0.2649 - val_loss: 0.2268\n",
            "Epoch 279/1000\n",
            " - 0s - loss: 0.2637 - val_loss: 0.2262\n",
            "Epoch 280/1000\n",
            " - 0s - loss: 0.2635 - val_loss: 0.2256\n",
            "Epoch 281/1000\n",
            " - 0s - loss: 0.2623 - val_loss: 0.2238\n",
            "Epoch 282/1000\n",
            " - 0s - loss: 0.2607 - val_loss: 0.2229\n",
            "Epoch 283/1000\n",
            " - 0s - loss: 0.2596 - val_loss: 0.2219\n",
            "Epoch 284/1000\n",
            " - 0s - loss: 0.2591 - val_loss: 0.2217\n",
            "Epoch 285/1000\n",
            " - 0s - loss: 0.2584 - val_loss: 0.2207\n",
            "Epoch 286/1000\n",
            " - 0s - loss: 0.2563 - val_loss: 0.2189\n",
            "Epoch 287/1000\n",
            " - 0s - loss: 0.2559 - val_loss: 0.2181\n",
            "Epoch 288/1000\n",
            " - 0s - loss: 0.2549 - val_loss: 0.2171\n",
            "Epoch 289/1000\n",
            " - 0s - loss: 0.2537 - val_loss: 0.2167\n",
            "Epoch 290/1000\n",
            " - 0s - loss: 0.2523 - val_loss: 0.2156\n",
            "Epoch 291/1000\n",
            " - 0s - loss: 0.2512 - val_loss: 0.2143\n",
            "Epoch 292/1000\n",
            " - 0s - loss: 0.2505 - val_loss: 0.2133\n",
            "Epoch 293/1000\n",
            " - 0s - loss: 0.2494 - val_loss: 0.2124\n",
            "Epoch 294/1000\n",
            " - 0s - loss: 0.2487 - val_loss: 0.2121\n",
            "Epoch 295/1000\n",
            " - 0s - loss: 0.2478 - val_loss: 0.2115\n",
            "Epoch 296/1000\n",
            " - 0s - loss: 0.2468 - val_loss: 0.2099\n",
            "Epoch 297/1000\n",
            " - 0s - loss: 0.2457 - val_loss: 0.2088\n",
            "Epoch 298/1000\n",
            " - 0s - loss: 0.2443 - val_loss: 0.2080\n",
            "Epoch 299/1000\n",
            " - 0s - loss: 0.2436 - val_loss: 0.2074\n",
            "Epoch 300/1000\n",
            " - 0s - loss: 0.2429 - val_loss: 0.2065\n",
            "Epoch 301/1000\n",
            " - 0s - loss: 0.2419 - val_loss: 0.2052\n",
            "Epoch 302/1000\n",
            " - 0s - loss: 0.2413 - val_loss: 0.2044\n",
            "Epoch 303/1000\n",
            " - 0s - loss: 0.2398 - val_loss: 0.2036\n",
            "Epoch 304/1000\n",
            " - 0s - loss: 0.2392 - val_loss: 0.2035\n",
            "Epoch 305/1000\n",
            " - 0s - loss: 0.2381 - val_loss: 0.2025\n",
            "Epoch 306/1000\n",
            " - 0s - loss: 0.2374 - val_loss: 0.2010\n",
            "Epoch 307/1000\n",
            " - 0s - loss: 0.2365 - val_loss: 0.2003\n",
            "Epoch 308/1000\n",
            " - 0s - loss: 0.2352 - val_loss: 0.1992\n",
            "Epoch 309/1000\n",
            " - 0s - loss: 0.2346 - val_loss: 0.1984\n",
            "Epoch 310/1000\n",
            " - 0s - loss: 0.2336 - val_loss: 0.1978\n",
            "Epoch 311/1000\n",
            " - 0s - loss: 0.2329 - val_loss: 0.1969\n",
            "Epoch 312/1000\n",
            " - 0s - loss: 0.2323 - val_loss: 0.1968\n",
            "Epoch 313/1000\n",
            " - 0s - loss: 0.2310 - val_loss: 0.1955\n",
            "Epoch 314/1000\n",
            " - 0s - loss: 0.2302 - val_loss: 0.1948\n",
            "Epoch 315/1000\n",
            " - 0s - loss: 0.2299 - val_loss: 0.1935\n",
            "Epoch 316/1000\n",
            " - 0s - loss: 0.2291 - val_loss: 0.1929\n",
            "Epoch 317/1000\n",
            " - 0s - loss: 0.2274 - val_loss: 0.1921\n",
            "Epoch 318/1000\n",
            " - 0s - loss: 0.2263 - val_loss: 0.1912\n",
            "Epoch 319/1000\n",
            " - 0s - loss: 0.2259 - val_loss: 0.1904\n",
            "Epoch 320/1000\n",
            " - 0s - loss: 0.2248 - val_loss: 0.1896\n",
            "Epoch 321/1000\n",
            " - 0s - loss: 0.2238 - val_loss: 0.1889\n",
            "Epoch 322/1000\n",
            " - 0s - loss: 0.2229 - val_loss: 0.1883\n",
            "Epoch 323/1000\n",
            " - 0s - loss: 0.2232 - val_loss: 0.1881\n",
            "Epoch 324/1000\n",
            " - 0s - loss: 0.2226 - val_loss: 0.1865\n",
            "Epoch 325/1000\n",
            " - 0s - loss: 0.2207 - val_loss: 0.1858\n",
            "Epoch 326/1000\n",
            " - 0s - loss: 0.2195 - val_loss: 0.1854\n",
            "Epoch 327/1000\n",
            " - 0s - loss: 0.2192 - val_loss: 0.1852\n",
            "Epoch 328/1000\n",
            " - 0s - loss: 0.2187 - val_loss: 0.1837\n",
            "Epoch 329/1000\n",
            " - 0s - loss: 0.2187 - val_loss: 0.1834\n",
            "Epoch 330/1000\n",
            " - 0s - loss: 0.2164 - val_loss: 0.1821\n",
            "Epoch 331/1000\n",
            " - 0s - loss: 0.2162 - val_loss: 0.1813\n",
            "Epoch 332/1000\n",
            " - 0s - loss: 0.2162 - val_loss: 0.1807\n",
            "Epoch 333/1000\n",
            " - 0s - loss: 0.2145 - val_loss: 0.1801\n",
            "Epoch 334/1000\n",
            " - 0s - loss: 0.2135 - val_loss: 0.1798\n",
            "Epoch 335/1000\n",
            " - 0s - loss: 0.2129 - val_loss: 0.1787\n",
            "Epoch 336/1000\n",
            " - 0s - loss: 0.2124 - val_loss: 0.1781\n",
            "Epoch 337/1000\n",
            " - 0s - loss: 0.2114 - val_loss: 0.1771\n",
            "Epoch 338/1000\n",
            " - 0s - loss: 0.2107 - val_loss: 0.1765\n",
            "Epoch 339/1000\n",
            " - 0s - loss: 0.2096 - val_loss: 0.1757\n",
            "Epoch 340/1000\n",
            " - 0s - loss: 0.2091 - val_loss: 0.1750\n",
            "Epoch 341/1000\n",
            " - 0s - loss: 0.2085 - val_loss: 0.1747\n",
            "Epoch 342/1000\n",
            " - 0s - loss: 0.2093 - val_loss: 0.1748\n",
            "Epoch 343/1000\n",
            " - 0s - loss: 0.2067 - val_loss: 0.1734\n",
            "Epoch 344/1000\n",
            " - 0s - loss: 0.2067 - val_loss: 0.1724\n",
            "Epoch 345/1000\n",
            " - 0s - loss: 0.2059 - val_loss: 0.1717\n",
            "Epoch 346/1000\n",
            " - 0s - loss: 0.2045 - val_loss: 0.1712\n",
            "Epoch 347/1000\n",
            " - 0s - loss: 0.2048 - val_loss: 0.1708\n",
            "Epoch 348/1000\n",
            " - 0s - loss: 0.2033 - val_loss: 0.1700\n",
            "Epoch 349/1000\n",
            " - 0s - loss: 0.2030 - val_loss: 0.1691\n",
            "Epoch 350/1000\n",
            " - 0s - loss: 0.2026 - val_loss: 0.1684\n",
            "Epoch 351/1000\n",
            " - 0s - loss: 0.2011 - val_loss: 0.1678\n",
            "Epoch 352/1000\n",
            " - 0s - loss: 0.2002 - val_loss: 0.1677\n",
            "Epoch 353/1000\n",
            " - 0s - loss: 0.2001 - val_loss: 0.1674\n",
            "Epoch 354/1000\n",
            " - 0s - loss: 0.1995 - val_loss: 0.1660\n",
            "Epoch 355/1000\n",
            " - 0s - loss: 0.1985 - val_loss: 0.1655\n",
            "Epoch 356/1000\n",
            " - 0s - loss: 0.1974 - val_loss: 0.1646\n",
            "Epoch 357/1000\n",
            " - 0s - loss: 0.1968 - val_loss: 0.1640\n",
            "Epoch 358/1000\n",
            " - 0s - loss: 0.1962 - val_loss: 0.1634\n",
            "Epoch 359/1000\n",
            " - 0s - loss: 0.1956 - val_loss: 0.1628\n",
            "Epoch 360/1000\n",
            " - 0s - loss: 0.1946 - val_loss: 0.1623\n",
            "Epoch 361/1000\n",
            " - 0s - loss: 0.1939 - val_loss: 0.1619\n",
            "Epoch 362/1000\n",
            " - 0s - loss: 0.1937 - val_loss: 0.1617\n",
            "Epoch 363/1000\n",
            " - 0s - loss: 0.1927 - val_loss: 0.1605\n",
            "Epoch 364/1000\n",
            " - 0s - loss: 0.1921 - val_loss: 0.1597\n",
            "Epoch 365/1000\n",
            " - 0s - loss: 0.1916 - val_loss: 0.1591\n",
            "Epoch 366/1000\n",
            " - 0s - loss: 0.1909 - val_loss: 0.1588\n",
            "Epoch 367/1000\n",
            " - 0s - loss: 0.1904 - val_loss: 0.1584\n",
            "Epoch 368/1000\n",
            " - 0s - loss: 0.1893 - val_loss: 0.1575\n",
            "Epoch 369/1000\n",
            " - 0s - loss: 0.1889 - val_loss: 0.1568\n",
            "Epoch 370/1000\n",
            " - 0s - loss: 0.1885 - val_loss: 0.1562\n",
            "Epoch 371/1000\n",
            " - 0s - loss: 0.1876 - val_loss: 0.1556\n",
            "Epoch 372/1000\n",
            " - 0s - loss: 0.1873 - val_loss: 0.1555\n",
            "Epoch 373/1000\n",
            " - 0s - loss: 0.1867 - val_loss: 0.1547\n",
            "Epoch 374/1000\n",
            " - 0s - loss: 0.1860 - val_loss: 0.1539\n",
            "Epoch 375/1000\n",
            " - 0s - loss: 0.1850 - val_loss: 0.1534\n",
            "Epoch 376/1000\n",
            " - 0s - loss: 0.1848 - val_loss: 0.1531\n",
            "Epoch 377/1000\n",
            " - 0s - loss: 0.1838 - val_loss: 0.1523\n",
            "Epoch 378/1000\n",
            " - 0s - loss: 0.1832 - val_loss: 0.1516\n",
            "Epoch 379/1000\n",
            " - 0s - loss: 0.1825 - val_loss: 0.1511\n",
            "Epoch 380/1000\n",
            " - 0s - loss: 0.1820 - val_loss: 0.1506\n",
            "Epoch 381/1000\n",
            " - 0s - loss: 0.1818 - val_loss: 0.1504\n",
            "Epoch 382/1000\n",
            " - 0s - loss: 0.1815 - val_loss: 0.1494\n",
            "Epoch 383/1000\n",
            " - 0s - loss: 0.1801 - val_loss: 0.1491\n",
            "Epoch 384/1000\n",
            " - 0s - loss: 0.1795 - val_loss: 0.1485\n",
            "Epoch 385/1000\n",
            " - 0s - loss: 0.1789 - val_loss: 0.1479\n",
            "Epoch 386/1000\n",
            " - 0s - loss: 0.1784 - val_loss: 0.1474\n",
            "Epoch 387/1000\n",
            " - 0s - loss: 0.1776 - val_loss: 0.1469\n",
            "Epoch 388/1000\n",
            " - 0s - loss: 0.1774 - val_loss: 0.1462\n",
            "Epoch 389/1000\n",
            " - 0s - loss: 0.1770 - val_loss: 0.1459\n",
            "Epoch 390/1000\n",
            " - 0s - loss: 0.1762 - val_loss: 0.1454\n",
            "Epoch 391/1000\n",
            " - 0s - loss: 0.1754 - val_loss: 0.1446\n",
            "Epoch 392/1000\n",
            " - 0s - loss: 0.1749 - val_loss: 0.1440\n",
            "Epoch 393/1000\n",
            " - 0s - loss: 0.1745 - val_loss: 0.1436\n",
            "Epoch 394/1000\n",
            " - 0s - loss: 0.1738 - val_loss: 0.1433\n",
            "Epoch 395/1000\n",
            " - 0s - loss: 0.1731 - val_loss: 0.1428\n",
            "Epoch 396/1000\n",
            " - 0s - loss: 0.1726 - val_loss: 0.1420\n",
            "Epoch 397/1000\n",
            " - 0s - loss: 0.1719 - val_loss: 0.1415\n",
            "Epoch 398/1000\n",
            " - 0s - loss: 0.1716 - val_loss: 0.1410\n",
            "Epoch 399/1000\n",
            " - 0s - loss: 0.1708 - val_loss: 0.1407\n",
            "Epoch 400/1000\n",
            " - 0s - loss: 0.1708 - val_loss: 0.1407\n",
            "Epoch 401/1000\n",
            " - 0s - loss: 0.1698 - val_loss: 0.1397\n",
            "Epoch 402/1000\n",
            " - 0s - loss: 0.1694 - val_loss: 0.1390\n",
            "Epoch 403/1000\n",
            " - 0s - loss: 0.1687 - val_loss: 0.1385\n",
            "Epoch 404/1000\n",
            " - 0s - loss: 0.1680 - val_loss: 0.1381\n",
            "Epoch 405/1000\n",
            " - 0s - loss: 0.1674 - val_loss: 0.1378\n",
            "Epoch 406/1000\n",
            " - 0s - loss: 0.1670 - val_loss: 0.1375\n",
            "Epoch 407/1000\n",
            " - 0s - loss: 0.1669 - val_loss: 0.1367\n",
            "Epoch 408/1000\n",
            " - 0s - loss: 0.1660 - val_loss: 0.1364\n",
            "Epoch 409/1000\n",
            " - 0s - loss: 0.1655 - val_loss: 0.1359\n",
            "Epoch 410/1000\n",
            " - 0s - loss: 0.1647 - val_loss: 0.1352\n",
            "Epoch 411/1000\n",
            " - 0s - loss: 0.1644 - val_loss: 0.1347\n",
            "Epoch 412/1000\n",
            " - 0s - loss: 0.1641 - val_loss: 0.1343\n",
            "Epoch 413/1000\n",
            " - 0s - loss: 0.1633 - val_loss: 0.1338\n",
            "Epoch 414/1000\n",
            " - 0s - loss: 0.1628 - val_loss: 0.1333\n",
            "Epoch 415/1000\n",
            " - 0s - loss: 0.1622 - val_loss: 0.1328\n",
            "Epoch 416/1000\n",
            " - 0s - loss: 0.1621 - val_loss: 0.1324\n",
            "Epoch 417/1000\n",
            " - 0s - loss: 0.1612 - val_loss: 0.1319\n",
            "Epoch 418/1000\n",
            " - 0s - loss: 0.1607 - val_loss: 0.1315\n",
            "Epoch 419/1000\n",
            " - 0s - loss: 0.1606 - val_loss: 0.1310\n",
            "Epoch 420/1000\n",
            " - 0s - loss: 0.1597 - val_loss: 0.1307\n",
            "Epoch 421/1000\n",
            " - 0s - loss: 0.1597 - val_loss: 0.1304\n",
            "Epoch 422/1000\n",
            " - 0s - loss: 0.1591 - val_loss: 0.1299\n",
            "Epoch 423/1000\n",
            " - 0s - loss: 0.1592 - val_loss: 0.1293\n",
            "Epoch 424/1000\n",
            " - 0s - loss: 0.1580 - val_loss: 0.1287\n",
            "Epoch 425/1000\n",
            " - 0s - loss: 0.1573 - val_loss: 0.1283\n",
            "Epoch 426/1000\n",
            " - 0s - loss: 0.1580 - val_loss: 0.1285\n",
            "Epoch 427/1000\n",
            " - 0s - loss: 0.1561 - val_loss: 0.1275\n",
            "Epoch 428/1000\n",
            " - 0s - loss: 0.1554 - val_loss: 0.1269\n",
            "Epoch 429/1000\n",
            " - 0s - loss: 0.1552 - val_loss: 0.1266\n",
            "Epoch 430/1000\n",
            " - 0s - loss: 0.1550 - val_loss: 0.1260\n",
            "Epoch 431/1000\n",
            " - 0s - loss: 0.1545 - val_loss: 0.1257\n",
            "Epoch 432/1000\n",
            " - 0s - loss: 0.1539 - val_loss: 0.1255\n",
            "Epoch 433/1000\n",
            " - 0s - loss: 0.1533 - val_loss: 0.1250\n",
            "Epoch 434/1000\n",
            " - 0s - loss: 0.1526 - val_loss: 0.1244\n",
            "Epoch 435/1000\n",
            " - 0s - loss: 0.1527 - val_loss: 0.1240\n",
            "Epoch 436/1000\n",
            " - 0s - loss: 0.1520 - val_loss: 0.1235\n",
            "Epoch 437/1000\n",
            " - 0s - loss: 0.1536 - val_loss: 0.1237\n",
            "Epoch 438/1000\n",
            " - 0s - loss: 0.1513 - val_loss: 0.1227\n",
            "Epoch 439/1000\n",
            " - 0s - loss: 0.1504 - val_loss: 0.1223\n",
            "Epoch 440/1000\n",
            " - 0s - loss: 0.1500 - val_loss: 0.1219\n",
            "Epoch 441/1000\n",
            " - 0s - loss: 0.1496 - val_loss: 0.1215\n",
            "Epoch 442/1000\n",
            " - 0s - loss: 0.1508 - val_loss: 0.1211\n",
            "Epoch 443/1000\n",
            " - 0s - loss: 0.1491 - val_loss: 0.1209\n",
            "Epoch 444/1000\n",
            " - 0s - loss: 0.1482 - val_loss: 0.1204\n",
            "Epoch 445/1000\n",
            " - 0s - loss: 0.1478 - val_loss: 0.1199\n",
            "Epoch 446/1000\n",
            " - 0s - loss: 0.1477 - val_loss: 0.1195\n",
            "Epoch 447/1000\n",
            " - 0s - loss: 0.1472 - val_loss: 0.1195\n",
            "Epoch 448/1000\n",
            " - 0s - loss: 0.1467 - val_loss: 0.1191\n",
            "Epoch 449/1000\n",
            " - 0s - loss: 0.1460 - val_loss: 0.1184\n",
            "Epoch 450/1000\n",
            " - 0s - loss: 0.1454 - val_loss: 0.1179\n",
            "Epoch 451/1000\n",
            " - 0s - loss: 0.1456 - val_loss: 0.1175\n",
            "Epoch 452/1000\n",
            " - 0s - loss: 0.1446 - val_loss: 0.1171\n",
            "Epoch 453/1000\n",
            " - 0s - loss: 0.1445 - val_loss: 0.1169\n",
            "Epoch 454/1000\n",
            " - 0s - loss: 0.1439 - val_loss: 0.1163\n",
            "Epoch 455/1000\n",
            " - 0s - loss: 0.1434 - val_loss: 0.1159\n",
            "Epoch 456/1000\n",
            " - 0s - loss: 0.1427 - val_loss: 0.1155\n",
            "Epoch 457/1000\n",
            " - 0s - loss: 0.1426 - val_loss: 0.1151\n",
            "Epoch 458/1000\n",
            " - 0s - loss: 0.1421 - val_loss: 0.1147\n",
            "Epoch 459/1000\n",
            " - 0s - loss: 0.1417 - val_loss: 0.1143\n",
            "Epoch 460/1000\n",
            " - 0s - loss: 0.1411 - val_loss: 0.1140\n",
            "Epoch 461/1000\n",
            " - 0s - loss: 0.1407 - val_loss: 0.1136\n",
            "Epoch 462/1000\n",
            " - 0s - loss: 0.1405 - val_loss: 0.1135\n",
            "Epoch 463/1000\n",
            " - 0s - loss: 0.1398 - val_loss: 0.1129\n",
            "Epoch 464/1000\n",
            " - 0s - loss: 0.1391 - val_loss: 0.1125\n",
            "Epoch 465/1000\n",
            " - 0s - loss: 0.1395 - val_loss: 0.1122\n",
            "Epoch 466/1000\n",
            " - 0s - loss: 0.1388 - val_loss: 0.1117\n",
            "Epoch 467/1000\n",
            " - 0s - loss: 0.1380 - val_loss: 0.1114\n",
            "Epoch 468/1000\n",
            " - 0s - loss: 0.1376 - val_loss: 0.1111\n",
            "Epoch 469/1000\n",
            " - 0s - loss: 0.1374 - val_loss: 0.1107\n",
            "Epoch 470/1000\n",
            " - 0s - loss: 0.1370 - val_loss: 0.1103\n",
            "Epoch 471/1000\n",
            " - 0s - loss: 0.1365 - val_loss: 0.1099\n",
            "Epoch 472/1000\n",
            " - 0s - loss: 0.1357 - val_loss: 0.1096\n",
            "Epoch 473/1000\n",
            " - 0s - loss: 0.1361 - val_loss: 0.1096\n",
            "Epoch 474/1000\n",
            " - 0s - loss: 0.1356 - val_loss: 0.1088\n",
            "Epoch 475/1000\n",
            " - 0s - loss: 0.1349 - val_loss: 0.1085\n",
            "Epoch 476/1000\n",
            " - 0s - loss: 0.1344 - val_loss: 0.1083\n",
            "Epoch 477/1000\n",
            " - 0s - loss: 0.1340 - val_loss: 0.1078\n",
            "Epoch 478/1000\n",
            " - 0s - loss: 0.1336 - val_loss: 0.1074\n",
            "Epoch 479/1000\n",
            " - 0s - loss: 0.1332 - val_loss: 0.1071\n",
            "Epoch 480/1000\n",
            " - 0s - loss: 0.1329 - val_loss: 0.1068\n",
            "Epoch 481/1000\n",
            " - 0s - loss: 0.1325 - val_loss: 0.1063\n",
            "Epoch 482/1000\n",
            " - 0s - loss: 0.1319 - val_loss: 0.1061\n",
            "Epoch 483/1000\n",
            " - 0s - loss: 0.1317 - val_loss: 0.1057\n",
            "Epoch 484/1000\n",
            " - 0s - loss: 0.1314 - val_loss: 0.1053\n",
            "Epoch 485/1000\n",
            " - 0s - loss: 0.1309 - val_loss: 0.1050\n",
            "Epoch 486/1000\n",
            " - 0s - loss: 0.1306 - val_loss: 0.1046\n",
            "Epoch 487/1000\n",
            " - 0s - loss: 0.1302 - val_loss: 0.1042\n",
            "Epoch 488/1000\n",
            " - 0s - loss: 0.1303 - val_loss: 0.1040\n",
            "Epoch 489/1000\n",
            " - 0s - loss: 0.1296 - val_loss: 0.1037\n",
            "Epoch 490/1000\n",
            " - 0s - loss: 0.1290 - val_loss: 0.1033\n",
            "Epoch 491/1000\n",
            " - 0s - loss: 0.1286 - val_loss: 0.1029\n",
            "Epoch 492/1000\n",
            " - 0s - loss: 0.1281 - val_loss: 0.1025\n",
            "Epoch 493/1000\n",
            " - 0s - loss: 0.1296 - val_loss: 0.1026\n",
            "Epoch 494/1000\n",
            " - 0s - loss: 0.1271 - val_loss: 0.1019\n",
            "Epoch 495/1000\n",
            " - 0s - loss: 0.1274 - val_loss: 0.1020\n",
            "Epoch 496/1000\n",
            " - 0s - loss: 0.1274 - val_loss: 0.1012\n",
            "Epoch 497/1000\n",
            " - 0s - loss: 0.1261 - val_loss: 0.1009\n",
            "Epoch 498/1000\n",
            " - 0s - loss: 0.1261 - val_loss: 0.1006\n",
            "Epoch 499/1000\n",
            " - 0s - loss: 0.1262 - val_loss: 0.1003\n",
            "Epoch 500/1000\n",
            " - 0s - loss: 0.1248 - val_loss: 0.1000\n",
            "Epoch 501/1000\n",
            " - 0s - loss: 0.1250 - val_loss: 0.0999\n",
            "Epoch 502/1000\n",
            " - 0s - loss: 0.1251 - val_loss: 0.0993\n",
            "Epoch 503/1000\n",
            " - 0s - loss: 0.1240 - val_loss: 0.0990\n",
            "Epoch 504/1000\n",
            " - 0s - loss: 0.1236 - val_loss: 0.0987\n",
            "Epoch 505/1000\n",
            " - 0s - loss: 0.1233 - val_loss: 0.0983\n",
            "Epoch 506/1000\n",
            " - 0s - loss: 0.1230 - val_loss: 0.0983\n",
            "Epoch 507/1000\n",
            " - 0s - loss: 0.1227 - val_loss: 0.0978\n",
            "Epoch 508/1000\n",
            " - 0s - loss: 0.1223 - val_loss: 0.0974\n",
            "Epoch 509/1000\n",
            " - 0s - loss: 0.1221 - val_loss: 0.0971\n",
            "Epoch 510/1000\n",
            " - 0s - loss: 0.1214 - val_loss: 0.0968\n",
            "Epoch 511/1000\n",
            " - 0s - loss: 0.1219 - val_loss: 0.0966\n",
            "Epoch 512/1000\n",
            " - 0s - loss: 0.1213 - val_loss: 0.0961\n",
            "Epoch 513/1000\n",
            " - 0s - loss: 0.1203 - val_loss: 0.0959\n",
            "Epoch 514/1000\n",
            " - 0s - loss: 0.1201 - val_loss: 0.0957\n",
            "Epoch 515/1000\n",
            " - 0s - loss: 0.1197 - val_loss: 0.0952\n",
            "Epoch 516/1000\n",
            " - 0s - loss: 0.1193 - val_loss: 0.0949\n",
            "Epoch 517/1000\n",
            " - 0s - loss: 0.1190 - val_loss: 0.0947\n",
            "Epoch 518/1000\n",
            " - 0s - loss: 0.1190 - val_loss: 0.0943\n",
            "Epoch 519/1000\n",
            " - 0s - loss: 0.1191 - val_loss: 0.0941\n",
            "Epoch 520/1000\n",
            " - 0s - loss: 0.1181 - val_loss: 0.0937\n",
            "Epoch 521/1000\n",
            " - 0s - loss: 0.1176 - val_loss: 0.0934\n",
            "Epoch 522/1000\n",
            " - 0s - loss: 0.1174 - val_loss: 0.0932\n",
            "Epoch 523/1000\n",
            " - 0s - loss: 0.1174 - val_loss: 0.0929\n",
            "Epoch 524/1000\n",
            " - 0s - loss: 0.1166 - val_loss: 0.0926\n",
            "Epoch 525/1000\n",
            " - 0s - loss: 0.1165 - val_loss: 0.0924\n",
            "Epoch 526/1000\n",
            " - 0s - loss: 0.1165 - val_loss: 0.0920\n",
            "Epoch 527/1000\n",
            " - 0s - loss: 0.1160 - val_loss: 0.0918\n",
            "Epoch 528/1000\n",
            " - 0s - loss: 0.1155 - val_loss: 0.0914\n",
            "Epoch 529/1000\n",
            " - 0s - loss: 0.1157 - val_loss: 0.0911\n",
            "Epoch 530/1000\n",
            " - 0s - loss: 0.1158 - val_loss: 0.0914\n",
            "Epoch 531/1000\n",
            " - 0s - loss: 0.1156 - val_loss: 0.0910\n",
            "Epoch 532/1000\n",
            " - 0s - loss: 0.1141 - val_loss: 0.0903\n",
            "Epoch 533/1000\n",
            " - 0s - loss: 0.1142 - val_loss: 0.0906\n",
            "Epoch 534/1000\n",
            " - 0s - loss: 0.1144 - val_loss: 0.0900\n",
            "Epoch 535/1000\n",
            " - 0s - loss: 0.1131 - val_loss: 0.0895\n",
            "Epoch 536/1000\n",
            " - 0s - loss: 0.1127 - val_loss: 0.0896\n",
            "Epoch 537/1000\n",
            " - 0s - loss: 0.1130 - val_loss: 0.0897\n",
            "Epoch 00537: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa95f844ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "xxorfMBEdkpY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Avaliando o Resultado da Classificação em Redes Neurais"
      ]
    },
    {
      "metadata": {
        "id": "zSN5xXUhdkpa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Calculando a Acurácia de Classificação\n",
        " \n",
        "A Acurácia é o número de linhas onde a rede neural previu corretamente a classe alvo. A Acurácia é usada apenas para classificação, não regressão.\n",
        "\n",
        "$ accuracy = \\frac{\\textit{#} \\ correct}{N} $\n",
        "\n",
        "Onde $N$ é o tamanho do conjunto avaliado (treinamento ou validação). A Acurácia deve ser tão alta quanto possível."
      ]
    },
    {
      "metadata": {
        "id": "cJHCwCggdkpb",
        "colab_type": "code",
        "outputId": "efd444ef-4a48-4373-f2d2-e747ad9d2de7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "\n",
        "# Avaliando o resultado usando a Acurácia\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 22)\n",
        "pred = model.predict(x_test)\n",
        "pred = np.argmax(pred,axis = 1) \n",
        "y_compare = np.argmax(y_test, axis = 1) \n",
        "score = metrics.accuracy_score(y_compare, pred)\n",
        "print(\"Accurácia: {}\".format(score))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accurácia: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qwHCaNJjdkpj",
        "colab_type": "code",
        "outputId": "6b61f41e-a4bf-404c-f499-67540fda447d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "print(pred[:1])\n",
        "print(y_test[:1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\n",
            "[[1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OU6oMHVgdkpo",
        "colab_type": "code",
        "outputId": "d461f906-75c8-4272-9958-880bf93f9e55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "y_test[:3]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "ifFUqabbdkpt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Calculando o Log Loss de Classificação\n",
        "\n",
        "As redes neurais podem prever uma probabilidade de cada uma das classes alvo. As redes neurais darão probabilidades elevadas a previsões que são mais prováveis. O Log Loss é uma métrica de erro que penaliza a confiança em respostas erradas. Valores baixos de Log Loss são desejados.\n",
        "\n",
        "Para qualquer modelo scikit-learn, existem duas maneiras de obter uma previsão:\n",
        "\n",
        "* **predict** - No caso da saída de classificação, o id numérico da classe prevista. Para a regressão, isso é simplesmente a previsão.\n",
        "* **predict_proba** - No caso da classificação, a probabilidade de cada uma das classes. Não utilizado para regressão.\n",
        "\n",
        "O código a seguir mostra a saída de predict_proba:"
      ]
    },
    {
      "metadata": {
        "id": "UKRx04mDdkpu",
        "colab_type": "code",
        "outputId": "2e3ed31c-7fcf-4bac-b778-3c465653c09a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "# Não exibe numpy na notação científica\n",
        "np.set_printoptions(precision=4)\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "# Gerar previsões\n",
        "pred = model.predict(x_test)\n",
        "\n",
        "print(\"Array Numpy de Previsões\")\n",
        "print(pred[0]*100)\n",
        "\n",
        "print(\"Como Percentual de Probabilidade\")\n",
        "display(pred[0:5])\n",
        "\n",
        "score = metrics.log_loss(y_test, pred)\n",
        "print(\"Log loss score: {}\".format(score))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Array Numpy de Previsões\n",
            "[99.9729  0.0271  0.    ]\n",
            "Como Percentual de Probabilidade\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[0.9997, 0.0003, 0.    ],\n",
              "       [0.    , 0.031 , 0.969 ],\n",
              "       [0.0144, 0.824 , 0.1616],\n",
              "       [0.    , 0.0694, 0.9305],\n",
              "       [0.0139, 0.8195, 0.1666]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Log loss score: 0.10957689619131593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zeEMBm8Cdkp2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Log loss é calculado assim:\n",
        "\n",
        "$ \\text{log loss} = -\\frac{1}{N}\\sum_{i=1}^N {( {y}_i\\log(\\hat{y}_i) + (1 - {y}_i)\\log(1 - \\hat{y}_i))} $"
      ]
    },
    {
      "metadata": {
        "id": "gZ-sLtj6dkp4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Avaliando o Resultado da Regressão em Redes Neurais"
      ]
    },
    {
      "metadata": {
        "id": "1JLaMSdudkp5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Avaliando Resultado de Regressão\n",
        "\n",
        "Os resultados da regressão são avaliados de forma diferente da classificação. Considere o seguinte código que treina uma rede neural para o conjunto de dados MPG."
      ]
    },
    {
      "metadata": {
        "id": "hQ_RZr4yUvRj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#files.upload()\n",
        "#!cp *.csv data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "879728bb-2981-498c-c975-212a78cd8eba",
        "id": "mrpc_1PwUnq8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Path\n",
        "path = \"./data/\"\n",
        "\n",
        "# Arquivo\n",
        "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
        "\n",
        "# Dataframe\n",
        "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
        "\n",
        "# Limpeza e transformação\n",
        "cars = df['name']\n",
        "df.drop('name',1,inplace=True)\n",
        "missing_median(df, 'horsepower')\n",
        "x,y = to_xy(df,\"mpg\")\n",
        "\n",
        "# Split em treino/teste\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 45)\n",
        "\n",
        "# Rede Neural\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_dim = x.shape[1], kernel_initializer = 'normal', activation = 'relu'))\n",
        "model.add(Dense(1, kernel_initializer = 'normal'))\n",
        "model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
        "monitor = EarlyStopping(monitor = 'val_loss', min_delta = 1e-3, patience = 5, verbose = 1, mode = 'auto')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 10)                80        \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 91\n",
            "Trainable params: 91\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3fbwvcjsUyRf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6698
        },
        "outputId": "686136dd-4c95-42a5-f183-1c972506157c"
      },
      "cell_type": "code",
      "source": [
        "# treina o modelo com early-stop\n",
        "model.fit(x, y, validation_data = (x_test,y_test), callbacks = [monitor], verbose = 2, epochs = 1000)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 398 samples, validate on 100 samples\n",
            "Epoch 1/1000\n",
            " - 0s - loss: 741.2371 - val_loss: 470.9362\n",
            "Epoch 2/1000\n",
            " - 0s - loss: 363.3744 - val_loss: 195.5957\n",
            "Epoch 3/1000\n",
            " - 0s - loss: 189.9451 - val_loss: 144.9436\n",
            "Epoch 4/1000\n",
            " - 0s - loss: 182.5271 - val_loss: 147.0537\n",
            "Epoch 5/1000\n",
            " - 0s - loss: 174.9367 - val_loss: 140.2814\n",
            "Epoch 6/1000\n",
            " - 0s - loss: 172.8778 - val_loss: 139.4642\n",
            "Epoch 7/1000\n",
            " - 0s - loss: 171.3224 - val_loss: 137.8108\n",
            "Epoch 8/1000\n",
            " - 0s - loss: 169.2389 - val_loss: 136.2487\n",
            "Epoch 9/1000\n",
            " - 0s - loss: 167.3649 - val_loss: 134.6479\n",
            "Epoch 10/1000\n",
            " - 0s - loss: 165.5337 - val_loss: 133.0965\n",
            "Epoch 11/1000\n",
            " - 0s - loss: 163.5397 - val_loss: 130.9522\n",
            "Epoch 12/1000\n",
            " - 0s - loss: 161.4692 - val_loss: 128.8972\n",
            "Epoch 13/1000\n",
            " - 0s - loss: 159.0936 - val_loss: 126.7564\n",
            "Epoch 14/1000\n",
            " - 0s - loss: 155.9524 - val_loss: 124.2145\n",
            "Epoch 15/1000\n",
            " - 0s - loss: 152.7376 - val_loss: 121.7466\n",
            "Epoch 16/1000\n",
            " - 0s - loss: 149.5563 - val_loss: 118.3858\n",
            "Epoch 17/1000\n",
            " - 0s - loss: 145.8741 - val_loss: 115.1657\n",
            "Epoch 18/1000\n",
            " - 0s - loss: 141.9836 - val_loss: 111.7671\n",
            "Epoch 19/1000\n",
            " - 0s - loss: 139.0888 - val_loss: 108.0808\n",
            "Epoch 20/1000\n",
            " - 0s - loss: 134.2937 - val_loss: 104.2995\n",
            "Epoch 21/1000\n",
            " - 0s - loss: 129.7156 - val_loss: 99.9515\n",
            "Epoch 22/1000\n",
            " - 0s - loss: 125.3269 - val_loss: 96.0198\n",
            "Epoch 23/1000\n",
            " - 0s - loss: 118.5790 - val_loss: 91.1799\n",
            "Epoch 24/1000\n",
            " - 0s - loss: 113.4493 - val_loss: 86.8506\n",
            "Epoch 25/1000\n",
            " - 0s - loss: 107.3874 - val_loss: 81.9871\n",
            "Epoch 26/1000\n",
            " - 0s - loss: 102.1274 - val_loss: 77.6876\n",
            "Epoch 27/1000\n",
            " - 0s - loss: 98.8138 - val_loss: 73.4830\n",
            "Epoch 28/1000\n",
            " - 0s - loss: 92.2620 - val_loss: 69.4501\n",
            "Epoch 29/1000\n",
            " - 0s - loss: 87.4389 - val_loss: 65.4887\n",
            "Epoch 30/1000\n",
            " - 0s - loss: 82.2384 - val_loss: 62.2945\n",
            "Epoch 31/1000\n",
            " - 0s - loss: 78.2873 - val_loss: 58.3683\n",
            "Epoch 32/1000\n",
            " - 0s - loss: 73.8339 - val_loss: 55.4025\n",
            "Epoch 33/1000\n",
            " - 0s - loss: 69.7208 - val_loss: 52.3340\n",
            "Epoch 34/1000\n",
            " - 0s - loss: 65.5080 - val_loss: 49.2540\n",
            "Epoch 35/1000\n",
            " - 0s - loss: 61.9362 - val_loss: 46.7108\n",
            "Epoch 36/1000\n",
            " - 0s - loss: 58.5175 - val_loss: 44.4322\n",
            "Epoch 37/1000\n",
            " - 0s - loss: 55.8072 - val_loss: 42.3911\n",
            "Epoch 38/1000\n",
            " - 0s - loss: 52.9360 - val_loss: 41.1758\n",
            "Epoch 39/1000\n",
            " - 0s - loss: 49.8449 - val_loss: 40.0057\n",
            "Epoch 40/1000\n",
            " - 0s - loss: 47.8911 - val_loss: 39.0876\n",
            "Epoch 41/1000\n",
            " - 0s - loss: 45.9633 - val_loss: 37.6059\n",
            "Epoch 42/1000\n",
            " - 0s - loss: 44.4212 - val_loss: 38.3032\n",
            "Epoch 43/1000\n",
            " - 0s - loss: 43.9712 - val_loss: 37.7962\n",
            "Epoch 44/1000\n",
            " - 0s - loss: 41.9196 - val_loss: 36.5643\n",
            "Epoch 45/1000\n",
            " - 0s - loss: 40.4782 - val_loss: 33.0143\n",
            "Epoch 46/1000\n",
            " - 0s - loss: 38.7987 - val_loss: 32.3897\n",
            "Epoch 47/1000\n",
            " - 0s - loss: 37.6515 - val_loss: 32.2848\n",
            "Epoch 48/1000\n",
            " - 0s - loss: 36.6698 - val_loss: 31.2476\n",
            "Epoch 49/1000\n",
            " - 0s - loss: 35.6721 - val_loss: 31.5114\n",
            "Epoch 50/1000\n",
            " - 0s - loss: 35.3045 - val_loss: 30.6728\n",
            "Epoch 51/1000\n",
            " - 0s - loss: 35.1639 - val_loss: 31.2722\n",
            "Epoch 52/1000\n",
            " - 0s - loss: 33.9427 - val_loss: 29.4911\n",
            "Epoch 53/1000\n",
            " - 0s - loss: 33.9082 - val_loss: 31.0753\n",
            "Epoch 54/1000\n",
            " - 0s - loss: 32.7592 - val_loss: 28.8839\n",
            "Epoch 55/1000\n",
            " - 0s - loss: 32.0823 - val_loss: 28.6597\n",
            "Epoch 56/1000\n",
            " - 0s - loss: 31.9879 - val_loss: 29.5798\n",
            "Epoch 57/1000\n",
            " - 0s - loss: 31.6018 - val_loss: 28.0207\n",
            "Epoch 58/1000\n",
            " - 0s - loss: 30.8490 - val_loss: 27.9035\n",
            "Epoch 59/1000\n",
            " - 0s - loss: 30.5899 - val_loss: 28.1357\n",
            "Epoch 60/1000\n",
            " - 0s - loss: 30.1597 - val_loss: 27.1354\n",
            "Epoch 61/1000\n",
            " - 0s - loss: 30.2191 - val_loss: 28.8095\n",
            "Epoch 62/1000\n",
            " - 0s - loss: 29.7219 - val_loss: 26.6995\n",
            "Epoch 63/1000\n",
            " - 0s - loss: 29.1126 - val_loss: 27.6331\n",
            "Epoch 64/1000\n",
            " - 0s - loss: 29.1856 - val_loss: 26.4039\n",
            "Epoch 65/1000\n",
            " - 0s - loss: 28.8664 - val_loss: 26.2672\n",
            "Epoch 66/1000\n",
            " - 0s - loss: 28.3820 - val_loss: 26.0723\n",
            "Epoch 67/1000\n",
            " - 0s - loss: 28.1718 - val_loss: 25.7071\n",
            "Epoch 68/1000\n",
            " - 0s - loss: 27.7025 - val_loss: 27.1863\n",
            "Epoch 69/1000\n",
            " - 0s - loss: 27.8331 - val_loss: 25.2933\n",
            "Epoch 70/1000\n",
            " - 0s - loss: 27.4446 - val_loss: 26.1468\n",
            "Epoch 71/1000\n",
            " - 0s - loss: 27.4083 - val_loss: 25.1668\n",
            "Epoch 72/1000\n",
            " - 0s - loss: 27.5358 - val_loss: 25.4288\n",
            "Epoch 73/1000\n",
            " - 0s - loss: 26.8833 - val_loss: 25.0788\n",
            "Epoch 74/1000\n",
            " - 0s - loss: 26.5757 - val_loss: 24.5881\n",
            "Epoch 75/1000\n",
            " - 0s - loss: 26.2179 - val_loss: 24.3438\n",
            "Epoch 76/1000\n",
            " - 0s - loss: 26.0266 - val_loss: 24.2732\n",
            "Epoch 77/1000\n",
            " - 0s - loss: 25.8689 - val_loss: 24.2568\n",
            "Epoch 78/1000\n",
            " - 0s - loss: 26.4803 - val_loss: 24.0144\n",
            "Epoch 79/1000\n",
            " - 0s - loss: 25.9066 - val_loss: 23.6856\n",
            "Epoch 80/1000\n",
            " - 0s - loss: 25.5388 - val_loss: 23.4374\n",
            "Epoch 81/1000\n",
            " - 0s - loss: 25.4264 - val_loss: 24.4548\n",
            "Epoch 82/1000\n",
            " - 0s - loss: 25.4764 - val_loss: 23.6206\n",
            "Epoch 83/1000\n",
            " - 0s - loss: 24.8150 - val_loss: 22.8069\n",
            "Epoch 84/1000\n",
            " - 0s - loss: 24.4089 - val_loss: 23.8033\n",
            "Epoch 85/1000\n",
            " - 0s - loss: 24.3139 - val_loss: 22.6345\n",
            "Epoch 86/1000\n",
            " - 0s - loss: 24.3283 - val_loss: 22.5016\n",
            "Epoch 87/1000\n",
            " - 0s - loss: 23.9498 - val_loss: 22.2384\n",
            "Epoch 88/1000\n",
            " - 0s - loss: 23.8172 - val_loss: 22.0228\n",
            "Epoch 89/1000\n",
            " - 0s - loss: 23.8061 - val_loss: 22.3576\n",
            "Epoch 90/1000\n",
            " - 0s - loss: 23.5555 - val_loss: 21.7811\n",
            "Epoch 91/1000\n",
            " - 0s - loss: 23.6047 - val_loss: 21.7775\n",
            "Epoch 92/1000\n",
            " - 0s - loss: 23.5811 - val_loss: 21.4478\n",
            "Epoch 93/1000\n",
            " - 0s - loss: 23.3773 - val_loss: 22.0622\n",
            "Epoch 94/1000\n",
            " - 0s - loss: 22.8127 - val_loss: 21.1494\n",
            "Epoch 95/1000\n",
            " - 0s - loss: 22.5521 - val_loss: 20.9423\n",
            "Epoch 96/1000\n",
            " - 0s - loss: 22.4120 - val_loss: 20.6092\n",
            "Epoch 97/1000\n",
            " - 0s - loss: 22.3406 - val_loss: 20.6891\n",
            "Epoch 98/1000\n",
            " - 0s - loss: 22.6989 - val_loss: 20.9470\n",
            "Epoch 99/1000\n",
            " - 0s - loss: 21.7986 - val_loss: 20.3099\n",
            "Epoch 100/1000\n",
            " - 0s - loss: 21.8450 - val_loss: 20.3021\n",
            "Epoch 101/1000\n",
            " - 0s - loss: 21.8113 - val_loss: 20.5059\n",
            "Epoch 102/1000\n",
            " - 0s - loss: 21.3927 - val_loss: 19.9063\n",
            "Epoch 103/1000\n",
            " - 0s - loss: 21.8327 - val_loss: 19.7395\n",
            "Epoch 104/1000\n",
            " - 0s - loss: 21.5420 - val_loss: 19.7790\n",
            "Epoch 105/1000\n",
            " - 0s - loss: 20.8976 - val_loss: 19.8035\n",
            "Epoch 106/1000\n",
            " - 0s - loss: 20.7643 - val_loss: 19.3542\n",
            "Epoch 107/1000\n",
            " - 0s - loss: 20.5017 - val_loss: 19.1827\n",
            "Epoch 108/1000\n",
            " - 0s - loss: 20.4447 - val_loss: 18.9935\n",
            "Epoch 109/1000\n",
            " - 0s - loss: 20.5035 - val_loss: 20.0248\n",
            "Epoch 110/1000\n",
            " - 0s - loss: 20.1882 - val_loss: 18.8033\n",
            "Epoch 111/1000\n",
            " - 0s - loss: 20.0419 - val_loss: 19.1285\n",
            "Epoch 112/1000\n",
            " - 0s - loss: 19.9293 - val_loss: 18.6057\n",
            "Epoch 113/1000\n",
            " - 0s - loss: 19.8421 - val_loss: 18.3978\n",
            "Epoch 114/1000\n",
            " - 0s - loss: 19.6344 - val_loss: 18.2765\n",
            "Epoch 115/1000\n",
            " - 0s - loss: 19.7693 - val_loss: 18.2796\n",
            "Epoch 116/1000\n",
            " - 0s - loss: 19.9156 - val_loss: 18.7549\n",
            "Epoch 117/1000\n",
            " - 0s - loss: 18.9473 - val_loss: 17.7697\n",
            "Epoch 118/1000\n",
            " - 0s - loss: 18.9801 - val_loss: 17.6551\n",
            "Epoch 119/1000\n",
            " - 0s - loss: 19.0291 - val_loss: 18.3791\n",
            "Epoch 120/1000\n",
            " - 0s - loss: 19.8701 - val_loss: 17.2904\n",
            "Epoch 121/1000\n",
            " - 0s - loss: 18.6678 - val_loss: 18.2333\n",
            "Epoch 122/1000\n",
            " - 0s - loss: 18.7543 - val_loss: 17.3139\n",
            "Epoch 123/1000\n",
            " - 0s - loss: 18.3029 - val_loss: 16.9227\n",
            "Epoch 124/1000\n",
            " - 0s - loss: 18.3013 - val_loss: 18.4997\n",
            "Epoch 125/1000\n",
            " - 0s - loss: 18.3799 - val_loss: 17.3061\n",
            "Epoch 126/1000\n",
            " - 0s - loss: 18.3053 - val_loss: 16.5107\n",
            "Epoch 127/1000\n",
            " - 0s - loss: 18.2047 - val_loss: 16.4773\n",
            "Epoch 128/1000\n",
            " - 0s - loss: 17.4573 - val_loss: 16.4799\n",
            "Epoch 129/1000\n",
            " - 0s - loss: 17.2001 - val_loss: 16.1433\n",
            "Epoch 130/1000\n",
            " - 0s - loss: 17.1539 - val_loss: 16.0434\n",
            "Epoch 131/1000\n",
            " - 0s - loss: 17.0547 - val_loss: 15.9273\n",
            "Epoch 132/1000\n",
            " - 0s - loss: 16.9845 - val_loss: 18.1314\n",
            "Epoch 133/1000\n",
            " - 0s - loss: 17.5646 - val_loss: 16.0080\n",
            "Epoch 134/1000\n",
            " - 0s - loss: 17.2702 - val_loss: 15.5308\n",
            "Epoch 135/1000\n",
            " - 0s - loss: 16.4477 - val_loss: 15.3515\n",
            "Epoch 136/1000\n",
            " - 0s - loss: 16.4301 - val_loss: 16.2193\n",
            "Epoch 137/1000\n",
            " - 0s - loss: 16.3085 - val_loss: 15.5263\n",
            "Epoch 138/1000\n",
            " - 0s - loss: 16.3368 - val_loss: 15.0567\n",
            "Epoch 139/1000\n",
            " - 0s - loss: 16.5155 - val_loss: 15.0736\n",
            "Epoch 140/1000\n",
            " - 0s - loss: 16.0928 - val_loss: 14.9885\n",
            "Epoch 141/1000\n",
            " - 0s - loss: 15.8668 - val_loss: 15.3135\n",
            "Epoch 142/1000\n",
            " - 0s - loss: 15.8449 - val_loss: 14.6541\n",
            "Epoch 143/1000\n",
            " - 0s - loss: 16.0738 - val_loss: 15.2882\n",
            "Epoch 144/1000\n",
            " - 0s - loss: 16.0493 - val_loss: 14.5628\n",
            "Epoch 145/1000\n",
            " - 0s - loss: 15.5815 - val_loss: 15.4709\n",
            "Epoch 146/1000\n",
            " - 0s - loss: 15.6287 - val_loss: 14.1254\n",
            "Epoch 147/1000\n",
            " - 0s - loss: 15.1968 - val_loss: 14.0155\n",
            "Epoch 148/1000\n",
            " - 0s - loss: 15.3374 - val_loss: 13.9848\n",
            "Epoch 149/1000\n",
            " - 0s - loss: 15.0291 - val_loss: 14.1199\n",
            "Epoch 150/1000\n",
            " - 0s - loss: 14.9174 - val_loss: 13.7668\n",
            "Epoch 151/1000\n",
            " - 0s - loss: 14.6619 - val_loss: 13.8208\n",
            "Epoch 152/1000\n",
            " - 0s - loss: 14.6315 - val_loss: 13.8959\n",
            "Epoch 153/1000\n",
            " - 0s - loss: 14.5918 - val_loss: 13.4940\n",
            "Epoch 154/1000\n",
            " - 0s - loss: 14.6617 - val_loss: 13.6096\n",
            "Epoch 155/1000\n",
            " - 0s - loss: 15.5593 - val_loss: 15.6679\n",
            "Epoch 156/1000\n",
            " - 0s - loss: 14.6912 - val_loss: 13.3296\n",
            "Epoch 157/1000\n",
            " - 0s - loss: 14.2643 - val_loss: 13.1819\n",
            "Epoch 158/1000\n",
            " - 0s - loss: 14.0569 - val_loss: 13.0990\n",
            "Epoch 159/1000\n",
            " - 0s - loss: 14.0035 - val_loss: 13.4298\n",
            "Epoch 160/1000\n",
            " - 0s - loss: 13.7416 - val_loss: 12.9493\n",
            "Epoch 161/1000\n",
            " - 0s - loss: 14.3901 - val_loss: 13.5390\n",
            "Epoch 162/1000\n",
            " - 0s - loss: 13.6945 - val_loss: 12.8109\n",
            "Epoch 163/1000\n",
            " - 0s - loss: 13.9109 - val_loss: 13.4148\n",
            "Epoch 164/1000\n",
            " - 0s - loss: 13.8992 - val_loss: 13.5928\n",
            "Epoch 165/1000\n",
            " - 0s - loss: 13.8413 - val_loss: 12.5903\n",
            "Epoch 166/1000\n",
            " - 0s - loss: 13.2624 - val_loss: 12.3282\n",
            "Epoch 167/1000\n",
            " - 0s - loss: 13.1814 - val_loss: 12.5467\n",
            "Epoch 168/1000\n",
            " - 0s - loss: 13.4645 - val_loss: 12.3396\n",
            "Epoch 169/1000\n",
            " - 0s - loss: 13.5235 - val_loss: 12.6611\n",
            "Epoch 170/1000\n",
            " - 0s - loss: 13.9120 - val_loss: 12.3066\n",
            "Epoch 171/1000\n",
            " - 0s - loss: 13.2421 - val_loss: 16.1955\n",
            "Epoch 172/1000\n",
            " - 0s - loss: 14.4316 - val_loss: 12.0508\n",
            "Epoch 173/1000\n",
            " - 0s - loss: 12.8346 - val_loss: 11.9243\n",
            "Epoch 174/1000\n",
            " - 0s - loss: 12.7519 - val_loss: 11.9661\n",
            "Epoch 175/1000\n",
            " - 0s - loss: 12.6325 - val_loss: 12.7053\n",
            "Epoch 176/1000\n",
            " - 0s - loss: 12.6095 - val_loss: 11.6416\n",
            "Epoch 177/1000\n",
            " - 0s - loss: 12.3906 - val_loss: 11.6002\n",
            "Epoch 178/1000\n",
            " - 0s - loss: 12.5208 - val_loss: 11.5021\n",
            "Epoch 179/1000\n",
            " - 0s - loss: 12.7747 - val_loss: 12.1950\n",
            "Epoch 180/1000\n",
            " - 0s - loss: 12.2840 - val_loss: 11.3286\n",
            "Epoch 181/1000\n",
            " - 0s - loss: 12.1298 - val_loss: 11.2307\n",
            "Epoch 182/1000\n",
            " - 0s - loss: 12.1625 - val_loss: 11.8185\n",
            "Epoch 183/1000\n",
            " - 0s - loss: 12.6245 - val_loss: 11.3622\n",
            "Epoch 184/1000\n",
            " - 0s - loss: 12.5192 - val_loss: 11.0135\n",
            "Epoch 185/1000\n",
            " - 0s - loss: 12.0181 - val_loss: 11.0394\n",
            "Epoch 186/1000\n",
            " - 0s - loss: 11.9298 - val_loss: 10.9393\n",
            "Epoch 187/1000\n",
            " - 0s - loss: 11.7415 - val_loss: 10.7789\n",
            "Epoch 188/1000\n",
            " - 0s - loss: 11.7491 - val_loss: 10.7317\n",
            "Epoch 189/1000\n",
            " - 0s - loss: 11.6935 - val_loss: 10.7525\n",
            "Epoch 190/1000\n",
            " - 0s - loss: 11.5730 - val_loss: 10.6164\n",
            "Epoch 191/1000\n",
            " - 0s - loss: 11.4225 - val_loss: 11.0294\n",
            "Epoch 192/1000\n",
            " - 0s - loss: 11.5480 - val_loss: 11.1819\n",
            "Epoch 193/1000\n",
            " - 0s - loss: 11.2871 - val_loss: 10.7445\n",
            "Epoch 194/1000\n",
            " - 0s - loss: 11.9697 - val_loss: 13.1118\n",
            "Epoch 195/1000\n",
            " - 0s - loss: 13.2248 - val_loss: 11.2955\n",
            "Epoch 00195: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa91972ae10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "Osj2KTocdkqI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mean Square Error\n",
        "\n",
        "O erro quadrático médio é a soma das diferenças quadradas entre a previsão ($\\hat{y}$) e o esperado ($y$). Os valores de MSE não são de uma unidade específica. Se um valor MSE diminuiu para um modelo, isso é bom. No entanto, além disso, não há muito mais que você possa determinar. São desejados valores de MSE baixos.\n",
        "\n",
        "$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n \\left(\\hat{y}_i - y_i\\right)^2 $\n"
      ]
    },
    {
      "metadata": {
        "id": "19tFUfZ6dkqJ",
        "colab_type": "code",
        "outputId": "96d045d3-c3f4-4209-94a8-9b8f0965854e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Previsões\n",
        "pred = model.predict(x_test)\n",
        "\n",
        "# Medindo o MSE\n",
        "score = metrics.mean_squared_error(pred,y_test)\n",
        "print(\"Final score (MSE): {}\".format(score))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final score (MSE): 11.295487403869629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UdAkulnZdkqN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Root Mean Square Error\n",
        "\n",
        "O quadrado médio da raiz (RMSE) é essencialmente a raiz quadrada do MSE. Por causa disso, o erro RMSE está nas mesmas unidades que o resultado dos dados de treinamento. São desejados valores RMSE baixos.\n",
        "\n",
        "$ \\text{MSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left(\\hat{y}_i - y_i\\right)^2} $"
      ]
    },
    {
      "metadata": {
        "id": "KOdeIXoKdkqP",
        "colab_type": "code",
        "outputId": "4510817c-5686-4b12-cd02-87bca3748654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Medindo o RMSE\n",
        "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
        "print(\"Final score (RMSE): {}\".format(score))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final score (RMSE): 3.3608760833740234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EDxWHzK7dkqX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Treinamento com Cross Validation\n",
        "\n",
        "A validação cruzada usa várias \"dobras\" (folds) e vários modelos, para gerar previsões de amostra em todo o conjunto de dados. É importante notar que haverá um modelo (rede neural) para cada dobra. Cada modelo contribui parte da previsão final fora da amostra.\n",
        "\n",
        "![K-Fold Crossvalidation](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/DNN/images/kfold.png?raw=true \"K-Fold Crossvalidation\")\n",
        "\n",
        "Para novos dados, que são dados não presentes no conjunto de treinamento, as previsões dos modelos dos folds podem ser tratadas de várias maneiras.\n",
        "\n",
        "* Escolha o modelo que teve o maior resultado de validação como modelo final.\n",
        "* Predefinição de novos dados para os 5 modelos e média do resultado \n",
        "* Retome um novo modelo (usando as mesmas configurações que a validação cruzada) em todo o conjunto de dados. Treine para vários passos e com a mesma estrutura de camada oculta.\n",
        "\n",
        "O código a seguir treina o conjunto de dados MPG usando uma validação cruzada 5 vezes. O desempenho esperado de uma rede neural, do tipo treinado aqui, seria a pontuação para as previsões geradas fora da amostra."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "q0BnTfYsdkqY",
        "colab_type": "code",
        "outputId": "72e5cb50-752d-4077-addb-71cdc05c27b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from scipy.stats import zscore\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "\n",
        "# Path\n",
        "path = \"./data/\"\n",
        "\n",
        "# Arquivos\n",
        "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
        "filename_write = os.path.join(path,\"auto-mpg-saida.csv\")\n",
        "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
        "\n",
        "# Shuffle\n",
        "np.random.seed(42)\n",
        "df = df.reindex(np.random.permutation(df.index))\n",
        "df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "# Preprocesso\n",
        "cars = df['name']\n",
        "df.drop('name',1,inplace=True)\n",
        "missing_median(df, 'horsepower')\n",
        "\n",
        "# Encode para uma matriz 2D \n",
        "x,y = to_xy(df,'mpg')\n",
        "\n",
        "# Cross validate\n",
        "kf = KFold(5)\n",
        "    \n",
        "oos_y = []\n",
        "oos_pred = []\n",
        "fold = 0\n",
        "\n",
        "# Treino\n",
        "for train, test in kf.split(x):\n",
        "    fold += 1\n",
        "    print(\"Fold #{}\".format(fold))\n",
        "        \n",
        "    x_train = x[train]\n",
        "    y_train = y[train]\n",
        "    x_test = x[test]\n",
        "    y_test = y[test]\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Dense(20, input_dim = x.shape[1], activation = 'relu'))\n",
        "    model.add(Dense(10, activation = 'relu', kernel_initializer = 'normal')) # add\n",
        "    #model.add(Dense(10, activation = 'relu')) # add\n",
        "    model.add(Dense(1, kernel_initializer = 'normal'))\n",
        "    model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
        "                                                  # melhora na perda, patience - num epochs para avaliar\n",
        "    monitor = EarlyStopping(monitor = 'val_loss', min_delta = 1e-3, patience = 5, verbose = 1, mode = 'auto')\n",
        "    model.fit(x_train, y_train,validation_data = (x_test,y_test), callbacks = [monitor], verbose = 0, epochs = 1200)\n",
        "    \n",
        "    pred = model.predict(x_test)\n",
        "    \n",
        "    oos_y.append(y_test)\n",
        "    oos_pred.append(pred)        \n",
        "\n",
        "    # Medindo o RMSE por Fold\n",
        "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
        "    print(\"Fold score (RMSE): {}\".format(score))\n",
        "\n",
        "\n",
        "# Crie a lista de previsões e calcula o erro.\n",
        "oos_y = np.concatenate(oos_y)\n",
        "oos_pred = np.concatenate(oos_pred)\n",
        "score = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\n",
        "print(\"Final Score (RMSE): {}\".format(score))    \n",
        "\n",
        "\n",
        "# Grava a previsão\n",
        "oos_y = pd.DataFrame(oos_y)\n",
        "oos_pred = pd.DataFrame(oos_pred)\n",
        "oosDF = pd.concat( [df, oos_y, oos_pred],axis=1 )\n",
        "oosDF.columns.values[len(oosDF.columns)-2] = 'y'\n",
        "oosDF.columns.values[len(oosDF.columns)-1] = 'y_pred'\n",
        "oosDF.to_csv(filename_write, index=False)\n",
        "#oosDF.head()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold #1\n",
            "Epoch 00226: early stopping\n",
            "Fold score (RMSE): 2.8479552268981934\n",
            "Fold #2\n",
            "Epoch 00094: early stopping\n",
            "Fold score (RMSE): 4.170076847076416\n",
            "Fold #3\n",
            "Epoch 00176: early stopping\n",
            "Fold score (RMSE): 3.556575298309326\n",
            "Fold #4\n",
            "Epoch 00257: early stopping\n",
            "Fold score (RMSE): 4.533331394195557\n",
            "Fold #5\n",
            "Epoch 00157: early stopping\n",
            "Fold score (RMSE): 3.7034265995025635\n",
            "Final Score (RMSE): 3.803931951522827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AbwS2sT-dkqf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Fold score (RMSE): 16.26688003540039\n",
        "Final Score (RMSE): 12.532354354858398\n",
        "    \n",
        "Fold score (RMSE): 3.6836001873016357\n",
        "Final Score (RMSE): 4.236278533935547"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "21g9EiAbdkqr",
        "colab_type": "code",
        "outputId": "11f4e08b-3690-4c29-87a8-b45512ff996c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "oosDF.columns\n",
        "oosDF.head()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>displacement</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>weight</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>year</th>\n",
              "      <th>origin</th>\n",
              "      <th>y</th>\n",
              "      <th>y_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33.0</td>\n",
              "      <td>4</td>\n",
              "      <td>91.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>1795</td>\n",
              "      <td>17.4</td>\n",
              "      <td>76</td>\n",
              "      <td>3</td>\n",
              "      <td>33.0</td>\n",
              "      <td>35.238647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28.0</td>\n",
              "      <td>4</td>\n",
              "      <td>120.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>2625</td>\n",
              "      <td>18.6</td>\n",
              "      <td>82</td>\n",
              "      <td>1</td>\n",
              "      <td>28.0</td>\n",
              "      <td>31.457569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.0</td>\n",
              "      <td>6</td>\n",
              "      <td>232.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2634</td>\n",
              "      <td>13.0</td>\n",
              "      <td>71</td>\n",
              "      <td>1</td>\n",
              "      <td>19.0</td>\n",
              "      <td>17.326378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3940</td>\n",
              "      <td>13.2</td>\n",
              "      <td>76</td>\n",
              "      <td>1</td>\n",
              "      <td>13.0</td>\n",
              "      <td>16.636600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>4237</td>\n",
              "      <td>14.5</td>\n",
              "      <td>73</td>\n",
              "      <td>1</td>\n",
              "      <td>14.0</td>\n",
              "      <td>15.855096</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
              "0  33.0          4          91.0        53.0    1795          17.4    76   \n",
              "1  28.0          4         120.0        79.0    2625          18.6    82   \n",
              "2  19.0          6         232.0       100.0    2634          13.0    71   \n",
              "3  13.0          8         318.0       150.0    3940          13.2    76   \n",
              "4  14.0          8         318.0       150.0    4237          14.5    73   \n",
              "\n",
              "   origin     y     y_pred  \n",
              "0       3  33.0  35.238647  \n",
              "1       1  28.0  31.457569  \n",
              "2       1  19.0  17.326378  \n",
              "3       1  13.0  16.636600  \n",
              "4       1  14.0  15.855096  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "Loltl5SAdkq5",
        "colab_type": "code",
        "outputId": "d20ab04c-41c1-47e4-8495-24dfdf503e02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "type(oos_y), type(oos_pred)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(pandas.core.frame.DataFrame, pandas.core.frame.DataFrame)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "s9Bow8jidkrF",
        "colab_type": "code",
        "outputId": "ce575073-5d98-4d3d-986c-5a62af8f1a36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "oos_pred[0].values[:10]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([35.2386, 31.4576, 17.3264, 16.6366, 15.8551, 29.1262, 27.884 ,\n",
              "       13.1511, 18.686 , 20.0719], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "N3lINFmvdkrL",
        "colab_type": "code",
        "outputId": "a46a9983-0162-4d2f-b43c-b9b4c8016822",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "oos_y[:5]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>33.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0\n",
              "0  33.0\n",
              "1  28.0\n",
              "2  19.0\n",
              "3  13.0\n",
              "4  14.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "Ql-GujsKdkrR",
        "colab_type": "code",
        "outputId": "3847b54b-a72e-4bbf-a923-e9aea45be493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "type(oos_y)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "A4ebMqz6dkrb",
        "colab_type": "code",
        "outputId": "5da45da0-2c0e-4a92-97bb-76468967f827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "oos_pred[0].values[:10]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([35.2386, 31.4576, 17.3264, 16.6366, 15.8551, 29.1262, 27.884 ,\n",
              "       13.1511, 18.686 , 20.0719], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "Z3g7nM9adkrm",
        "colab_type": "code",
        "outputId": "3fead708-79ea-480f-d089-b67ef33fffb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "chart_regression(oos_pred[0].values, oos_y[0].values, sort=True)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFKCAYAAAAwrQetAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVAj/HxGrlDPdZIEJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}