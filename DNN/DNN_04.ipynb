{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNN-04.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladimiralencar/DeepLearning-LANA/blob/master/DNN/DNN_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "jL53yYQ9edWm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Algoritmos de Otimização"
      ]
    },
    {
      "metadata": {
        "id": "n8vL6WzbedWn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<ul>\n",
        "  <li>ADADELTA</li>\n",
        "  <li>ADAGRAD</li>\n",
        "  <li>ADAM</li>\n",
        "  <li>NESTEROVS</li>\n",
        "  <li>RMSPROP</li>\n",
        "  <li>SGD</li>\n",
        "  <li>CONJUGATE GRADIENT</li>\n",
        "  <li>HESSIAN FREE</li>\n",
        "  <li>LBFGS</li>\n",
        "  <li>LINE GRADIENT DESCENT</li>\n",
        "</ul>"
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "4-K5AUS0edWp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Backpropagation\n",
        "\n",
        "Backpropagation é o principal meio pelo qual os pesos de uma rede neural são determinados durante o treinamento. Os programadores frequentemente treinam redes neurais profundas com backpropagation, porque ele escala muito bem quando executado em unidades gráficas de processamento (GPUs). Para entender esse algoritmo para redes neurais, devemos examinar como treiná-lo, bem como como ele processa um padrão.\n",
        "\n",
        "A pós-propagação clássica foi ampliada e modificada para dar origem a vários algoritmos de treinamento diferentes. Vamos discutir os algoritmos de treinamento mais utilizados para as redes neurais. Começamos com backpropagation clássico.\n",
        "\n",
        "Backpropagation é um tipo de descida de gradiente, e muitos livros usam esses dois termos de forma intercambiável. A descida gradual refere-se ao cálculo de um gradiente em cada peso na rede neural para cada elemento de treinamento. Como a rede neural não emitirá o valor esperado para um elemento de treinamento, o gradiente de cada peso lhe dará uma indicação sobre como modificar cada peso para alcançar o resultado esperado. Se a rede neural produzisse exatamente o que era esperado, o gradiente para cada peso seria 0, indicando que nenhuma alteração no peso é necessária.\n",
        "\n",
        "O gradiente é a derivada da função de erro no valor atual do peso. A função de erro mede a distância da saída da rede neural da saída esperada. Na verdade, podemos usar gradient descent, um processo em que o valor do gradiente de cada peso pode alcançar valores ainda mais baixos da função de erro.\n",
        "\n",
        "![Backpropagation](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/DNN/images/backpropagation.png?raw=true \"Backpropagation\")\n",
        "\n",
        "Com respeito à função de erro, o gradiente é essencialmente a derivada parcial de cada peso na rede neural. Cada peso tem um gradiente que é a inclinação da função de erro. Um peso é uma conexão entre dois neurônios. O cálculo do gradiente da função de erro permite que o método de treinamento determine se deve aumentar ou diminuir o peso. Por sua vez, esta determinação irá diminuir o erro da rede neural. O erro é a diferença entre o resultado esperado e a saída real da rede neural. Muitos métodos de treinamento diferentes chamados algoritmos de treinamento de propagação utilizam gradientes. Em todos eles, o sinal do gradiente informa a rede neural as seguintes informações:\n",
        "\n",
        "Gradiente zero     - o peso não está contribuindo para o erro da rede neural. \n",
        "\n",
        "Gradiente negativo - O peso deve ser aumentado para obter um erro menor. \n",
        "\n",
        "Gradiente positivo - O peso deve ser diminuído para obter um erro menor.\n",
        "\n",
        "Como muitos algoritmos dependem do cálculo do gradiente, começaremos com uma análise desse processo."
      ]
    },
    {
      "metadata": {
        "id": "bsPc6bCEedWr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Backpropagation funciona calculando um valor de mudança de peso ($ v_t $) para cada peso ($\\theta$) na rede neural. Esse valor é subtraído de cada peso pela seguinte equação:\n",
        "\n",
        "$ \\theta_t = \\theta_{t-1} - v_t $\n",
        "\n",
        "Este processo é repetido para cada iteração ($t$). Como a mudança de peso é calculada, depende do algoritmo de treinamento. A \"backpropagation\" clássica simplesmente calcula um gradiente ($\\nabla$) para cada peso na rede neural em relação à função de erro ($J$) da rede neural. O gradiente é escalado por uma taxa de aprendizado ($\\eta $).\n",
        "\n",
        "$ v_t = \\eta \\nabla_{\\theta_{t-1}} J(\\theta_{t-1}) $\n",
        "\n",
        "A taxa de aprendizagem é um conceito importante para o treinamento de backpropagation. Definir a taxa de aprendizagem pode ser complexo:\n",
        "\n",
        "* Um valor muito baixo de uma taxa de aprendizado geralmente converge para uma boa solução; No entanto, o processo será muito lento.\n",
        "* Um valor muito alto de uma taxa de aprendizado falhará de forma definitiva ou convergirá para um erro maior do que uma taxa de aprendizado menor.\n",
        "\n",
        "Valores comuns para taxa de aprendizagem são: 0.1, 0.01, 0.001, etc."
      ]
    },
    {
      "metadata": {
        "id": "E5UEIVc9edWs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## O Que é um Gradiente?"
      ]
    },
    {
      "metadata": {
        "id": "Dk3yFE3VedWs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Em primeiro lugar, vamos examinar o gradiente. Essencialmente, o treinamento é uma busca pelo conjunto de pesos que causará a rede neural ter o menor erro para um conjunto de treinamento. Se tivéssemos uma quantidade infinita de recursos computacionais, simplesmente tentaríamos todas as combinações possíveis de pesos para determinar aquele que proporcionasse o menor erro durante o treinamento. Como não temos recursos computacionais ilimitados, temos que usar algum tipo de atalho para evitar a necessidade de examinar todas as possíveis combinações de pesos. Esses métodos de treinamento utilizam técnicas inteligentes para evitar a busca de força bruta de todos os valores de peso. Este tipo de busca exaustiva seria impossível porque mesmo as pequenas redes têm um número infinito de combinações de peso. Considere um gráfico que mostra o erro de uma rede neural para cada peso possível. A figura abaixo é um gráfico que demonstra o erro para um único peso:"
      ]
    },
    {
      "metadata": {
        "id": "NvuwGwijedWu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Erro de um único peso](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/DNN/images/deriv.png?raw=true \"Erro de um único peso\")"
      ]
    },
    {
      "metadata": {
        "id": "hAhKhc3IedWu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Olhando para este gráfico, você pode facilmente ver que o peso ótimo é o local onde a linha possui o menor valor y. O problema é que vemos apenas o erro para o valor atual do peso. Não vemos todo o gráfico porque esse processo exigiria uma busca exaustiva. No entanto, podemos determinar a inclinação da curva de erro em um peso específico. No gráfico acima, vemos a inclinação da curva de erro em 1.5. A linha reta que quase não toca a curva de erro em 1.5 dá a inclinação. Neste caso, a inclinação, ou gradiente, é -0.5622. O declive negativo indica que um aumento no peso diminuirá o erro. O gradiente é a inclinação instantânea da função de erro no peso especificado. A derivada da curva de erro nesse ponto dá o gradiente. Esta linha nos informa a inclinação da função de erro no peso fornecido. Derivadas são um dos conceitos mais fundamentais em cálculo. Uma derivada fornece a inclinação de uma função em um ponto específico. Uma técnica de treinamento e essa inclinação podem dar-lhe as informações para ajustar o peso por um erro menor. "
      ]
    },
    {
      "metadata": {
        "id": "p7LV6Tb2edWv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Momentum Backpropagation\n",
        "\n",
        "Momentum adiciona outro termo ao cálculo de $ v_t $:\n",
        "\n",
        "$ v_t = \\eta \\nabla_{\\theta_{t-1}} J(\\theta_{t-1}) + \\lambda v_{t-1} $\n",
        "\n",
        "Como a taxa de aprendizado, o momentum agrega outro parâmetro de treinamento que reduz o efeito do impulso. A pós-propagação de Momentum tem dois parâmetros de treinamento: taxa de aprendizado ($\\eta$) e momentum ($\\lambda$). Momentum simplesmente adiciona o valor escalado do valor da mudança de peso anterior ($ v_ {t-1} $) para o valor atual da mudança de peso ($ v_t $).\n",
        "\n",
        "Isso tem o efeito de adicionar força adicional atrás de uma direção em que um peso estava se movendo. Isso pode permitir que o peso escape de um mínimo local:\n",
        "\n",
        "![Momentum](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/DNN/images/momentum.png?raw=true   \"Momentum\")\n",
        "\n",
        "Um valor muito comum para o momento é 0.9.\n"
      ]
    },
    {
      "metadata": {
        "id": "o7n6m66qedWx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Batch e Online Backpropagation\n",
        "\n",
        "Quantas vezes os pesos de uma rede neural devem ser atualizados? \n",
        "\n",
        "Backpropagation é um método de treinamento simples que ajusta os pesos da rede neural com seus gradientes calculados. Este método é uma forma de descida do gradiente, uma vez que estamos descendo os gradientes para valores mais baixos. À medida que o programa ajusta esses pesos, a rede neural deve produzir resultados mais desejáveis. O erro global da rede neural deve cair à medida fazemos o treinamento. Existem duas maneiras diferentes de atualizar os pesos.\n",
        "\n",
        "Quantas vezes os pesos de uma rede neural devem ser atualizados? Os gradientes podem ser calculados para um elemento do conjunto de treinamento. Esses gradientes também podem ser resumidos em lotes e os pesos atualizados uma vez por lote.\n",
        "\n",
        "* **Online Training** - Atualiza os pesos com base em gradientes calculados a partir de um único elemento do conjunto de treinamento.\n",
        "* **Batch Training** - Atualiza os pesos com base na soma dos gradientes sobre todos os elementos do conjunto de treinamento.\n",
        "* **Batch Size** - Atualiza os pesos com base na soma de algum tamanho do lote dos elementos do conjunto de treinamento.\n",
        "* **Mini-Batch Training** - O mesmo que o tamanho do lote, mas com um tamanho de lote muito pequeno. Os mini-lotes são muito populares e geralmente estão no intervalo de elementos 32-64.\n",
        "\n",
        "Já mostramos como calcular os gradientes para um elemento de conjunto de treinamento individual. Por exemplo, calculamos os gradientes para um caso em que damos à rede neural uma entrada de [1,0] e esperamos uma saída de [1]. Este resultado é aceitável para um único elemento de treinamento. No entanto, a maioria dos conjuntos de treinamento tem muitos elementos. Portanto, podemos lidar com múltiplos elementos do conjunto de treinamento através de duas abordagens chamadas de treinamento online e em lote. O treinamento online modifica os pesos após cada elemento do conjunto de treinamento. Usando os gradientes obtidos no primeiro elemento do conjunto de treinamento, você calcula e aplica uma alteração aos pesos. O treinamento avança para o próximo elemento do conjunto de treinamento e também calcula uma atualização para a rede neural. Este treinamento continua até você usar todos os elementos do conjunto de treinamento. Neste ponto, uma iteração, ou época, de treinamento completou. O treinamento em lote também utiliza todos os elementos do conjunto de treinamento. No entanto, não atualizamos os pesos. Em vez disso, somamos os gradientes para cada elemento de conjunto de treinamento. Ao final, somamos os gradientes dos elementos do conjunto de treinamento, e podemos atualizar os pesos das redes neurais. Neste ponto, a iteração está completa.\n",
        "\n",
        "Às vezes, podemos definir um tamanho de lote. Por exemplo, você pode ter um tamanho de conjunto de treinamento de 10.000 elementos. Você pode optar por atualizar os pesos da rede neural a cada 1.000 elementos, fazendo com que os pesos da rede neural atualizem dez vezes durante a iteração do treinamento. O treinamento online foi o método original para o backpropagation, mas com o volume de dados cada vez maior, o treinamento em batch ou mini-batch, acaba sendo mais eficiente.\n",
        "\n",
        "Como o tamanho do lote é menor do que o tamanho completo do conjunto de treinamento, pode demorar vários lotes para uma passada completa através do conjunto de treinamento.\n",
        "\n",
        "* **Step/Iteration** - O número de lotes que foram processados.\n",
        "* **Epoch** - O número de vezes que o conjunto de treinamento completo foi processado.\n"
      ]
    },
    {
      "metadata": {
        "id": "_BYObGZ-edWy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Stochastic Gradient Descent\n",
        "\n",
        "O treinamento em lote e online não são as únicas opções de backpropagation. A descida estocástica do gradiente (SGD) é o mais popular dos algoritmos de backpropagation. O SGD pode funcionar tanto em lote como em modo online. A descida estocástica do gradiente online simplesmente seleciona um elemento do conjunto de treinamento aleatoriamente e, em seguida, calcula o gradiente e executa uma atualização de peso. Este processo continua até que o erro atinja um nível aceitável. A escolha de elementos de conjunto de treinamento aleatório geralmente converge para um peso aceitável mais rápido que o looping em todo o conjunto de treinamento para cada iteração. O SGD em lote funciona escolhendo o tamanho de um lote. Para cada iteração, um mini-lote é escolhido selecionando aleatoriamente uma série de elementos de conjunto de treinamento até o tamanho do lote escolhido. Os gradientes do mini-lote são somados apenas como atualização regular do lote de backpropagation. Esta atualização é muito semelhante à atualização regular do lote, exceto que os mini-lotes são escolhidos aleatoriamente sempre que são necessários. As iterações normalmente processam um único lote no SGD. Os lotes geralmente são muito menores que o tamanho completo do conjunto de treinamento. Uma escolha comum para o tamanho do lote é 600.\n",
        "\n",
        "A descida estocástica do gradiente (SGD - Stochastic Gradient Descent) é atualmente um dos algoritmos de treinamento de rede neural mais populares. Funciona de forma muito semelhante ao treinamento em Lote / Mini-Lote, exceto que os lotes são constituídos por um conjunto aleatório de elementos de treinamento.\n",
        "\n",
        "Isso leva a uma convergência muito irregular durante o treinamento:\n",
        "\n",
        "![SGD Error](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/DNN/images/sgd_error.png?raw=true  \"SGD Error\")\n",
        "\n",
        "\n",
        "Como a rede neural é treinada em uma amostra aleatória do conjunto de treinamento completo de cada vez, o erro não faz uma transição suave para baixo. No entanto, o erro geralmente diminui.\n",
        "\n",
        "As vantagens do SGD incluem:\n",
        "\n",
        "* Computacionalmente eficiente. Mesmo com um conjunto de treinamento muito grande, cada etapa de treinamento pode ser relativamente rápida.\n",
        "* Diminui a superposição focalizando apenas uma parte do treinamento definido em cada etapa."
      ]
    },
    {
      "metadata": {
        "id": "xWqtQ5U3edWz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Outros Algoritmos\n",
        "\n",
        "A escolha do algoritmo de otimização para seu modelo de aprendizagem profunda pode significar a diferença entre bons resultados em minutos, horas e dias.\n",
        "\n",
        "Um problema com algoritmos simples de treinamento retropropagação é que eles são altamente sensíveis à taxa de aprendizagem e momentum (impulso). Isso é difícil porque:\n",
        "\n",
        "* A taxa de aprendizado deve ser ajustada para um nível pequeno o suficiente para treinar uma rede neural precisa.\n",
        "* Momentum deve ser grande o suficiente para superar mínimos locais, mas pequeno o suficiente para não desestabilizar o treinamento.\n",
        "* Uma taxa / momento de aprendizado único geralmente não é bom o suficiente para todo o processo de treinamento. Muitas vezes, é útil reduzir automaticamente a taxa de aprendizagem à medida que o treinamento avança.\n",
        "* Todos os pesos compartilham uma única taxa / impulso de aprendizado.\n",
        "\n",
        "Outras técnicas de treinamento:\n",
        "\n",
        "* **Resilient Propagation** - Usa apenas a magnitude do gradiente e permite que cada neurônio aprenda a sua própria taxa. Não há necessidade de taxa / impulso de aprendizagem. No entanto, só funciona no modo de lote completo.\n",
        "* **Nesterov accelerated gradient** - Ajuda a mitigar o risco de escolher um mini-lote ruim.\n",
        "* **Adagrad** - Permite uma taxa de aprendizado e um conceito de impulso por deterioração automaticamente.\n",
        "* **Adadelta** - Extensão de Adagrad que busca reduzir sua taxa de aprendizado agressiva e monotonicamente decrescente.\n",
        "* **Non-Gradient Methods** - Os métodos não gradientes podem * às vezes * ser úteis, embora raramente superem os métodos de backpropagação baseados em gradientes. "
      ]
    },
    {
      "metadata": {
        "id": "OOi9zgoLedWz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ADAM Update\n",
        "\n",
        "O algoritmo de otimização Adam é uma extensão da descida estocástica do gradiente que recentemente viu adoção mais ampla para aplicações de aprendizado profundo em visão computacional e processamento de linguagem natural.\n",
        "\n",
        "Adam é um algoritmo de otimização que pode ser usado em vez do procedimento clássico de descida estocástica do gradiente para atualizar os pesos da rede iterativos com base em dados de treinamento.\n",
        "\n",
        "Adam foi apresentado por Diederik Kingma da OpenAI e Jimmy Ba da Universidade de Toronto em seu artigo de 2015 intitulado \"Adam: um método para otimização estocástica\" (link na seção de links úteis). O nome Adam é derivado da estimativa do momento adaptativo.\n",
        "\n",
        "Ao introduzir o algoritmo, os autores apresentam os benefícios atrativos em problemas de otimização não convexos, da seguinte forma:\n",
        "\n",
        "* Simples para implementar.\n",
        "* Computacionalmente eficiente.\n",
        "* Poucos requisitos de memória.\n",
        "* Invariante para a escala diagonal dos gradientes.\n",
        "* Bem adequado para problemas que são grandes em termos de dados e / ou parâmetros.\n",
        "* Adequado para objetivos não estacionários.\n",
        "* Adequado para problemas com gradientes muito ruidosos ou escassos.\n",
        "* Os hiperparâmetros têm interpretação intuitiva e geralmente requerem pouca afinação.\n",
        "\n",
        "Adam é diferente da descida estocástica do gradiente clássica. O SGD mantém uma taxa de aprendizagem única (denominada alfa) para todas as atualizações de peso e a taxa de aprendizado não muda durante o treinamento. \n",
        "\n",
        "Com Adam, uma taxa de aprendizagem é mantida para cada peso da rede (parâmetro) e adaptada separadamente à medida que a aprendizagem se desenrola.\n",
        "\n",
        "Os autores descrevem Adam como combinando as vantagens de duas outras extensões do SGD. Especificamente:\n",
        "\n",
        "* Algoritmo de Gradiente Adaptativo (AdaGrad) que mantém uma taxa de aprendizado por parâmetro que melhora o desempenho em problemas com gradientes escassos (por exemplo, linguagem natural e problemas de visão computacional).\n",
        "\n",
        "* Root Mean Square Propagation (RMSProp) que também mantém as taxas de aprendizagem por parâmetro que são adaptadas com base na média das magnitudes recentes dos gradientes para o peso (por exemplo, com que rapidez ele está mudando). Isso significa que o algoritmo funciona bem em problemas online e não estacionários (por exemplo, dados com muitos ruídos).\n",
        "\n",
        "Adam contém os benefícios do AdaGrad e RMSProp. Em vez de adaptar as taxas de aprendizado de parâmetros com base no primeiro momento médio (a média) como em RMSProp, Adam também faz uso da média dos segundos momentos dos gradientes (a variância não centrada). Especificamente, o algoritmo calcula uma média móvel exponencial do gradiente e do gradiente quadrado e os parâmetros beta1 e beta2 controlam as taxas de decaimento dessas médias móveis. O valor inicial das médias móveis e os valores beta1 e beta2 próximos de 1,0 (recomendado) resultam em uma tendência de estimativa do momento para zero. \n",
        "\n",
        "![Adam](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/DNN/images/adam.png?raw=true  \"Adam\")\n",
        "\n",
        "ADAM é um dos primeiros algoritmos de treinamento que você deve tentar. É muito eficaz. Kingma e Ba (2014) introduziram a regra de atualização de Adam que deriva seu nome das estimativas de momento adaptativo que ele usa. Adam estima a média e a variância para determinar as correções de peso. Adam começa com uma média exponencialmente descrescente de gradientes passados (m):\n",
        "\n",
        "$ m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t $\n",
        "\n",
        "Esta média atinge um objetivo semelhante como atualização de momentum clássico. No entanto, seu valor é calculado automaticamente com base no gradiente atual ($g_t$). A regra de atualização calcula o segundo momentum ($v_t$):\n",
        "\n",
        "$ v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2 $\n",
        "\n",
        "Os valores $m_t$ e $v_t$ são estimativas do primeiro momentum (a média) e do segundo momentum (a variância não centralizada) dos gradientes, respectivamente. No entanto, eles terão um forte viés para zero nos ciclos iniciais de treinamento. O viés do primeiro momento é corrigido da seguinte forma.\n",
        "\n",
        "$ \\hat{m}_t = \\frac{m_t}{1-\\beta^t_1} $\n",
        "\n",
        "Da mesma forma, o segundo momento também é corrigido:\n",
        "\n",
        "$ \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t} $\n",
        "\n",
        "Essas estimativas de primeiro e segundo tempo corrigidas por polarização são aplicadas na regra final de atualização de Adam, da seguinte forma:\n",
        "\n",
        "$ \\theta_t = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}_t}+\\eta} \\hat{m}_t $\n",
        "\n",
        "Adam é muito tolerante com a taxa de aprendizado inicial (η) e outros parâmetros de treinamento. Kingma e Ba (2014) propõem valores padrão de 0,9 para $ \\ beta_1 $, 0,999 para $ \\ beta_2 $ e 10-8 para $ \\ eta $."
      ]
    },
    {
      "metadata": {
        "id": "_KVCajY5edW1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Métodos Comparados\n",
        "\n",
        "As imagens abaixo mostram como cada um desses algoritmos realiza o treinamento ([Referência](http://sebastianruder.com/optimizing-gradient-descent/index.html#visualizationofalgorithms) ):\n",
        "\n",
        "![Training Techniques](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/DNN/images/contours_evaluation_optimizers.gif?raw=true   \"Training Techniques\")\n",
        "\n",
        "![Training Techniques](https://github.com/vladimiralencar/DeepLearning-LANA/blob/master/DNN/images/saddle_point_evaluation_optimizers.gif?raw=true  \"Training Techniques\")"
      ]
    },
    {
      "metadata": {
        "id": "eTFFaWlpedW2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Qual Otimizador Usar?\n",
        "\n",
        "Então, qual otimizador você deve usar agora? Se os dados de entrada forem escassos (esparsos), provavelmente você obterá os melhores resultados utilizando um dos métodos adaptativos de taxa de aprendizagem. Um benefício adicional é que você não precisará ajustar a taxa de aprendizado, mas provavelmente consegue os melhores resultados com o valor padrão.\n",
        "\n",
        "O RMSProp é uma extensão do Adagrad que lida com suas taxas de aprendizado que diminuem radicalmente. É idêntico ao Adadelta, exceto que o Adadelta usa o RMS de atualizações de parâmetros na regra de atualização. Adam, finalmente, adiciona correção de polarização e impulso para RMSProp. Na verdade, RMSprop, Adadelta e Adam são algoritmos muito parecidos que funcionam em circunstâncias semelhantes. Kingma et al. [15] mostram que sua correção de bias ajuda Adam a superar ligeiramente o RMSprop no final da otimização à medida que os gradientes tornam-se mais dispersos. Na medida em que, Adam pode ser a melhor escolha geral.\n",
        "\n",
        "Curiosamente, muitos artigos recentes usam SGD sem momentum e uma taxa de aprendizagem simples. Como foi mostrado, o SGD normalmente consegue encontrar um mínimo, mas pode demorar significativamente mais do que com alguns otimizadores, é muito mais dependente de uma programação robusta de inicialização e pode ficar preso mínimos locais. Consequentemente, se você se preocupa com a convergência rápida no treinamento uma rede neural profunda ou complexa, você deve escolher um dos métodos de taxa de aprendizagem adaptativa."
      ]
    },
    {
      "metadata": {
        "id": "JtM8Mgj8edW3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Especificando a Regra de Atualização no Tensorflow/Keras\n",
        "\n",
        "TensorFlow permite que a regra de atualização seja definida como uma das seguintes:\n",
        "\n",
        "* Adagrad\n",
        "* **Adam**\n",
        "* Ftrl\n",
        "* Momentum\n",
        "* RMSProp\n",
        "* **SGD**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yKo7lQjtedW5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Funções Auxiliares\n",
        "\n",
        "É uma boa prática criar suas próprias funções de limpeza e transformação dos dados que serão processados pelo modelo de rede neural. Use esses exemplos como referência."
      ]
    },
    {
      "metadata": {
        "id": "aQpuGP_uedW8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Encoding dos valores de texto para variáveis nominais\n",
        "def encode_text_dummy(df, name):\n",
        "    dummies = pd.get_dummies(df[name])\n",
        "    for x in dummies.columns:\n",
        "        dummy_name = \"{}-{}\".format(name, x)\n",
        "        df[dummy_name] = dummies[x]\n",
        "    df.drop(name, axis=1, inplace=True)\n",
        "\n",
        "\n",
        "# Encoding dos valores de texto para uma única variável dummy. As novas colunas (que não substituem o antigo) terão 1\n",
        "# em todos os locais onde a coluna original (nome) corresponde a cada um dos valores-alvo. Uma coluna é adicionada para\n",
        "# cada valor alvo.\n",
        "def encode_text_single_dummy(df, name, target_values):\n",
        "    for tv in target_values:\n",
        "        l = list(df[name].astype(str))\n",
        "        l = [1 if str(x) == str(tv) else 0 for x in l]\n",
        "        name2 = \"{}-{}\".format(name, tv)\n",
        "        df[name2] = l\n",
        "\n",
        "\n",
        "# Encoding dos valores de texto para índices (ou seja, [1], [2], [3] para vermelho, verde, azul por exemplo).\n",
        "def encode_text_index(df, name):\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    df[name] = le.fit_transform(df[name])\n",
        "    return le.classes_\n",
        "\n",
        "\n",
        "# Normalização Z-score\n",
        "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
        "    if mean is None:\n",
        "        mean = df[name].mean()\n",
        "\n",
        "    if sd is None:\n",
        "        sd = df[name].std()\n",
        "\n",
        "    df[name] = (df[name] - mean) / sd\n",
        "\n",
        "\n",
        "# Converte todos os valores faltantes na coluna especificada para a mediana\n",
        "def missing_median(df, name):\n",
        "    med = df[name].median()\n",
        "    df[name] = df[name].fillna(med)\n",
        "\n",
        "\n",
        "# Converte todos os valores faltantes na coluna especificada para o padrão\n",
        "def missing_default(df, name, default_value):\n",
        "    df[name] = df[name].fillna(default_value)\n",
        "\n",
        "\n",
        "# Converte um dataframe Pandas para as entradas x, y que o TensorFlow precisa\n",
        "def to_xy(df, target):\n",
        "    result = []\n",
        "    for x in df.columns:\n",
        "        if x != target:\n",
        "            result.append(x)\n",
        "    # Descobre o tipo da coluna de destino. \n",
        "    target_type = df[target].dtypes\n",
        "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
        "    # Encoding para int. TensorFlow gosta de 32 bits.\n",
        "    if target_type in (np.int64, np.int32):\n",
        "        # Classificação\n",
        "        dummies = pd.get_dummies(df[target])\n",
        "        return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
        "    else:\n",
        "        # Regressão\n",
        "        return df.as_matrix(result).astype(np.float32), df.as_matrix([target]).astype(np.float32)\n",
        "\n",
        "# String de tempo bem formatado\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
        "\n",
        "\n",
        "# Chart de Regressão\n",
        "def chart_regression(pred,y,sort=True):\n",
        "    t = pd.DataFrame({'pred' : pred, 'y' : y.flatten()})\n",
        "    if sort:\n",
        "        t.sort_values(by=['y'],inplace=True)\n",
        "    a = plt.plot(t['y'].tolist(),label='expected')\n",
        "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
        "    plt.ylabel('output')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Remove todas as linhas onde a coluna especificada em +/- desvios padrão\n",
        "def remove_outliers(df, name, sd):\n",
        "    drop_rows = df.index[(np.abs(df[name] - df[name].mean()) >= (sd * df[name].std()))]\n",
        "    df.drop(drop_rows, axis=0, inplace=True)\n",
        "\n",
        "\n",
        "# Normalização Range\n",
        "def encode_numeric_range(df, name, normalized_low=-1, normalized_high=1, data_low=None, data_high=None):\n",
        "    if data_low is None:\n",
        "        data_low = min(df[name])\n",
        "        data_high = max(df[name])\n",
        "\n",
        "    df[name] = ((df[name] - data_low) / (data_high - data_low)) * (normalized_high - normalized_low) + normalized_low"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ObND4XhHedXA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Definindo o Algoritmo de Otimização"
      ]
    },
    {
      "metadata": {
        "id": "p5p0372XC3oM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Leitura do dataset"
      ]
    },
    {
      "metadata": {
        "id": "5_YlphwFCz9s",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "3d5b9aa1-18d5-41eb-b111-ebe01aba9c07"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir data\n",
        "!cp *.csv data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cc7db81f-edcf-4977-b555-daeecbc40ae5\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-cc7db81f-edcf-4977-b555-daeecbc40ae5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving auto-mpg-saida.csv to auto-mpg-saida (1).csv\n",
            "Saving iris.csv to iris (1).csv\n",
            "Saving auto-mpg.csv to auto-mpg (1).csv\n",
            "Saving wcbreast_wdbc.csv to wcbreast_wdbc (1).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tege_GkkedXB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://keras.io/optimizers/"
      ]
    },
    {
      "metadata": {
        "id": "flJjSI9vedXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4c34d20e-de1f-4ef5-eb54-d4aa86f120b1"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib.pyplot import figure, show\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from scipy.stats import zscore\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "path = \"./data/\"\n",
        "preprocess = False\n",
        "\n",
        "filename_read = os.path.join(path,\"auto-mpg.csv\")\n",
        "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
        "\n",
        "# Vetor de features\n",
        "missing_median(df, 'horsepower')\n",
        "encode_text_dummy(df, 'origin')\n",
        "df.drop('name',1,inplace=True)\n",
        "\n",
        "preprocess = False\n",
        "if preprocess:\n",
        "    encode_numeric_zscore(df, 'horsepower')\n",
        "    encode_numeric_zscore(df, 'weight')\n",
        "    encode_numeric_zscore(df, 'cylinders')\n",
        "    encode_numeric_zscore(df, 'displacement')\n",
        "    encode_numeric_zscore(df, 'acceleration')\n",
        "\n",
        "# Dados de treino\n",
        "x,y = to_xy(df,'mpg')\n",
        "\n",
        "# Split em treino/teste\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
        "\n",
        "# Rede Neural\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_dim = x.shape[1], kernel_initializer = 'normal', activation = 'relu'))\n",
        "model.add(Dense(1, kernel_initializer = 'normal'))\n",
        "model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
        "monitor = EarlyStopping(monitor = 'val_loss', min_delta = 1e-3, patience = 5, verbose = 1, mode = 'auto')\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 10)                100       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 111\n",
            "Trainable params: 111\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8kqe0KVZCQ40",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0512b07f-d988-4489-e0fc-0e1ab2eb6acc"
      },
      "cell_type": "code",
      "source": [
        "# treinamento do modelo\n",
        "model.fit(x, y, validation_data = (x_test, y_test), callbacks = [monitor], verbose = 0, epochs = 1000)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 00223: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7f8ffa3ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "yul8y179CXHs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "879fb759-d72a-4961-b10f-18cccdd957b6"
      },
      "cell_type": "code",
      "source": [
        "# Previsões\n",
        "pred = model.predict(x_test)\n",
        "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
        "print(\"Score (RMSE): {}\".format(score))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score (RMSE): 2.7260096073150635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1UygEGlQCZA1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "66125268-87be-4bba-e469-0a20b012ba76"
      },
      "cell_type": "code",
      "source": [
        "# Plot \n",
        "chart_regression(pred.flatten(),y_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "3dL-O6hEedXK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# sem normalização\n",
        "Epoch 00090: early stopping\n",
        "Score (RMSE): 2.742065191268921\n",
        "    \n",
        "Epoch 00191: early stopping\n",
        "Score (RMSE): 2.724492311477661\n",
        "    \n",
        "# com normalização\n",
        "Epoch 00134: early stopping\n",
        "Score (RMSE): 3.280609130859375\n",
        "    \n",
        "Epoch 00157: early stopping\n",
        "Score (RMSE): 3.239733934402466"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "K-ToqoGKedXO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Parâmetros Importantes\n",
        "\n",
        "* **learning_rate** - Quão rápido o otimizador tentará treinar a rede neural. Muito alto não conseguirá treinar. Muito baixo irá treinar muito devagar.\n",
        "* **momentum** - Somente usado com o otimizador de momentum. Quanto da direção de mudança de peso anterior deve ser usada na etapa atual.<br>\n",
        "Bom para combater mínimos locais\n",
        "* **every_n_steps** - How often should the validation set be evaluated.\n",
        "* **early_stopping_rounds** - Com que frequência o conjunto de validação deve ser avaliado."
      ]
    },
    {
      "metadata": {
        "id": "DcUKoJ7eedXP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Momentum e taxa de aprendizagem contribuem para o sucesso do treinamento, mas na verdade não fazem parte da rede neural. Uma vez que o treinamento está completo, os pesos treinados permanecem e não utilizam momentum ou a taxa de aprendizado. Eles são essencialmente parte do andaime temporário que cria uma rede neural treinada. Escolher o momentum e a taxa de aprendizagem corretos pode afetar a eficácia do seu treinamento. A taxa de aprendizagem afeta a velocidade na qual sua rede neural treina. Diminuir a taxa de aprendizagem torna o treinamento mais meticuloso. As taxas de aprendizagem superiores podem ignorar as configurações de peso ótimas. Uma taxa de treinamento menor sempre produzirá melhores resultados. No entanto, reduzir a taxa de treinamento pode aumentar consideravelmente o tempo de execução. Reduzir a taxa de aprendizagem à medida que a rede treina pode ser uma técnica eficaz. Você pode usar o momentum para combater mínimos locais. Se você encontrar a rede neural estagnada, um maior valor de momentum pode empurrar o treinamento após o mínimo local que encontrou. Em última análise, escolher bons valores para o momentum e taxa de aprendizagem é um processo de tentativa e erro. Você pode variar tanto quanto o treinamento avança. Momentum é geralmente definido como 0.9 e a taxa de aprendizado em 0.1 ou inferior."
      ]
    },
    {
      "metadata": {
        "id": "9EWcSe7gedXQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Cálculo do Erro da Rede"
      ]
    },
    {
      "metadata": {
        "id": "Txfp4VsHedXR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "O cálculo do erro da rede é a diferença entre o valor previsto gerado pelo algoritmo e o valor observado usado durante o treinamento. A definição do erro da rede, é parte crucial do treinamento com Backpropagation. Existem algumas formas de calcular o erro da rede:\n",
        "\n",
        "* Função de Erro quadrática (Também conhecida como Mean Square Error)\n",
        "* Cross Entropy Error Function\n",
        "\n",
        "No próximo item de aprendizagem você encontra detalhes sobre essas medidas de erro!"
      ]
    },
    {
      "metadata": {
        "id": "z826k5EkedXS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Entropia Cruzada"
      ]
    },
    {
      "metadata": {
        "id": "nzsJ0ZQuEpXe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A entropia cruzada indica a distância entre o que a rede acredita que essa distribuição deve ser e o que realmente deveria ser.\n",
        "\n",
        "Ela tende a permitir que erros alterem pesos mesmo quando houvernós saturados (o que significa que suas derivadas são próximas de 0). \n",
        "\n",
        "A  entropia cruzada  é  comumente  usada  para  quantificar  a  diferença  entre  duas distribuições de probabilidade.\n",
        "\n",
        "**Entropia Cruzada:** \n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "H(p,q)   =-\\sum_{x} p(x) \\: log\\: q(x)  \n",
        "\\end{align}\n",
        "$"
      ]
    },
    {
      "metadata": {
        "id": "FmjVcED3edXU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ce5692c-ebe3-490d-a184-10a6434ab9b3"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(suppress=True, precision=3)\n",
        "\n",
        "p_observado = np.array([0., 1., 0.])\n",
        "q_previsto = np.array([0.228, 0.619, 0.153])\n",
        "\n",
        "h = - np.sum(p_observado  * np.log(q_previsto))\n",
        "\n",
        "print(h)               "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.47965000629754095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lpsV1YKoQ6ff",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ae64f205-df30-4c96-8acc-21e8c0743249"
      },
      "cell_type": "code",
      "source": [
        "print(q_previsto)\n",
        "print(np.argmax(q_previsto))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.228 0.619 0.153]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kxYdCN3MRRWB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac547004-ef1a-4567-96fb-a9eef2d28545"
      },
      "cell_type": "code",
      "source": [
        "q_previsto = np.array([0.228, 0.619, 0.153, 0.8])\n",
        "print(np.argmax(q_previsto))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k5vyWlIYSAnJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Regularização L1 e L2\n",
        "\n",
        "\n",
        "A regularização L1 e L2 basicamente penalizam os coeficientes, mas ambas possuem diferentes propriedades e são usadas de diferentes maneiras.\n",
        "A magnitude dos coeficentes é penalizada e os erros são minimizados entre os valores previstos e os valores observados.\n",
        "\n",
        "\n",
        "A penalidade de peso é uma forma padrão de regularização, amplamente utilizada na formação de outros tipos de modelo. Baseia-se fortemente na suposição implícita de que um modelo com pesos pequenos é de alguma forma mais simples do que uma rede com grandespesos. \n",
        "\n",
        "As penalidades tentam manter os pesos pequenos ou inexistentes (zero) a menos que existam  grandes  gradientes  para  neutralizá-lo,  o  que  torna  os  modelos  também  mais interpretáveis. Um nome alternativo na literatura para ponderações de peso é \"queda de peso\" (weight decay), uma vez que força os pesos a diminuir em direção a zero.\n",
        "\n",
        "### Regularização L2\n",
        "\n",
        "Penaliza o valor quadrado do peso (o que também explica o \"2\" do nome). \n",
        "Tende a conduzir todos os pesos para valores menores.\n",
        "\n",
        "### Regularização L1\n",
        "Penaliza o valor absoluto do peso. T\n",
        "ende a conduzir alguns pesos para exatamente zero (introduzindo  sparsity  no  modelo),  enquanto  permite  que  alguns  pesos  sejam  grandes.\n"
      ]
    }
  ]
}