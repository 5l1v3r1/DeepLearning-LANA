{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN03WordEmbedding.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladimiralencar/DeepLearning-LANA/blob/master/RNN/RNN03WordEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "HbZ21wOGumIX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Word Embedding\n",
        "\n",
        "Em poucas palavras, Word Embedding transforma texto em números. Essa transformação é necessária porque muitos algoritmos de aprendizado de máquina (incluindo redes neurais profundas) exigem que sua entrada seja vetores de valores contínuos; Eles simplesmente não trabalharão em strings de texto simples.\n",
        "\n",
        "Assim, uma técnica de modelagem de linguagem natural, como o Word Embedding, é usada para mapear palavras ou frases de um vocabulário para um vetor correspondente de números inteiros.\n",
        "Além de ser acessível ao processamento por algoritmos de ML, essa representação vetorial possui duas propriedades importantes e vantajosas:\n",
        "\n",
        "* Dimensionality Reduction - é uma representação mais eficiente\n",
        "* Contextual Similarity - é uma representação mais expressiva\n",
        "\n",
        "Se você está familiarizado com a abordagem do Bag of Words (muito usado em Text Mining), você saberá que muitas vezes o resultado é um conjunto de vetores enormes, muito esparsos, onde a dimensionalidade dos vetores que representam cada documento é igual ao tamanho do vocabulário suportado.\n",
        "Word Embedding visa criar uma representação vetorial com um espaço dimensional muito menor.\n",
        "\n",
        "Word Embedding é usado para análise semântica, para extrair significado do texto a fim de permitir a compreensão da linguagem natural. \n",
        "Para que um modelo de linguagem seja capaz de prever o significado do texto, ele precisa estar ciente da similaridade contextual das palavras. Por exemplo, que tendemos a encontrar palavras de frutas (como maçã ou laranja) em frases onde elas são cultivadas, colhidas, comidas e bebidas, mas não esperamos encontrar esses mesmos conceitos tão perto de, digamos, a palavra avião.\n",
        "\n",
        "Os vetores criados pela Word Embedding preservam essas semelhanças, de modo que as palavras que ocorrem regularmente nas proximidades do texto também estarão próximas do espaço vetorial. \n",
        "\n",
        "Portanto, \"O que é Word Embedding?\" é um meio de construir uma representação vetorial de baixa dimensionalidade a partir do corpus de texto, que preserva a semelhança contextual das palavras."
      ]
    },
    {
      "metadata": {
        "id": "0XhDYPQiumIZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classificação de Sequências em Texto com LSTMs"
      ]
    },
    {
      "metadata": {
        "id": "SSzkkhriumIb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A classificação de sequências é um problema de modelagem preditiva em que você possui algumas sequências de dados ao longo do espaço ou do tempo e a tarefa é prever uma categoria para a sequência. O que torna esse problema difícil é que as sequências podem variar em comprimento, ser compostas por um vocabulário muito grande de símbolos de entrada e pode exigir que o modelo aprenda o contexto ou dependências de longo prazo entre símbolos na sequência de entrada. Vejamos como desenvolver um modelo de rede neural recorrente LSTM para problemas de classificação de sequência em Python usando o framework Keras."
      ]
    },
    {
      "metadata": {
        "id": "eaAPU93bumIc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "O problema que usaremos para demonstrar a aprendizagem de sequências é a classificação do sentimento dos reviews de filmes do IMDB. Podemos rapidamente desenvolver um pequeno modelo LSTM para o problema IMDB e obter uma boa precisão. \n",
        "\n",
        "http://www.imdb.com/"
      ]
    },
    {
      "metadata": {
        "id": "h0MsBzI0umId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "891643c2-cca7-4866-bfb6-5cab671f3f36"
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "6CMpSwiRumIn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Random seed\n",
        "numpy.random.seed(7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y368A1CuumIr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://keras.io/datasets/"
      ]
    },
    {
      "metadata": {
        "id": "0rMZmmkcumIs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4820e456-8461-40cc-cf07-6f6ed3ce2112"
      },
      "cell_type": "code",
      "source": [
        "# Carrega o conjunto de dados, mas mantém apenas as palavras n superiores, zerando o resto\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = top_words)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PDPYwfLaumIx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "42fa572b-4194-440f-aa38-15424077281e"
      },
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32])\n",
            " list([1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
            " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 2, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n",
            " ...\n",
            " list([1, 11, 6, 230, 245, 2, 9, 6, 1225, 446, 2, 45, 2174, 84, 2, 4007, 21, 4, 912, 84, 2, 325, 725, 134, 2, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 2, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 2, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 2, 2, 5, 4241, 18, 4, 2, 2, 250, 11, 1818, 2, 4, 4217, 2, 747, 1115, 372, 1890, 1006, 541, 2, 7, 4, 59, 2, 4, 3586, 2])\n",
            " list([1, 1446, 2, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 2, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23])\n",
            " list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 2, 9, 24, 6, 78, 1099, 17, 2345, 2, 21, 27, 2, 2, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 2, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 2, 8, 2197, 2, 2, 544, 5, 383, 1271, 848, 1468, 2, 497, 2, 8, 1597, 2, 2, 21, 60, 27, 239, 9, 43, 2, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q537jd3YumI5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# converte para texto\n",
        "def conv_to_text(int_sentence):\n",
        "    word_index = imdb.get_word_index()\n",
        "    index_word = {v:k for k,v in word_index.items()}\n",
        "    text = ' '.join(index_word.get(w) for w in int_sentence)\n",
        "    return text\n",
        "def get_sentiment(int_sentiment):\n",
        "    return \"Positive\" if int_sentiment == 1 else \"Negative\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c4qN0GzoumI8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "9c1f849d-35eb-4b14-9840-8d4a813c9dab"
      },
      "cell_type": "code",
      "source": [
        "text = conv_to_text(X_train[0])\n",
        "print(text, \" ===> \",  get_sentiment(y_train[0]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 0us/step\n",
            "the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of 70s and with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other and in of seen over and for anyone of and br show's to whether from than out themselves history he name half some br of and odd was two most of mean for 1 any an boat she he should is thought and but of script you not while history he heart to real at and but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with and film want an  ===>  Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qD6IgnV7umJC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9917d59c-9a87-4350-9276-83342f087959"
      },
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "ca2LLSauumJG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "36f5f121-cb35-4b38-8725-5cf334d70d6d"
      },
      "cell_type": "code",
      "source": [
        "text = conv_to_text(X_train[len(X_train)-20])\n",
        "print(text, \" ===> \",  get_sentiment(y_train[len(X_train)-20]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the movie live one equal as real one will so shot tie gives you to that horror in we for series don't its br of before than and in 1 that not of dr it her and zombie cops to waiting film is quite br theme and death restaurant louise it and film its br any for if an movies and to expect five as it so and buy probably or louise believe all and and of conflicts scenes know needed eye case sympathy when kind bit of see shows like trying better had much it tedious pass like it is its ryan deeply and parties action one will is him seed not all with is france going  ===>  Negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pewdnmIDumJL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f20cc27-4fc6-4394-bb14-494b9e2ce489"
      },
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 0 ... 0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eaDyU5lBumJQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Em seguida, precisamos truncar e ajustar as sequências de entrada para que elas tenham o mesmo comprimento \n",
        "# para modelagem. O modelo aprenderá que os valores zero não possuem informações, de modo que as sequências\n",
        "# não tem o mesmo comprimento em termos de conteúdo, mas os vetores de mesmo comprimento são necessários para \n",
        "# executar a computação no Keras.\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen = max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen = max_review_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OnJK-FRUumJT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "879af76d-ea5e-4887-96a5-68985ad45853"
      },
      "cell_type": "code",
      "source": [
        "len(X_train[0])\n",
        "X_train[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    1,   14,   22,   16,\n",
              "         43,  530,  973, 1622, 1385,   65,  458, 4468,   66, 3941,    4,\n",
              "        173,   36,  256,    5,   25,  100,   43,  838,  112,   50,  670,\n",
              "          2,    9,   35,  480,  284,    5,  150,    4,  172,  112,  167,\n",
              "          2,  336,  385,   39,    4,  172, 4536, 1111,   17,  546,   38,\n",
              "         13,  447,    4,  192,   50,   16,    6,  147, 2025,   19,   14,\n",
              "         22,    4, 1920, 4613,  469,    4,   22,   71,   87,   12,   16,\n",
              "         43,  530,   38,   76,   15,   13, 1247,    4,   22,   17,  515,\n",
              "         17,   12,   16,  626,   18,    2,    5,   62,  386,   12,    8,\n",
              "        316,    8,  106,    5,    4, 2223,    2,   16,  480,   66, 3785,\n",
              "         33,    4,  130,   12,   16,   38,  619,    5,   25,  124,   51,\n",
              "         36,  135,   48,   25, 1415,   33,    6,   22,   12,  215,   28,\n",
              "         77,   52,    5,   14,  407,   16,   82,    2,    8,    4,  107,\n",
              "        117,    2,   15,  256,    4,    2,    7, 3766,    5,  723,   36,\n",
              "         71,   43,  530,  476,   26,  400,  317,   46,    7,    4,    2,\n",
              "       1029,   13,  104,   88,    4,  381,   15,  297,   98,   32, 2071,\n",
              "         56,   26,  141,    6,  194,    2,   18,    4,  226,   22,   21,\n",
              "        134,  476,   26,  480,    5,  144,   30,    2,   18,   51,   36,\n",
              "         28,  224,   92,   25,  104,    4,  226,   65,   16,   38, 1334,\n",
              "         88,   12,   16,  283,    5,   16, 4472,  113,  103,   32,   15,\n",
              "         16,    2,   19,  178,   32], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "vpVPpxkLumJa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://keras.io/layers/embeddings/"
      ]
    },
    {
      "metadata": {
        "id": "fuI56JOiumJc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "f2721afd-71d2-4ec4-b283-c4350dea66e9"
      },
      "cell_type": "code",
      "source": [
        "# Cria o modelo\n",
        "# Agora podemos definir, compilar e ajustar nosso modelo LSTM. A primeira camada é a camada Embedding, \n",
        "# que usa 32 vetores para representar cada palavra. A próxima camada é a camada LSTM com 100\n",
        "# unidades de memória (neurônios inteligentes). Finalmente, como este é um problema de classificação, \n",
        "# usamos uma camada Densa com um único neurônio de saída e ativação sigmóide, que vai gerar previsões igual a 0 ou 1.\n",
        "# Por se tratar de um problema de classificação binária, usamos como a função de perda \n",
        "#(crossentropy binário em Keras). O ADAM conhecido\n",
        "# O algoritmo de otimização ADAM é usado. O modelo é treinado por apenas 3 épocas pois ele rapidamente \n",
        "# pode apresentar overfitting. \n",
        "# Um grande tamanho do lote de 64 avaliações é usado para espaçar as atualizações de peso.\n",
        "\n",
        "embedding_vector_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vector_length, input_length = max_review_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs = 3, batch_size = 64)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 390s 16ms/step - loss: 0.4230 - acc: 0.8006\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 387s 15ms/step - loss: 0.3066 - acc: 0.8725\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 389s 16ms/step - loss: 0.2488 - acc: 0.9024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbce8f1eb00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "lqg6aTi7umJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "860c7492-a39d-4e89-cb21-d1a2f951722c"
      },
      "cell_type": "code",
      "source": [
        "# Avaliação final do modelo\n",
        "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
        "print(\"Acurácia: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acurácia: 86.57%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b9QJ3AqZumJo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Você pode ver que este simples modelo LSTM com pouco ajuste atinge resultados próximos do estado da arte para o problema de classificação de texto do IMDB. Importante, este é um modelo que você pode usar para aplicar redes LSTM\n",
        "para seus próprios problemas de classificação de sequência. Agora, vejamos algumas extensões deste simples modelo."
      ]
    },
    {
      "metadata": {
        "id": "kS0q6dM5umJq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Aplicando Regularização com Dropout em LSTMs - Versão 1"
      ]
    },
    {
      "metadata": {
        "id": "iCrivu_mumJr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "1d8ed26e-c7a4-4b3e-b7b9-34cb0f5d0fca"
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "# Random seed \n",
        "numpy.random.seed(7)\n",
        "\n",
        "# Carrega o conjunto de dados, mas mantém apenas as palavras n superiores, zerando o resto\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = top_words)\n",
        "\n",
        "# Em seguida, precisamos truncar e ajustar as sequências de entrada para que elas tenham o mesmo comprimento \n",
        "# para modelagem. \n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen = max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen = max_review_length)\n",
        "\n",
        "# Cria o modelo\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length = max_review_length))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs = 3, batch_size = 64)\n",
        "\n",
        "# Avaliação final do modelo\n",
        "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
        "print(\"Acurácia: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 500, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 392s 16ms/step - loss: 0.4688 - acc: 0.7726\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 391s 16ms/step - loss: 0.3296 - acc: 0.8710\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 390s 16ms/step - loss: 0.2823 - acc: 0.8884\n",
            "Acurácia: 85.91%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CkVDWqaYumJy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Aplicando Regularização com Dropout em LSTMs - Versão 2"
      ]
    },
    {
      "metadata": {
        "id": "qt0FTsstumJ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "326386e6-3f08-4a80-f1ec-6a71433ddd8e"
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "# Random seed \n",
        "numpy.random.seed(7)\n",
        "\n",
        "# Carrega o conjunto de dados, mas mantém apenas as palavras n superiores, zerando o resto\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = top_words)\n",
        "\n",
        "# Em seguida, precisamos truncar e ajustar as sequências de entrada para que elas tenham o mesmo comprimento \n",
        "# para modelagem. \n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen = max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen = max_review_length)\n",
        "\n",
        "# Cria o modelo\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length = max_review_length))\n",
        "model.add(LSTM(100, dropout = 0.2, recurrent_dropout = 0.2))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs = 3, batch_size = 64)\n",
        "\n",
        "# Avaliação final do modelo\n",
        "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
        "print(\"Acurácia: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 449s 18ms/step - loss: 0.4934 - acc: 0.7596\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 448s 18ms/step - loss: 0.3579 - acc: 0.8506\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 447s 18ms/step - loss: 0.3579 - acc: 0.8508\n",
            "Acurácia: 83.76%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SUwxA8YjumJ5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## LSTMs e CNNs Para Classificação de Sequências"
      ]
    },
    {
      "metadata": {
        "id": "fnEbi5SqumJ7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As redes neurais convolucionais são excelentes para aprender a estrutura espacial em dados de entrada. No IMDB os dados de reviews têm uma estrutura espacial unidimensional na sequência de palavras nas revisões e uma CNN poderá escolher características invariantes para o sentimento bom e ruim. Estas características espaciais aprendidas podem então ser aprendidas como sequências por uma camada LSTM. Podemos facilmente adicionar uma CNN unidimensional e as camadas de Max Pooling após a camada Embedding que, em seguida, alimente os recursos consolidados para o LSTM. Podemos usar um pequeno conjunto de 32 recursos com um pequeno filtro de comprimento igual a 3. A camada de pooling pode usar o comprimento padrão de 2 para reduzir a metade o tamanho do mapa de recursos."
      ]
    },
    {
      "metadata": {
        "id": "57U3KkD-umJ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "23db282e-7956-4e53-9637-96b0c7a6c422"
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.convolutional import Convolution1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "# Random seed \n",
        "numpy.random.seed(7)\n",
        "\n",
        "# Carrega o conjunto de dados, mas mantém apenas as palavras n superiores, zerando o resto\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = top_words)\n",
        "\n",
        "# Em seguida, precisamos truncar e ajustar as sequências de entrada para que elas tenham o mesmo comprimento \n",
        "# para modelagem. \n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "\n",
        "# Cria o modelo\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length = max_review_length))\n",
        "model.add(Convolution1D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
        "model.add(MaxPooling1D(pool_size = 2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs = 3, batch_size = 64)\n",
        "\n",
        "# Avaliação final do modelo\n",
        "scores = model.evaluate(X_test, y_test, verbose = 0)\n",
        "print(\"Acurácia: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 500, 32)           3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 216,405\n",
            "Trainable params: 216,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 204s 8ms/step - loss: 0.4385 - acc: 0.7845\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 201s 8ms/step - loss: 0.2489 - acc: 0.9025\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 200s 8ms/step - loss: 0.1994 - acc: 0.9237\n",
            "Acurácia: 87.86%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}