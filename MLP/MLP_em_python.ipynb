{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP-em-python.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladimiralencar/DeepLearning-LANA/blob/master/MLP/MLP_em_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "B8kxbTaVLVLn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Construindo Um Algoritmo Para Rede Neural Multilayer Perceptron"
      ]
    },
    {
      "metadata": {
        "id": "P2gJ2utbLVLo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Otimização com Stochastic Gradient Descent "
      ]
    },
    {
      "metadata": {
        "id": "UT478YWHLVLq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Stochastic Gradient Descent (SGD) é uma versão de Gradient Descent, onde em cada passagem para a frente, obtemos um lote de dados com amostras aleatórias do conjunto de dados total. Aqui onde entra em cena o batch_size. Esse é o tamanho do lote. Idealmente, todo o conjunto de dados seria alimentado na rede neural em cada passagem para a frente, mas na prática isso acaba não sendo possível, devido a restrições de memória. SGD é uma aproximação de Gradient Descent, quanto mais lotes processados pela rede neural, melhor será a aproximação."
      ]
    },
    {
      "metadata": {
        "id": "x4SjrpdQLVLs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Uma implementação do SGD envolve:\n",
        "\n",
        "1. Gerar lotes de dados de amostras aleatórias do conjunto de dados total.\n",
        "\n",
        "2. Executar a rede para frente (Forward Pass) e para trás (Backward pass) para calcular o gradiente (com dados de (1)).\n",
        "\n",
        "3. Aplicar a atualização de descida do gradiente.\n",
        "\n",
        "4. Repitir as etapas 1-3 até a convergência ou o loop for parado por outro mecanismo (como o número de épocas, por exemplo).\n",
        "\n",
        "Se tudo correr bem, a perda da rede vai diminuindo, indicando pesos e bias mais úteis ao longo do tempo."
      ]
    },
    {
      "metadata": {
        "id": "sGY_0rFCLVLt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Neuronio:\n",
        "    \"\"\"\n",
        "    Classe base para os nós da rede.\n",
        "\n",
        "    Argumentos:\n",
        "\n",
        "        \"nodes_entrada\": Uma lista de nós com arestas para este nó.\n",
        "    \"\"\"\n",
        "    def __init__(self, nodes_entrada = []):\n",
        "        \"\"\"\n",
        "        O construtor do nó (é executado quando o objeto é instanciado). \n",
        "        Define propriedades que podem ser usadas por todos os nós.\n",
        "        \"\"\"\n",
        "        # Lista de nós com arestas para este nó.\n",
        "        self.nodes_entrada = nodes_entrada\n",
        "        \n",
        "        # Lista de nós para os quais este nó gera saída.\n",
        "        self.nodes_saida = []\n",
        "        \n",
        "        # O valor calculado por este nó. É definido executando o método forward().\n",
        "        self.valor = None\n",
        "        \n",
        "        # Este objeto é um dicionário com pares chaves/valor entre {} \n",
        "        # As chaves (keys) são os inputs para este nó e o valores (values) são as paciais deste nó em relação ao input.\n",
        "        self.gradientes = {}\n",
        "        \n",
        "        # Configuramos este nó como um nó de saída para todos os nós de entrada.\n",
        "        for n in nodes_entrada:\n",
        "            n.nodes_saida.append(self)\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"\n",
        "        Todo o nó que usar essa classe como uma classe base, precisa definir seu próprio método \"forward\".\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self):\n",
        "        \"\"\"\n",
        "        Todo o nó que usar essa classe como uma classe base, precisa definir seu próprio método \"backward\".\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "\n",
        "class Input(Neuronio):\n",
        "    \"\"\"\n",
        "    Input genérico para a rede.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # O construtor da classe base deve ser executado para configurar todas as propriedades aqui.\n",
        "        #\n",
        "        # A propriedade mais importante de Input é valor.\n",
        "        # self.valor é definido na função topological_sort().\n",
        "        Neuronio.__init__(self)\n",
        "\n",
        "    def forward(self):\n",
        "        # Nada a ser feito aqui.\n",
        "        pass\n",
        "\n",
        "    def backward(self):\n",
        "        # Um nó de Input não possui entradas (pois ele já é a entrada) e assim o gradiente (derivada) é zero.\n",
        "        # A palavra reservada \"self\", é referência para este objeto.\n",
        "        self.gradientes = {self: 0}\n",
        "        \n",
        "        # Pesos e bias podem ser inputs, assim precisamos somar o gradiente de outros gradientes de saída\n",
        "        for n in self.nodes_saida:\n",
        "            self.gradientes[self] += n.gradientes[self]\n",
        "            \n",
        "\n",
        "class Linear(Neuronio):\n",
        "    \"\"\"\n",
        "    Representa um nó que realiza transformação linear.\n",
        "    \"\"\"\n",
        "    def __init__(self, X, W, b):\n",
        "        # O construtor da classe base (nó). \n",
        "        # Pesos e bias são tratados como nós de entrada (nodes_entrada).\n",
        "        Neuronio.__init__(self, [X, W, b])\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"\n",
        "        Executa a matemática por trás da transformação linear.\n",
        "        \"\"\"\n",
        "        X = self.nodes_entrada[0].valor\n",
        "        W = self.nodes_entrada[1].valor\n",
        "        b = self.nodes_entrada[2].valor\n",
        "        self.valor = np.dot(X, W) + b\n",
        "\n",
        "    def backward(self):\n",
        "        \"\"\"\n",
        "        Calcula o gradiente com base nos valores de saída.\n",
        "        \"\"\"\n",
        "        # Inicializa um parcial para cada um dos nodes_entrada.\n",
        "        self.gradientes = {n: np.zeros_like(n.valor) for n in self.nodes_entrada}\n",
        "        \n",
        "        # Ciclo através dos outputs. \n",
        "        # O gradiente mudará dependendo de cada output, assim os gradientes são somados sobre todos os outputs.\n",
        "        for n in self.nodes_saida:\n",
        "            \n",
        "            # Obtendo parcial da perda em relação a este nó.\n",
        "            grad_cost = n.gradientes[self]\n",
        "            \n",
        "            # Definindo o parcial da perda em relação às entradas deste nó.\n",
        "            self.gradientes[self.nodes_entrada[0]] += np.dot(grad_cost, self.nodes_entrada[1].valor.T)\n",
        "            \n",
        "            # Definindo o parcial da perda em relação aos pesos deste nó.\n",
        "            self.gradientes[self.nodes_entrada[1]] += np.dot(self.nodes_entrada[0].valor.T, grad_cost)\n",
        "            \n",
        "            # Definindo o parcial da perda em relação ao bias deste nó.\n",
        "            self.gradientes[self.nodes_entrada[2]] += np.sum(grad_cost, axis = 0, keepdims = False)\n",
        "\n",
        "\n",
        "class Sigmoid(Neuronio):\n",
        "    \"\"\"\n",
        "    Representa o nó da função de ativação Sigmoid.\n",
        "    \"\"\"\n",
        "    def __init__(self, node):\n",
        "        # O construtor da classe base.\n",
        "        Neuronio.__init__(self, [node])\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        \"\"\"\n",
        "        Este método é separado do `forward` porque ele também será usado com \"backward\".\n",
        "\n",
        "        `x`: Um array Numpy.\n",
        "        \"\"\"\n",
        "        return 1. / (1. + np.exp(-x))\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"\n",
        "        Executa a função _sigmoid e define a variável self.valor\n",
        "        \"\"\"\n",
        "        input_value = self.nodes_entrada[0].valor\n",
        "        self.valor = self._sigmoid(input_value)\n",
        "\n",
        "    def backward(self):\n",
        "        \"\"\"\n",
        "        Calcula o gradiente usando a derivada da função sigmoid \n",
        "        \n",
        "        O método backward da classe Sigmoid, soma as derivadas (é uma derivada normal quando há apenas uma variável) \n",
        "        em relação à única entrada sobre todos os nós de saída.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Inicializa os gradientes com zero.\n",
        "        self.gradientes = {n: np.zeros_like(n.valor) for n in self.nodes_entrada}\n",
        "        \n",
        "        # Soma a parcial em relação ao input sobre todos os outputs.\n",
        "        for n in self.nodes_saida:\n",
        "            grad_cost = n.gradientes[self]\n",
        "            sigmoid = self.valor\n",
        "            self.gradientes[self.nodes_entrada[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
        "\n",
        "\n",
        "class MSE(Neuronio):\n",
        "    def __init__(self, y, a):\n",
        "        \"\"\"\n",
        "        Função de custo para calcular o erro médio quadrático.\n",
        "        Deve ser usado como último nó da rede.\n",
        "        \"\"\"\n",
        "        # Chamada ao construtor da classe base.\n",
        "        Neuronio.__init__(self, [y, a])\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"\n",
        "        Calcula o erro médio ao quadrado.\n",
        "        \"\"\"\n",
        "        # Fazemos o reshape para evitar possíveis problemas nas operações de matrizes/vetores \n",
        "        #\n",
        "        # Convertendo os 2 arrays (3,1) garantimos que o resultado será (3,1) e, assim, \n",
        "        # teremos uma subtração elementwise.\n",
        "        y = self.nodes_entrada[0].valor.reshape(-1, 1)\n",
        "        a = self.nodes_entrada[1].valor.reshape(-1, 1)\n",
        "\n",
        "        self.m = self.nodes_entrada[0].valor.shape[0]\n",
        "        \n",
        "        # Salva o output computado para o backward pass.\n",
        "        self.diff = y - a\n",
        "        self.valor = np.mean(self.diff**2)\n",
        "\n",
        "    def backward(self):\n",
        "        \"\"\"\n",
        "        Calcula o gradiente do custo.\n",
        "        \"\"\"\n",
        "        self.gradientes[self.nodes_entrada[0]] = (2 / self.m) * self.diff\n",
        "        self.gradientes[self.nodes_entrada[1]] = (-2 / self.m) * self.diff\n",
        "\n",
        "\n",
        "def topological_sort(feed_dict):\n",
        "    \"\"\"\n",
        "    Classifica os nós em ordem topológica usando o Algoritmo de Kahn.\n",
        "\n",
        "    `Feed_dict`: um dicionário em que a chave é um nó `Input` e o valor é o respectivo feed de valor para esse nó.\n",
        "\n",
        "    Retorna uma lista de nós ordenados.\n",
        "    \"\"\"\n",
        "\n",
        "    input_nodes = [n for n in feed_dict.keys()]\n",
        "\n",
        "    G = {}\n",
        "    nodes = [n for n in input_nodes]\n",
        "    while len(nodes) > 0:\n",
        "        n = nodes.pop(0)\n",
        "        if n not in G:\n",
        "            G[n] = {'in': set(), 'out': set()}\n",
        "        for m in n.nodes_saida:\n",
        "            if m not in G:\n",
        "                G[m] = {'in': set(), 'out': set()}\n",
        "            G[n]['out'].add(m)\n",
        "            G[m]['in'].add(n)\n",
        "            nodes.append(m)\n",
        "\n",
        "    L = []\n",
        "    S = set(input_nodes)\n",
        "    while len(S) > 0:\n",
        "        n = S.pop()\n",
        "\n",
        "        if isinstance(n, Input):\n",
        "            n.valor = feed_dict[n]\n",
        "\n",
        "        L.append(n)\n",
        "        for m in n.nodes_saida:\n",
        "            G[n]['out'].remove(m)\n",
        "            G[m]['in'].remove(n)\n",
        "            if len(G[m]['in']) == 0:\n",
        "                S.add(m)\n",
        "    return L\n",
        "\n",
        "\n",
        "def forward_and_backward(graph):\n",
        "    \"\"\"\n",
        "    Executa uma passagem para a frente e uma passagem para trás através de uma lista de nós ordenados.\n",
        "\n",
        "     Argumentos:\n",
        "\n",
        "         `Graph`: O resultado de `topological_sort`.\n",
        "    \"\"\"\n",
        "    # Forward pass\n",
        "    for n in graph:\n",
        "        n.forward()\n",
        "\n",
        "    # Backward pass\n",
        "    # O valor negativo no slice permite fazer uma cópia da mesma lista na ordem inversa.\n",
        "    for n in graph[::-1]:\n",
        "        n.backward()\n",
        "\n",
        "\n",
        "def sgd_update(params, learning_rate = 1e-2):\n",
        "    \"\"\"\n",
        "    Atualiza o valor de cada parâmetro treinável com o SGD.\n",
        "\n",
        "    Argumentos:\n",
        "\n",
        "         `Trainables`: uma lista de nós `Input` que representam pesos / bias.\n",
        "         `Learning_rate`: a taxa de aprendizado.\n",
        "    \"\"\"\n",
        "    # Executa o SGD\n",
        "    #\n",
        "    # Loop sobre todos os parâmetros\n",
        "    for t in params:\n",
        "        # Alterar o valor do parâmetro, subtraindo a taxa de aprendizado \n",
        "        # multiplicado pela parte do custo em relação a esse parâmetro\n",
        "        partial = t.gradientes[t]\n",
        "        t.valor -= learning_rate * partial"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6iFFlLrmLVLy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Executando o Grafo"
      ]
    },
    {
      "metadata": {
        "id": "7I9lzQKvLVLz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html"
      ]
    },
    {
      "metadata": {
        "id": "lqIhgVFZLVLz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Reamostragem (com reposição)"
      ]
    },
    {
      "metadata": {
        "id": "GV4DMqXdLVL2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "50753551-1476-41d4-c986-ce567a4c9cf6"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "y = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n",
        "batch_size = 2\n",
        "print(resample(y, n_samples=batch_size))\n",
        "print(resample(y, n_samples=batch_size))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[15  6]\n",
            "[17  7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CNkP4t-7LVL-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Chamada da Rede Neural"
      ]
    },
    {
      "metadata": {
        "id": "gkrG5bURLVMA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5bb8c9ab-613c-494c-8043-67cff63d383a"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.utils import shuffle, resample\n",
        "\n",
        "# Carrega os dados\n",
        "data = load_boston()\n",
        "\n",
        "# Variáveis de entrada e saída para treinamento supervisionado\n",
        "X_ = data['data']\n",
        "y_ = data['target']\n",
        "\n",
        "# Normaliza os dados\n",
        "X_ = (X_ - np.mean(X_, axis = 0)) / np.std(X_, axis = 0)\n",
        "\n",
        "# Número de features e número de neurônios\n",
        "n_features = X_.shape[1] # 13 atributos\n",
        "n_hidden = 10 # numero de neuroneos\n",
        "\n",
        "# Define valores randômicos para inicializar pesos e bias\n",
        "W1_ = np.random.randn(n_features, n_hidden)\n",
        "b1_ = np.zeros(n_hidden)\n",
        "W2_ = np.random.randn(n_hidden, 1)\n",
        "b2_ = np.zeros(1)\n",
        "\n",
        "# Rede Neural\n",
        "X, y = Input(), Input()\n",
        "W1, b1 = Input(), Input()\n",
        "W2, b2 = Input(), Input()\n",
        "\n",
        "l1 = Linear(X, W1, b1)\n",
        "s1 = Sigmoid(l1)\n",
        "l2 = Linear(s1, W2, b2)\n",
        "cost = MSE(y, l2)\n",
        "\n",
        "# Define o feed_dict\n",
        "feed_dict = {\n",
        "    X: X_,\n",
        "    y: y_,\n",
        "    W1: W1_,\n",
        "    b1: b1_,\n",
        "    W2: W2_,\n",
        "    b2: b2_\n",
        "}\n",
        "\n",
        "# Número de epochs (altere esse valor para ver as mudanças no resultado)\n",
        "epochs = 2000\n",
        "\n",
        "# Número total de exemplos\n",
        "m = X_.shape[0]\n",
        "\n",
        "# Batch size\n",
        "batch_size = 11\n",
        "steps_per_epoch = m // batch_size\n",
        "\n",
        "# Define o grafo computacional\n",
        "graph = topological_sort(feed_dict)\n",
        "\n",
        "# Valores que serão aprendidos pela rede\n",
        "params = [W1, b1, W2, b2]\n",
        "\n",
        "# Número total de exemplos\n",
        "print(\"Número Total de Exemplos = {}\".format(m))\n",
        "\n",
        "# Treinamento do modelo\n",
        "import sys\n",
        "for i in range(epochs):\n",
        "    loss = 0\n",
        "    for j in range(steps_per_epoch):\n",
        "        \n",
        "        # Passo 1 - Testa aleatoriamente um lote de exemplos\n",
        "        X_batch, y_batch = resample(X_, y_, n_samples = batch_size)\n",
        "\n",
        "        # Reset dos valores de X e y \n",
        "        X.valor = X_batch\n",
        "        y.valor = y_batch\n",
        "\n",
        "        # Passo 2 - Forward e Backpropagation\n",
        "        forward_and_backward(graph)\n",
        "\n",
        "        # Passo 3 - Otimização por SGD\n",
        "        sgd_update(params)\n",
        "\n",
        "        loss += graph[-1].valor\n",
        "\n",
        "    sys.stdout.write(\"\\rEpoch: {}, Custo: {:.3f}\".format(i+1, loss/steps_per_epoch))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número Total de Exemplos = 506\n",
            "Epoch: 2000, Custo: 3.507"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dhVmGcAqLVME",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "40e1f991-59e9-4227-d2b7-3585cf1a4003"
      },
      "cell_type": "code",
      "source": [
        "print(n_features)\n",
        "print(len(X_))\n",
        "print(len(y_))\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "print(W1.valor)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13\n",
            "506\n",
            "506\n",
            "[[ -2.198  -1.066  -0.29   -0.696   0.219   3.756  -1.594  -4.279  -0.808\n",
            "    0.319]\n",
            " [  1.971  -2.936   1.301   2.514  -0.082  -0.807  -1.823  -5.18    0.572\n",
            "    0.146]\n",
            " [ -1.972  -2.536   0.966   1.159   0.169   1.955  -1.461  -3.403  -0.802\n",
            "    1.691]\n",
            " [  0.31   -4.984   0.025   2.239  -0.55   -0.806  -4.696   0.016   6.163\n",
            "    0.142]\n",
            " [ -4.041   0.793   0.658  -3.692   1.261   2.624   2.295  -6.324  -1.177\n",
            "   -0.712]\n",
            " [ -2.643  -5.224   4.95   -0.746   1.908   3.058  -0.564  -2.287   2.636\n",
            "    2.281]\n",
            " [  2.39    3.252  -1.817   3.393  -1.991  -0.609  -1.648  -6.33    0.049\n",
            "   -0.379]\n",
            " [ -6.36    3.207  -3.972  -8.849   1.879   5.332  -5.645  -2.248  -1.222\n",
            "   -0.251]\n",
            " [ -1.668   3.478   5.235  13.111  -4.069   7.306  -1.128   3.184  -1.174\n",
            "    0.063]\n",
            " [ -0.607  -5.342   1.247 -12.877   1.618  -0.809  -1.737   3.528  -3.232\n",
            "    1.327]\n",
            " [ -2.546   2.9    -0.495  -7.367  -1.494   5.044   4.254  -5.833  -0.558\n",
            "    0.335]\n",
            " [ -0.202   0.554  -2.971  -0.494   1.344  -0.771  -5.315   3.889  -3.523\n",
            "    1.434]\n",
            " [ -4.493   0.905  -6.624   0.188   1.968   3.594   0.927  -2.981  -2.855\n",
            "   -1.731]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fvsGqurALVMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4bef5a10-d398-4d8d-f1b0-ee2b3e31a9db"
      },
      "cell_type": "code",
      "source": [
        "print(b1.valor)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  1.904  -1.641 -11.188  -6.358  -1.601   0.495  -2.175   6.44   -4.289\n",
            "  -1.059]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}